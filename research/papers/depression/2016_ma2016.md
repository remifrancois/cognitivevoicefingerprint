# Ma2016 et al. (2016) — Full Text Extraction

**Source file:** 2016_ma2016.pdf
**Pages:** 25
**Extracted by:** MemoVoice research pipeline (PyMuPDF)

---

## Full Paper Content

SAMPLING AND ESTIMATION FOR (SPARSE)
EXCHANGEABLE GRAPHS
VICTOR VEITCH AND DANIEL M. ROY
Abstract. Sparse exchangeable graphs on R+, and the associated graphex
framework for sparse graphs, generalize exchangeable graphs on N, and the
associated graphon framework for dense graphs.
We develop the graphex
framework as a tool for statistical network analysis by identifying the sampling
scheme that is naturally associated with the models of the framework, and
by introducing a general consistent estimator for the parameter (the graphex)
underlying these models. The sampling scheme is a modiﬁcation of independent
vertex sampling that throws away vertices that are isolated in the sampled
subgraph. The estimator is a dilation of the empirical graphon estimator,
which is known to be a consistent estimator for dense exchangeable graphs;
both can be understood as graph analogues to the empirical distribution in
the i.i.d. sequence setting. Our results may be viewed as a generalization of
consistent estimation via the empirical graphon from the dense graph regime
to also include sparse graphs.
Contents
1.
Introduction
1
2.
Preliminaries
8
3.
Sampling
10
4.
Estimation with known sizes
11
5.
Estimation for unknown sizes
21
Acknowledgements
25
References
25
1. Introduction
This paper is concerned with mathematical foundations for the statistical analysis
of real-world networks. For densely connected networks, the graphon framework
has emerged as powerful tool for both theory and applications in network analysis;
many of the models used in practice are within the remit of this framework (see
[OR15] for a review). However, in most real-world situations, networks are sparsely
connected; i.e., as one studies larger networks, one ﬁnds that they tend to exhibit
only a vanishing fraction of all possible links.
In this paper, we continue our study of sparse exchangeable graphs, i.e., random
graphs whose vertices may be identiﬁed with nonnegative reals, R+, and whose edge
sets are then modeled by exchangeable point processes on R2
+. In a pioneering paper,
Caron and Fox [CF14] introduced the notion of sparse exchangeable graphs in the
context of nonparametric Bayesian analysis. Building on this work, the general family
of all sparse exchangeable graphs was characterized by Veitch and Roy [VR15]; Borgs,
Chayes, Cohn, and Holden [BCCH16], and shown to generalize the graphon models
for dense graphs to include the sparse graph regime. Sparse exchangeable graphs
have a number of desirable properties, including that they deﬁne a natural projective
1
arXiv:1611.00843v1  [math.ST]  2 Nov 2016


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
2
family of subgraphs of growing size, which can be used to model the process of
observing a larger and larger fraction of a ﬁxed underlying network. This property
also provides a ﬁrm foundation for the study of various asymptotics, as demonstrated
by [VR15; BCCH16] and the results here. [VR15] characterized the asymptotic
degree distribution and connectedness of sparse exchangeable graphs, demonstrating
that sparse exchangeable graphs allow for sparsity and admit the rich graph structure
(such as small-world connectivity and power law degree distributions) found in large
real-world networks. On this basis, Veitch and Roy argue that sparse exchangeable
graphs can serve as a general statistical model for network data.
Despite our
understanding of these models, the statistical meaning remains somewhat opaque.
Put simply, when would it be natural to use the sparse exchangeable graph model?
The present paper further develops this framework for statistical network analysis
by answering two fundamental questions:
(1) What is the notion of sampling naturally associated with this statistical
network model? and
(2) How can we use an observed dataset to consistently estimate the statistical
network model?
The answers to these questions signiﬁcantly clarify both the meaning of the modeling
framework, and its connection to the dense graph framework and the classical i.i.d.
sequence framework at the foundation of classical statistics. These questions may
be viewed as speciﬁc examples of a general approach to formalizing the problem of
statistical analysis on network data being carried out by Orbanz [Orb16].
To explain the results, we ﬁrst recall the modeling framework of [VR15; BCCH16].
The basic setup introduces a family of ﬁnite, symmetric point processes Γs ⊆
[0, s] × [0, s], for s ∈R+, where each Γs is interpreted as the edge set of a random
graph whose vertices are points in the interval [0, s]. Hence, for θ, θ′ ∈[0, s], there
is an edge between θ and θ′ if and only if (θ, θ′) ∈Γs. The edge set Γs determines a
graph over its active vertex set: those elements θ ∈[0, s] such that θ exhibits some
edge in Γs. Accordingly, (Γs)s∈R+ are understood as (R+-labeled) graph-valued
random variables that are nested in the sense that Γr ⊆Γs whenever r ≤s. We
will argue below that the indices s ∈R+ are properly understood as specifying the
sample size of the corresponding observations Γs.
The natural parameter of the distributions of these graphs is a graphex W =
(I, S, W) deﬁned on some locally ﬁnite measure space (ϑ, Bϑ, ν) where I ∈R+,
S : ϑ →R+ is an integrable function, and W : ϑ2 →[0, 1] is a symmetric function
satisfying several weak integrability conditions we formalize later. (Without loss of
generality, one can always take (ϑ, Bϑ) to be the non-negative reals, R+, with its
standard Borel structure, and take ν to be Lebesgue measure Λ.) The component
W is a natural generalization of the graphon of the dense graph models [VR15;
BCCH16], and for this reason we refer to it as a graphon. Although the results of
the present paper hold for general graphexes, for simplicity of exposition, we will
temporarily restrict our attention to graphexes of the form W = (0, 0, W), giving a
full treatment in subsequent sections.
We begin by giving a construction of the graphex process for a graphex of the
form W = (0, 0, W): Let Π be a Poisson (point) process on R+ × ϑ with intensity
Λ ⊗ν, i.e., for two intervals J1, J2 ⊆R+ and two measurable subsets B1, B2 ⊆ϑ,
the number of points of Π in J1 × B1 and in J2 × B2 are Poisson random variables
with mean |J1|ν(B1) and |J2|ν(B2) respectively, where |Ji| = Λ(Ji) is the length of


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
3
the interval, and these variables are even independent when J1 × B1 ∩J2 × B2 = ∅.
(If ν is also Lebesgue measure on R+, then Π is then simply a unit-rate Poisson
process on R2
+.) Write {(θi, ϑi)}i∈N for the points of Π, let Πs be the restriction of
Π to [0, s]2, and let (ζ{i,j})i≤j∈N be an i.i.d. collection of uniform random variables
in [0, 1].
For every s ∈R+, deﬁne the size-s random edge set Γs on [0, s] to be exactly the
set of distinct pairs (θi, θj) where θi, θj ≤s and ζ{i,j} ≤W(ϑi, ϑj). In other words,
for every distinct pair of points (θ, ϑ), (θ′, ϑ′) ∈Πs, the edge set Γs includes the
edge (θ, θ′) independently with probability W(ϑ, ϑ′). The vertex set of the graph
corresponding to the edge set Γs is deﬁned to be those points that appear in some
edge; hence, this model does not allow for isolated vertices. The entire family of
graphs Γs, for s ∈R+, is a projective family with respect to subset restriction, i.e.,
Γr = Γs ∩[0, r]2 for every r, s ∈R+ with r ≤s. See Fig. 1 for an illustration of the
generative model for a general graphex W deﬁned on R+ with Lebesgue measure.
A rigorous deﬁnition is provided in Section 2.
We refer to Γ as the graphex process generated by W: we also use this nomencla-
ture for the family (Γs)s∈R+. Note that the Γ has the property that its distribution is
invariant to the action of the maps (x, y) 7→(φ(x), φ(y)), where φ : R+ →R+ is mea-
sure preserving. A random graph with this property is called a sparse exchangeable
graph [CF14]. In Section 2, we quote the result due to Veitch and Roy [VR15]; Borgs,
Chayes, Cohn, and Holden [BCCH16], building oﬀwork by Kallenberg [Kal90], that
proves that every (sparse) exchangeable graph is a graphex process generated by
some (potentially random) graphex. For a ﬁnite labeled graph G, such as each Γs,
for s ∈R+, we will write G(G) to denote the unlabeled1 graph corresponding to G.
The ﬁrst contribution of the present paper is the identiﬁcation of a sampling
scheme that is naturally associated with the graphex processes:
Deﬁnition 1.1. A p-sampling of an unlabeled graph G is obtained by selecting
each vertex of G independently with probability p ∈[0, 1], and then returning the
edge set of the random vertex-induced subgraph of G.
It is important to note that only the edge set of the vertex-induced subgraph is
returned; in other words, vertices that are isolated from the other sampled vertices
are thrown away. The key fact about this sampling scheme is that: For s > 0 and
r ∈[0, s], if Gr is an r/s-sampling of G(Γs) then Gr
d= G(Γr). This result justiﬁes
the interpretation of the parameter s as a sample size.
In the estimation problem for the graphex process, the observed dataset is a
realization of the random sequence of graphs G1, G2, . . . such that Gk = G(Γsk),
and s1, s2, . . . is some sequence of sizes such that sk ↑∞as k →∞. The task is to
take such an observation and return an estimate for W, where W is the graphex
that generated (Γs)s∈R+. Both the formulation and solution of this problem depend
on whether the sizes sk are included as part of the observations.
We ﬁrst treat the simpler case where the sizes are known. To formalize the
estimation problem we must introduce a notion of when one graphex is a good
approximation for another. Intuitively, our notion is that, for any ﬁxed s, a size-s
1The unlabelled graph corresponding to a labelled graph G is the equivalence class of graphs
isomorphic to G. Restricting ourselves to ﬁnite unlabelled graphs, we can represent the unlabelled
graphs formally in terms of their homomorphism counts, (NF ), where F ranges over the countable
set of all ﬁnite simple graphs whose vertex set is [n] for some n ∈N, and NF is the number of
homomorphisms from F to G.


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
4
s
t
ϑ8
θ8
ϑ2
θ2
ϑ1
θ1
ϑ9
θ9
ϑ4
θ4
ϑ3
θ3
s
t
s
s
t
t
θ8
θ8
θ2
θ2
θ1
θ1
θ9
θ9
θ4
θ4
θ3
θ3
0
s
t
Figure 1.
Generative process of a graphex process generated by a graphex W =
(I, S, W) deﬁned on R+ with Lebesgue measure, observed at sizes s and t. First
panel: a (necessarily truncated) realization of the latent Poisson process Πt on
[0, t] × R+. A countably inﬁnite number of points lie above the six points visualized.
Second panel: Edges due to the graphon component W are sampled by connecting
each distinct pair of points (θi, ϑi), (θj, ϑj) ∈Πt independently with probability
W(ϑi, ϑj). Integrability conditions on W imply that only a ﬁnite number of edges
will appear, despite there being an inﬁnite number of points in Πt. Assume the three
edges are the only ones. Third panel: The edge set Γt represented as an adjacency
measure on [0, t]2. The edges in the graphon component appears as (symmetric
pairs of) black dots; the edges corresponding to the star component S appear in
green; the isolated edges (from the I component) appear in blue. At size s, only the
edges in [0, s]2 (inner dashed black line) appear in the graph. The edges {θj, σjk}
of the star (S) component of the process (green) centered at θj are realizations of
a rate-S(ϑj) Poisson process {σjk} along the line through θj (show as green dots
along grey dotted lines). Hence, at size t, each point θi is the center of Poi(t S(ϑi))
star process rays. The edges {ρi, ρ′
j} generated by the isolated edge (I) component
of the process (blue) are a realization of a rate-I Poisson process on the upper (or
lower) triangle of [0, t]2, reﬂected. At size t, there are Poi(t2 I) isolated edges due
to this part of the graphex. The ﬁnal panel shows the graphs corresponding to the
sampled adjacency measure at sizes s and t.
random graph generated by an estimator should be close in distribution to a size-s
random graph generated by the true graphex. Let uKEG(W, s) be the distribution
of an unlabeled size-s graphex process, i.e., the distribution of G(Γs) where Γ is
generated by W. Approximation is then formalized by the following notion of
convergence:
Deﬁnition 1.2. Write Wk →GP W as k →∞, when uKEG(Wk, s) →uKEG(W, s)
weakly as k →∞, for all s ∈R+.
Our goal in the estimation problem is then to take a sequence of observations and use
these to produce a sequence of graphexes W1, W2, . . . that are consistent in the sense
that Wk →GP W as k →∞. This is a natural analogue of the deﬁnition of consistent
estimation used for the convergence of the empirical cumulative distribution function
in the i.i.d. sequence setting, and of the deﬁnition of consistent estimation used for
the convergence of the empirical graphon in the dense graph setting.
Let v(G) denote the number of vertices of graph G. Our estimator is the dilated
empirical graphon
ˆW(Gk,sk) : [0, v(Gk)/sk)2 →{0, 1},
(1.1)


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
5
deﬁned by transforming the adjacency matrix of Gk into a step function on
[0, v(Gk)/sk)2 where each pixel has size 1/sk ×1/sk; see Fig. 2. Intuitively, when the
generating graphex is W = (0, 0, W), we have sk ↑∞as k →∞, and the estimator is
an increasingly higher and higher resolution pixel picture of the generating graphon.
Formally, given a non-empty ﬁnite graph G with n vertices labeled 1, . . . , n, we
deﬁne the empirical graphon ˜WG : [0, 1]2 →{0, 1} by partitioning [0, 1] into adjacent
intervals I1, . . . , In each of length 1/n and taking ˜WG = 1 on Ii × Ij if i and j are
connected in G, and taking ˜W = 0 otherwise. The dilated empirical graphon with
dilation s is then deﬁned by ˆW(G,s)(x, y) = ˜WG(x/s, y/s). To map an unlabeled
graph to a (dilated) empirical graphon we must introduce a labeling of the vertices.
Notice that if φ : R+ →R+ is a measure-preserving transformation, φ ⊗φ is the
map (φ ⊗φ)(x, y) = (φ(x), φ(y)), W = (I, S, W), and W′ = (I, S ◦φ, W ◦(φ ⊗φ)),
then uKEG(W, s) = uKEG(W′, s) for all s ∈R+. In particular, the dilated empiri-
cal graphon functions corresponding to diﬀerent labelings of the vertices of G are
related by obvious measure-preserving transformations in this way. For the purposes
of this paper, graphexes that give rise to the same distributions over graphs are
equivalent. We then deﬁne the empirical graphon of an unlabeled graph to be the
empirical graphon of that graph with some arbitrary labeling, and we deﬁne the
dilated empirical graphon similarly. These functions may be thought as arbitrary
representatives of the equivalence class on graphons given by equating two graphons
whenever they correspond to isomorphic graphs.
The ﬁrst main estimation result is that ˆW(Gk,sk) →GP W in probability as
k →∞. That is, for every inﬁnite sequence N ⊆N there is a further inﬁnite
subsequence N ′ ⊆N such that ˆW(Gk,sk) →GP W almost surely along N ′. Subject
to an additional technical constraint (implied by integrability of W) the convergence
in probability may be replaced by convergence almost surely. Note that consistency
holds for observations generated by an arbitrary graphex W = (I, S, W), not just
those with the form W = (0, 0, W); see Fig. 3.
We now turn to the setting where the observation sizes s1, s2, . . . are not included
as part of the observations. In this case, we study two natural models for the dataset.
The ﬁrst is to treat the observed graphs Gk as realizations of G(Γsk) for some
(unknown) sequence sk ↑∞as k →∞that is independent of Γ. Another natural
model is to take G1, G2, . . . to be the sequence of all distinct graph structures taken
on by (Γs)s∈R+; in this case, for all k, we take Gk = G(Γτk), where τk is the latent
size at the kth occasion that the graph structure changes. In this later case, we
call G (Γ) = (G(Γτ1), G(Γτ2), . . . ) the graph sequence of Γ. (We deﬁne the graph
sequence formally in Section 2.)
Intuitively, G (Γ) is the graphex process Γ with the size information stripped away.
In this sense, the graph sequence of Γ is the random object naturally associated to
W when the sizes are unobserved. Thus, in this setting, convergence in distribution
of the graph sequences induced by the estimators is a natural notion of consistency.
Deﬁnition 1.3. Write Wk →GS W as k →∞when G (Γk)
d
−→G (Γ) as k →∞, for
Γk generated by Wk and Γ generated by W.
The notion of consistent estimation corresponding to this convergence is that, for any
ﬁxed ℓ∈N, the distribution of the length ℓpreﬁx of the graph sequence generated
by the estimator should be close to the distribution of the length ℓpreﬁx of the
graph sequence generated by W. Convergence in distribution of every ﬁnite-size
preﬁx is equivalent to convergence in distribution of the entire sequence.


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
6
Figure 2.
Realizations of dilated empirical graphons of graphex processes generated
by (0, 0, W) for W given in the rightmost column, at observation sizes given in the
bottom row. Note that the ordering of the vertices used to deﬁne the estimator
is arbitrary.
Here we have suggestively ordered the vertices according to the
latent values from the process simulations; with this ordering the dilated empirical
graphons are approximate pixel pictures of the generating graphon where the
resolution becomes ﬁner as the observation size grows. All three graphons satisfy
∥W∥1 = 1, and thus the expected number of edges (black pixels) at each size s is
1
2s2 in each column. Note that the rate of dilation is faster for sparser graphs; as
established in [VR15], the topmost graphex process used for this example is sparser
than the middle graphex process, and the graphon generating the bottom graphex
process is compactly supported and thus corresponds to a dense graph.
To explain our estimator for this setting, we will need the following concept:
Deﬁnition 1.4. Let c ∈R+ and let W = (I, S, W) be a graphex. A c-dilation of
W is the graphex Wc = (c2I, cS(·/c), W(·/c, ·/c)).
The key fact about c-dilations is that uKEG(W, s) = uKEG(Wc, s/c) for all s ∈R+,
and thus also G (Γ)
d= G (Γc) whenever Γ is generated by W and Γc is generated
by Wc. That is, the law of the graph sequence is invariant to dilations of the
generating graphex. This means, in particular, that the dilation of a graphex is
not an identiﬁable parameter when the observation sizes are not included as part
of the observation. The obvious guess for the estimator in this setting is then the
estimator for the known-sizes setting with the dilation information stripped away.
That is, our estimator is the dilated empirical graphon modulo dilation; i.e., it is
simply the empirical graphon ˜WGk : [0, 1]2 →[0, 1] deﬁned above. In this setting,
the empirical graphon is acting as a representative of its equivalence class under the
relation that equates graphons that generate graph sequences with the same laws.
The main estimation result is that if either
(1) There is some (possibly random) sequence (sk), independent from Γ, such
that sk ↑∞a.s. and Gk = G(Γsk) for all k ∈N, or
(2) (G1, G2, . . . ) = G (Γ),


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
7
Figure 3.
Realization of unlabeled graphex process generated by W = (I, S, W) at
size s = 15 (right panel), and associated dilated empirical graphon (left and center
panels). The generating graphex is W = (x + 1)−2(y + 1)−2, S = 1/2 exp(−(x + 1)),
and I = 0.1. The observation size is s = 15. The dilated empirical graphex is
pictured as two equivalent representations ˆ
W(G,15) and ˆ
W ′
(G,15), each with support
[0, 12)2 (180 vertices at size 15). Edges from the W component are shown in black,
edges from the S component are shown in green, and edges from the I component
are shown in blue. Recall that the ordering of the dilated empirical graphon is
arbitrary, so the left and center panels depict diﬀerent representations of the same
estimator. The leftmost panel shows the dilated empirical graphon with a random
ordering. The middle panel shows the dilated empirical graphon sorted to group
the I, S, and W edges, with the W edges sorted as in Fig. 2. The middle panel
gives some intuition for why the dilated empirical graphon is able to estimate the
entire graphex triple: When a graphex process is generated according to ˆ
W(G,15)
with latent Poisson process Π, the disjoint structure of the dilated graphon regions
due to the I, S, and W components induces a natural partitioning of Π into
independent Poisson processes that reproduce the independence structure used in
the full generative model Eq. (2.1).
then ˜WGk →GS W in probability as k →∞. Subject to an additional techni-
cal constraint (implied by integrability), the convergence in probability may be
strengthened to convergence almost surely.
Our estimation results are inspired by Kallenberg’s development of the theory of
estimation for exchangeable arrays [Kal99]. Restricted to the graph setting (that is,
2-dimensional arrays interpreted as adjacency matrices), and translated into modern
language, that paper introduced the empirical graphon (although not named as such)
and formalized consistency in terms of the weak topology: Wk →W as k →∞
when the graphs generated by Wk converge in distribution to the graphs generated
by W. The estimation results of the present paper may be seen as generalizations
of [Kal99] to the sparse graph regime.
The present paper is also closely related to the recent paper [BCCH16]. Spe-
cialized to the case ϑ = R+ equipped with Lebesgue measure, that paper extends
the cut distance between compactly supported graphons—a core tool in the limit
theory of dense graphs—to arbitrary integrable graphons. Convergence in the
cut distance then gives a notion of limit for sequences of graphons. This is ex-
tended to a notion of convergence for sequences of (sparse) graphs by saying that
a sequence G1, G2, . . . converges in the stretched cut distance sense if and only if
ˆW(G1,√
e(G1), ˆW(G2,√
e(G2), . . . converges with respect to the cut distance. That is,


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
8
each graph Gk is mapped to the empirical graphon dilated by v(Gk)/
p
e(Gk). The
same paper also establishes that e(Gk)/s2
k →∥W∥1 a.s.. Thus, in the ∥W∥1 = 1 case,
these dilated empirical graphons, considered as pixel pictures, will look asymptoti-
cally identical to the v(Gk)/sk-dilated empirical graphons that we use as estimators
in the known sizes case. This suggests that a close connection between consistent
estimation and convergence in the cut distance. Indeed, in the dense graph setting
these notions of convergence are known to be equivalent (in the dense setting, the
convergence Wk →GP W as k →∞is equivalent to left convergence [DJ08], and
left convergence is equivalence to convergence in the cut norm [BCLS+08]). An
analogous result in the sparse graph setting would allow for a very diﬀerent approach
to proving our convergence result in the known size setting, restricted to the special
case that the generating graphex is an integrable graphon.
The paper is organized as follows: In Section 2 we give formal deﬁnitions for the
basic tools of the paper. The sampling result is derived in Section 3. In Section 4
we prove the estimation result for the setting where observation sizes are included
as part of the observation. We build on this in Section 5 to prove the estimation
result for the setting where the true underlying observation sizes are not observed.
2. Preliminaries
The basic object of interest in this paper is point processes on R2
+, interpreted as
the edge sets of random graphs with vertices labeled in R+.
Deﬁnition 2.1. An adjacency measure is a purely atomic, symmetric, simple,
locally ﬁnite measure on R2
+.
If ξ = P
i,j δ(θi,θj) is an adjacency measure then the associated graph with labels in
R+ is one with edge set {(θi, θj)}, where θi ≤θj; the vertex set is deduced from the
edge set.
The deﬁning property of graphex processes is that, intuitively speaking, the
labels of the vertices of the graph are uninformative about the graph structure.
This is formalized by requiring that the associated adjacency measure is jointly
exchangeable, where
Deﬁnition 2.2. A random measure ξ on R2
+ is jointly exchangeable if ξ◦(φ⊗φ)
d= ξ
for any measure-preserving transformation φ : R+ →R+.
A representation theorem for jointly exchangeable random measures on R2
+ was
given by Kallenberg [Kal05; Kal90]. This result was translated to the setting of ran-
dom graphs in [VR15]. Writing Λ for Lebesgue measure and µW (·) =
´
R+ W(x, ·)dx,
the deﬁning object of the representation theorem is:
Deﬁnition 2.3. A graphex is a triple (I, S, W), where I ≥0 is a non-negative real,
S : R+ →R+ is integrable, and the graphon W : R2
+ →[0, 1] is symmetric, and
satisﬁes
(1) Λ{µW = ∞} = 0 and Λ{µW > 1} < ∞,
(2) Λ2[W; µW ∨µW ≤1] =
´
R2
+ W(x, y) 1[µW (x) ≤1] 1[µW (y) ≤1]dxdy < ∞,
(3)
´
R+ W(x, x) dx < ∞.
We say that a graphex is non-trivial if I + ∥S∥1 + ∥W∥1 > 0, i.e. if it is not the
case that the graphex is 0 a.e.
The representation theorem is:


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
9
Theorem 2.4. Let ξ be a random adjacency measure. ξ is jointly exchangeable iﬀ
there exists a (possibly random) graphex W = (I, S, W) such that, almost surely,
ξ =
X
i,j
1[W(ϑi, ϑj) ≤ζ{i,j}]δθi,θj
+
X
j,k
1[χjk ≤S(ϑj)](δθj,σjk + δσjk,θj)
+
X
k
1[ηk ≤I](δρk,ρ′
k + δρ′
k,ρk),
(2.1)
for some collection of independent uniformly distributed random variables (ζ{i,j}) in
[0, 1]; some independent unit-rate Poisson processes {(θj, ϑj)} and {(σij, χij)}j, for
i ∈N, on R2
+ and {(ρj, ρ′
j, ηj)} on R3
+.
Deﬁnition 2.5. A graphex process associated with graphex (I, S, W) is the random
adjacency measure Γ of the form given in Eq. (2.1). The graphex process model is
the family (Γs)s∈R+, where Γs(·) = Γ(· ∩[0, s]2).
Remark 2.6. In [VR15] the Kallenberg exchangeable graph was deﬁned as the
random graph with vertex labels in R+ associated with Γ. The deﬁnition of the
graphex process diﬀers slightly, motivated by the use of techniques from the theory
of distributional convergence of point processes, which makes explicit appeal to
the point process structure desirable. It will sometimes be useful in exposition to
conﬂate the graphex process with the associated labeled graph, so statements such
as “the number of edges of Γs” are sensible.
◁
We will often have occasion to refer to the unlabeled ﬁnite graph associated with
a ﬁnite adjacency measure.
Deﬁnition 2.7. Let ξ be a ﬁnite adjacency measure. The unlabelled graph associated
with ξ is G(ξ).
A particularly important case is the graph associated to the size-s graphex process
Γs, which is almost surely ﬁnite. We will have frequent occasion to refer to the
distributions of both the labeled and unlabeled graphs:
Deﬁnition 2.8. Let (Γs)s∈R+ be a graphex process generated by W. The ﬁnite
graphex process distribution with parameters W and s is KEG(W, s) = P(Γs ∈
· | W, s), and KEG(W) = KEG(W, ∞).
The ﬁnite unlabeled graphex process
distribution with parameters W and s is uKEG(W, s) = P(G(Γs) ∈· | W, s).
In order to pass from G(ξ) back to some adjacency measure ξ′ such that G(ξ′) =
G(ξ), we must reintroduce labels. A simple scheme is to produce labels independently
and uniformly in some range:
Deﬁnition 2.9. Let G be an unlabeled graph with edge set E, and let s > 0.
A random labeling of G into [0, s], Lbls(G, {Ui}), is a random adjacency measure
Lbls(G, {Ui}) = P
(i,j)∈E δ(Ui,Uj), where Ui
iid
∼Uni[0, s], for i ∈N. Where there is
no risk of confusion, we will write Lbls(G) for Lbls(G, {Ui}) where Ui
iid
∼Uni[0, t],
for i ∈N, independently of everything else.
Because our notion of consistent estimation is a requirement of distributional
convergence, the distributions of these random labelings will play a large role.
Clearly, the distribution of Lbls(G) is a measurable function of G and s.


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
10
Deﬁnition 2.10. We write embed(G, s)(·) = P(Lbls(G) ∈·) for the distribution of
Lbls(G). When G is itself random, a random embedding of G into [0, s] is deﬁned
by embed(G, s) = P[Lbls(G)|G].
We typically think of graphex processes as deﬁning a nested collection of R+-
labeled graph valued random variables (Γs)s∈R+. In modeling situations where the
labeling is irrelevant, it is natural to instead look at the (countable) collection of all
distinct graph structures taken on by (Γs)s∈R+; this is the graph sequence associated
with Γ. We now turn to formally deﬁning the graph sequence associated with an
arbitrary adjacency measure ξ. To that end, deﬁne E : R+ →N by
E(s) = 1
2ξ[0, s]2
for s ∈R+.
(2.2)
In the absence of self loops, E(s) is the number of edges present between vertices
with labels in [0, s]. In general, the jumps of E correspond with the appearance of
edges.
Deﬁnition 2.11. Let ξ be an adjacency measure. The jump times of ξ, written as
τ(ξ), is the sequence τ1, τ2, . . . of jumps of E in order of appearance.
Note that the map ξ 7→τ(ξ) is measureable. Intuitively, τ1, τ2, . . . are the sample
sizes at which edges are added to the unlabeled graph associated with the adjacency
measure.
Let χs denote the operation of restricting an adjacency measure to those vertices
with labels in [0, s], in the sense that χsξ(·) = ξ(· ∩[0, s]2). We now formalize the
sequence of all distinct unlabeled graphs associated with (χsξ)s∈R+:
Deﬁnition 2.12. The graph sequence associated with ξ, written G (ξ), is the sequence
G(χτ1ξ), G(χτ2ξ), . . . , where τ1, τ2, . . . are the jump times of ξ.
3. Sampling
Γr, a graphex process of size r, may be generated from Γs, a graphex process of
size s > r, by restricting Γs to [0, r]2. In this section we show that this restriction
has a natural relation to p-sampling: G(Γr) may be generated as an r/s-sampling of
G(Γs).
The ﬁrst result we need is that random labelings preserve the law of exchangeable
adjacency measures. Intuitively, the labels of the size-s graphex process can be
invented by labeling each vertex i.i.d. Uni[0, s].
Lemma 3.1. Let s > 0 and let Γs be a size-s graphex process generated by W.
Then, KEG(W, s) = E[embed(G(Γs), s)].
Proof. It suﬃces to show that Lbls(G(Γs))
d= Γs.
Suppose Γs is generated as in Eq. (2.1). For simplicity of exposition, suppose
that the generating graphex is (0, 0, W), and the associated latent Poisson process
is Πs. Let {θ′
i}i∈N
iid
∼Uni[0, s], and let Π′
s = {(θ′
i, ϑi) : (θi, ϑi) ∈Πs}. By a
property of the Poisson process, Π′
s
d= Πs. Let Γ′
s be a size-s graphex process
generated using the same latent variables as Γs, but with Π′
s replacing Πs. Then,
by construction, Γ′
s
d= Lbls(G(Γs)). Moreover, Γ′
s is distributed as a size-s graphex
process, so Γ′
s
d= Γs.
An essentially identical argument proves the result for a graphex process generated
by the full graphex.
□


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
11
The main sampling result is:
Theorem 3.2. Let W be a graphex, let s > 0 and r ∈[0, s], let Gs ∼uKEG(W, s),
and let Gr be an r/s-sampling of Gs. Then, Gr ∼uKEG(W, s).
Proof. Let ξs = Lbls(Gs). It is an obvious consequence of Lemma 3.1 that ξs is
equal in distribution to a size-s graphex process generated by W. Let ξr be the
restriction of ξs to [0, r]2, so G(ξr) ∼uKEG(W, r). Each vertex of ξs has a label
in [0, r] independently with probability r/s; thus, G(ξr)
d= Gr.
□
4. Estimation with known sizes
This section explains our estimation results for the case where the observations
are (G1, s1), (G2, s2), . . . , where Gk = G(Γsk) for some graphex process Γ generated
according to a graphex W and some sequence sk ↑∞in R+. We consider both the
case of an arbitrary non-random divergent sequence and the case where the sizes are
taken to be the jumps of the graphex process (that is, the sizes at which new edges
enter the graph), in which case we denote the sequence as τ1, τ2, . . . As motivated
in the introduction, our notion of estimation is formalized as:
Deﬁnition 4.1. Let W1, W2, . . . be a sequence of graphexes. Write Wn →GP W
as n →∞when, for all s ∈R+, it holds that uKEG(Wn, s) →uKEG(W, s) weakly
as n →∞.
The goal of estimation is: given a sequence of observations (G1, s1), (G2, s2), . . . ,
produce
ˆW(Gk,sk) : R2
+ →[0, 1]
(4.1)
such that ˆW(Gk,sk) →GP W as k →∞, where the convergence may be almost sure
or merely in probability.
The main result of this section is that the dilated empirical graphons ˆW(Gk,sk) →GP
W for (G1, s1), (G2, s2), . . . generated by a graphex W; i.e. the dilated empirical
graphon is a consistent estimator for W.
We now turn to an intuitive description of the broad structure of the argument.
Conditional on Gk, let ξk = Lblsk(Gk) and let embed(Gk, sk) be the distribution of
ξk conditional on Gk. The ﬁrst convergence result, Theorem 4.3, is that, almost
surely, the random distributions embed(Gk, sk) converge weakly to L(Γ) = KEG(W).
That is, for almost every realization of a graphex process, the point processes deﬁned
by randomly labeling the observed ﬁnite graphs converge in distribution to the
original graphex process. The analogous statement in the i.i.d. sequence setting is
that, given some (X1, X2, . . . ) where Xk
iid
∼P, and σn a random permutation on
[1, . . . , n], the random distributions P(Xσn(1), . . . , Xσn(n) ∈· | X1, . . . , Xn) converge
weakly almost surely to P((X1, X2, . . . ) ∈·) as n →∞.
The convergence in distribution of the point processes on R2
+ is equivalent to
convergence in distribution of the point processes restricted to [0, r]2 for every
ﬁnite r ∈R+. This perspective lends itself naturally to the interpretation of the
limit result as a qualitative approximation theorem: intuitively, P(ξk([0, r]2 ∩·) ∈
· | Gk) approximates KEG(W, r), with the approximation becoming exact in the
limit r/sk →0. This perspective also makes clear the ﬁrst critical connection
between estimation and sampling: conditional on Gk, G(ξk([0, r]2 ∩·)) has the same
distribution as an r/sk-sampling of Gk.


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
12
The second key observation is that, conditional on Gk, a sample from uKEG( ˆW(Gk,sk), r)
may be generated by sampling Poi(r/skv(Gk)) vertices with replacement from Gk
and returning the induced edge set. The second step in the proof is to show that
this sampling scheme is asymptotically equivalent to r/sk-sampling in the limit of
sk ↑∞; this is the role of Lemmas 4.5 and 4.7.
Theorem 4.8 then puts together these results to conclude that, almost surely,
KEG( ˆW(Gk,sk)) →KEG(W) weakly as k →∞. Some additional technical rigmarole
is required to show that this also gives convergence of the (unlabeled) random
graphs. This later convergence is the main result of this section, and is established
in Theorem 4.12.
4.1. Convergence in Distribution of Random Embeddings. This subsection
uses results from the theory of distributional convergence of point processes to show
that, almost surely, embed(Gk, sk) →uKEG(W, ∞) weakly as k →∞.
We will need the following deﬁnition and technical lemma: A separating class for
a locally compact second countable Hausdorﬀspace S is a class U ⊂S such that for
any compact open sets with K ⊂G there is some U ∈U with K ⊂U ⊂G.
Lemma 4.2. Let φ, φ1, φ2, . . . be simple point processes on a locally compact second
countable Hausdorﬀspace S. If
φn(U)
d
−→φ(U), n →∞
(4.2)
weakly for all U in some separating class for S then
φn
d
−→φ, n →∞
(4.3)
weakly.
Proof. By [Kal01, Thms. 16.28 and 16.29], it suﬃces to check that P(φn(U) = 0) →
P(φ(U) = 0) and that lim supn P(φn(U) > 1) ≤P(φ(U) > 1). Because φn(U) is a
non-negative integer a.s., both conditions are implied by φn(U)
d
−→φ(U).
□
Theorem 4.3. Let Γ be a graphex process generated by a non-trivial graphex W, let
s1, s2, . . . be some sequence in R+ such that sk ↑∞as k →∞and let Gk = G(Γsk)
for all k. Then embed(Gk, sk) →KEG(W) weakly almost surely.
Proof. For each k ∈N, conditional on Gk, let ξk be a point process with law
embed(Gk, sk). Note that Γ ∼KEG(W). Observe that the collection U of ﬁnite
unions of rectangles with rational end points is a separating class for R2
+. Further,
ξk is simple for all k ∈N, as is Γ. Thus by Lemma 4.2, to show the claimed result
it will suﬃce to show that, for all U ∈U, P(ξk(U) ∈· | Gk) →P(Γ(U) ∈·) weakly
as k →∞.
Fix U. To establish this condition we ﬁrst show that for all bounded continuous
functions f, it holds that limk→∞E[f(ξk(U)) | Gk] = E[f(Γ(U))] a.s. Let F−s be
the partially labelled graph derived from Γ by forgetting the labels of all nodes with
label θi < s. Take r ∈R+ large enough so that U ⊂[0, r]2. Then for sk > r,
E[f(Γ(U)) | F−sk] = E[f(ξk(U)) | Gk].
(4.4)
Deﬁne Ut = U + (t, t) for t ∈R+ and let
X(r)
s
=
1
s −r
ˆ s−r
0
f(Γ(Ut))dt.
(4.5)


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
13
Observe that for s such that t ≤s −r, the joint exchangeability of Γ implies
E[f(Γ(Ut)) | F−s] = E[f(Γ(U)) | F−s].
(4.6)
Moreover, by the linearity of conditional expectation, for s > r, it holds that
E[X(r)
s
| F−s] = E[f(Γ(U)) | F−s].
A standard result [Dur10, Ex. 5.6.2] shows that limk→∞E[X(r)
sk | F−sk] = E[X(r)
∞|
F−∞] a.s. if X(r)
sk →X(r)
∞a.s. and there is some integrable random variable that
dominates X(r)
sk for all k; the second condition holds because f is bounded. Notice
that Yt = f(Γ(Bt)) is a stationary stochastic process. Moreover, it’s easy to see from
the graphex process construction that Yt and Yt′ are independent whenever |t−t′| > r,
so (Yt) is mixing. The ergodic theorem then gives limk→∞X(r)
sk = E[f(Γ(U))] a.s.
This means
lim
k→∞E[f(ξk(U)) | Gk] →E[f(Γ(U))] a.s.,
(4.7)
as promised.
For l ∈Z+, let fl(·) = 1[· ≤l], let A(U)
l
, for each U ∈U, be the set on which
lim
k→∞E[fl(ξk(U)) | Gk] = E[fl(Γ(U))]
(4.8)
and let AU = T
l A(U)
l
. We have shown that P(A(U)
l
) = 1, and so P(AU) = 1
and on AU it holds that limk→∞P(ξk(U) ∈· | Gk) = P(Γ(U) ∈·) weakly. Let
A = T
U∈U AU, then P(A) = 1 and on A it holds that
lim
k→∞P(ξk(U) ∈· | Gk) = P(Γ(U) ∈·)
(4.9)
weakly for all U ∈U, completing the proof.
□
We need to do a little bit more work to show convergence in the case where the
observations are taken at the jumps of the graphex process.
Theorem 4.4. Let Γ be a graphex process generated by a non-trivial graphex W,
and let τ1, τ2, . . . be the jump times of Γ. Let Gk = G(Γτk) for each k ∈N. Then
embed(Gk, τk) →uKEG(W, ∞) weakly almost surely as k →∞.
Proof. For each k ∈N, let ξk be a point process with law embed(Gk, τk).
As in the proof of Theorem 4.3, to establish the claim it suﬃces to show that, for
all bounded continuous functions f and all rectangles U, it holds that
lim
k→∞E[f(ξk(U)) | Gk, τk] = E[f(Γ(U))] a.s.
(4.10)
Let F−s be as in proof of Theorem 4.3. It is clear that F−τk ⊂F−τ(k−1) for all
k. Because U ⊂[0, r]2 for some ﬁnite r and τk ↑∞a.s. as k →∞it holds that
lim
k→∞E[f(Γ(U)) | F−τk] = lim
k→∞E[f(ξk(U)) | Gk, τk] a.s.
(4.11)
Applying reverse martingale convergence to the l.h.s. we conclude the r.h.s. exists
a.s.
It remains to identify the limit. To that end, we will deﬁne a coupling between
the counts on test set U at a subsequence of the jump times and the counts on U at
some deterministic sequence, which is known to converge to the desired limit. Let
sk = Pk
n=1
1
n, let {τkj} be a subsequence of the jump times deﬁned such that at
most one point in {τkj} lies in [sl, sl+1) for all l and deﬁne skj to be the subsequence


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
14
of {sk} such that skj is the largest value in {sk} that is smaller than τkj. Intuitively,
this gives a random subsequence of the jump times and a random subsequence of
{sk} such that the points skj and τkj become arbitrarily close as j →∞. For each
j ∈N, let Gs
j = G(Γskj ), and let Gτ
j = G(Γτkj ).
By construction, Gs
j ⊂Gτ
j . Label the vertices of Gτ
j as 1, . . . , v(Gτ
j ) such that
1, . . . , v(Gs
j) is the vertex set of Gs
j.
Let ξ(s,j) = Lblskj (Gs
j), and let ξ(τ,j) =
Lblτkj (Gτ
j ). The occupancy counts of the test may then sampled according to:
(1) V1, . . . , Vv(Gτ
j )
iid
∼Uni[0, 1]
(2) ξ(τ,j)(U) = |{(vi, vj) ∈e(Gτ
j ) : (Viτkj, Vjτkj) ∈U}|
(3) ξ(s,j)(U) = |{(vi, vj) ∈e(Gs
j) : (Viskj, Vjskj) ∈U}|
By construction, Gτ
j \Gs
j is a star; call the center of this star c. Choosing r such
that U ⊂[0, r]2, it is clear that if Vcτkj /∈[0, r] then ξ(τ,j)(U) ≤ξ(s,j)(U) under
this coupling. The occupancy counts are the number of edges in random induced
subgraphs given by including each vertex with probability
r
τkj and
r
skj respectively.
This perspective makes it clear that, conditional on c not being included when
sampling from Gτ
j , the counts will be equal as long as no vertices of the induced
subgraph of Gs
j are “forgotten” when the inclusion probability is reduced to
r
τkj . The
probability that Viskj ∈[0, r] but Viτkj /∈[0, r] is
r
τkj skj (τkj −skj). Moreover, there
are at most ξ(s,j)(U) vertices in the subgraph sampled from Gs
j so, in particular,
E[ξ(s,j)(U) −ξ(τ,j)(U) | E¯c, Γ] ≤
r
τkjskj
(τkj −skj)ξ(s,j)(U),
(4.12)
where E¯c denotes the event that c is not included in the subgraph sampled from
Gτ
j . Then, denoting the event {ξ(s,j)(U) = ξ(τ,j)(U)} as EU,
P(EU | Γ) ≥(1 −P(E¯c))(1 −P( ¯
EU | Γ, E¯c))
(4.13)
≥(1 −r
τkj
)(1 −
r
τkjskj
(τkj −skj)ξ(s,j)(U).
(4.14)
By construction, τkj −skj ≤
1
kj , so limj→∞
τkj −skj
skj
= 0. In combination with
limj→∞ξ(s,j)(U) = Γ(U) a.s. and the fact that Γ(U) is almost surely ﬁnite, the
inequality we have just derived then implies that
lim
j→∞P(ξ(s,j)(U) ̸= ξ(τ,j)(U) | Γ) = 0 a.s.
(4.15)
In view of Theorem 4.3, we thus have that
lim
k→∞E[f(ξk(U)) | Gk, τk] = E[f(Γ(U))] a.s.,
(4.16)
as required.
□
4.2. Asymptotic Equivalence of Sampling Schemes. As alluded to above, a
key insight for showing that ˆW(Gk,sk) is a valid estimator is that, conditional on
Gk, a graph generated according to uKEG( ˆW(Gk,sk), r) may be viewed as a random
subgraph of Gk induced by sampling Poi( r
sk v(Gk)) vertices from Gk with replacement
and returning the edge set of the vertex-induced subgraph. The correctness of this
scheme can be seen as follows:


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
15
(1) Let Π be the latent Poisson process used to generate a sample from
uKEG( ˆW(Gk,sk), r), as in Theorem 2.4, and let Πr = Π(· ∩[0, r]2). Be-
cause ˆW(Gk,sk) has compact support [0, v(Gk)/sk]2, only Πr restricted to
[0, r] × [0, v(Gk)/sk] can participate in the graph.
(2) Πr restricted to [0, r]×[0, v(Gk)/sk] may be generated by producing Jsk,r ∼
Poi(rv(Gk)/sk) points (θi, ϑj) where, conditional on Jsk,r, θi
iid
∼Uni[0, r]
and ϑi
iid
∼Uni[0, v(Gk)/sk], also independently of each other.
(3) The {0, 1}-valued structure of ˆW(Gk,sk) means that choosing latent values
ϑi
iid
∼Uni[0, v(Gk)/sk] is equivalent to choosing vertices of Gk uniformly
at random with replacement.
Our task is to show that the sampling scheme just described is asymptotically
equivalent to r/sk-sampling of Gk. To that end, we observe that r/sk-sampling
is the same as sampling Bin(v(Gk), r/sk) vertices of Gk without replacement and
returning the induced edge set.
This makes it clear that there are two main
distinctions between the sampling schemes: Binomial vs. Poisson number of vertices
sampled, and with vs. without replacement sampling. This motivates deﬁning three
distinct random subgraphs of Gk:
(1) X(k)
r
: Sample Bin(v(Gk), r
sk ) vertices without replacement and return the
induced edge set
(2) H(k)
r
: Sample Bin(v(Gk), r
sk ) vertices with replacement and return the
induced edge set
(3) M (k)
r
: Sample Poi( r
sk v(Gk)) vertices with replacement and return the in-
duced edge set
The observation that, conditional on Gk, ξk
r
d= Lblr(X(k)
r
) makes the connection
with the previous subsection clear.
Our aim is to show that when r/sk is small the diﬀerent random subgraphs are
all close in distribution. A natural way to encode this is the total variation distance
between their distributions. However, because the distributions are themselves
random (Gk measurable) variables this is rather awkward. It is instead convenient
to work with couplings of the random subgraphs conditional on Gk; this gives a
natural notion of conditional total variation distance. See [Hol12] for an introduction
to coupling arguments.
Although we only need the sampling equivalence for sequences of graphs corre-
sponding to a graphex process, we state the theorems for generic random graphs
where possible.
The following result, which plays a similar role in the estimation theory of
graphons in the dense setting, is simply the asymptotic equivalence of sampling
with and without replacement.
Lemma 4.5. Let G be an almost surely ﬁnite random graph, with e edges and v
vertices. let Xr be a random subgraph of G given by sampling Bin(v(G), r
s) vertices
without replacement and returning the induced edge set, and let Hr be a random
subgraph of G given by sampling Bin(v(G), r
s) vertices with replacement and returning
the induced edge set. Then there is a coupling such that
P(Hr ̸= Xr | G) ≤2e
r3
s3 + 2 r3
s3v2 + 3 r2
s2v +
r
sv2

(4.17)


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
16
Moreover, specializing to the graphex process case, with H(k)
r
and X(k)
r
deﬁned as
above, under the same coupling,
P(H(k)
r
̸= X(k)
r
| Gk)
p−→0,
(4.18)
as k →∞. Further, if τ1, τ2, . . . are the jump times of Γ then taking sk = τk for all
k ∈N, it holds that under this coupling
P(H(k)
r
̸= X(k)
r
| Gk, τk)
p−→0,
(4.19)
as k →∞.
Proof. Given G, we may sample Xr according to the following scheme:
(1) Sample Ks,r ∼Bin(v, r
s)
(2) Sample a list L = (L1, L2, . . . , LKs,r) of vertices from G without replacement
(3) Return the edge set of the induced subgraph given by restricting G to L
Given G, we may sample Hr similarly, except we use a list sampled with replace-
ment; we couple Hr and Xr by coupling with and without replacement sampling of
the vertex list. The following sampling scheme for a list ˜L returns a list that, given
G, has the distribution of a length Ks,r list of vertices sampled with replacement
from G. Given G we sample ˜L according to:
(1) Sample L as above
(2) ˜L1 = L1
(3) For j = 1 . . . Ks,r, set ˜Lj = Lj with probability 1 −j−1
v . Otherwise, sample
˜Lj uniformly at random from {L1, . . . , Lj−1}.
Hr is then sampled by returning the edge set of the induced subgraph given by
taking ˜L as the vertex set.
Evidently, under this coupling, Xr = Hr as long as
(1) Every entry of L where L ̸= ˜L does not participate in an edge in Xr
(2) Every entry of ˜L where L ̸= ˜L does not participate in an edge in Xr
Call the number of entries violating the ﬁrst condition F1 and the number of entries
violating the second condition F2, and let N be the total number of entries where
L, ˜L diﬀer. Observe that when Ks,r > 0, almost surely,
E[F1 | v(Hr), N, Ks,r, G] = v(Xr)
Ks,r
N
(4.20)
E[F2 | v(Hr), N, Ks,r, G] = v(Xr)
Ks,r
N.
(4.21)
Further observe that because the sites where the lists disagree are chosen without
reference to the graph structure it holds that v(Xr) and N are independent given
G and Ks,r, so
E[F1 + F2 | v(Xr), Ks,r, G] = 2v(Xr)
Ks,r
E[N | Ks,r, G].
(4.22)
Moreover, almost surely,
E[N | Ks,r, G] =
Ks,r
X
j=2
j −1
v
(4.23)
= 1
2v (K2
s,r −Ks,r).
(4.24)


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
17
Using Markov’s inequality along with the observation that
P(Xr ̸= Hr | Ks,r < 2) = 0,
(4.25)
and K2
s,r −Ks,r ≤K2
s,r on Ks,r ≥2, Eq. (4.24) implies that, almost surely,
P(Xr ̸= Hr | v(Xr), Ks,r, G) ≤E[F1 + F2 | v(Xr), Ks,r, G]
(4.26)
≤Ks,r
v
v(Xr).
(4.27)
To prove the ﬁrst assertion of the theorem statement, we now observe that
v(Xr) ≤2e(Xr) and E[e(Xr) | s, Ks,r, G] ≤e
K2
s,r
v2
(since each edge is included with
marginal probability at most
K2
s,r
v2 ), so it holds almost surely that
P(Xr ̸= Hr | s, G) ≤e 2
v3 E[K3
s,r | G]
(4.28)
= 2e(r3
s3 −3 r3
s3v + 2 r3
s3v2 + 3 r2
s2v −3 r2
s2v2 +
r
sv2 ).
(4.29)
To prove the second assertion of the theorem statement we apply Eq. (4.27) to
the graph Gk sampled at rate r/sk, so
P(X(k)
r
̸= H(k)
r
| v(X(k)
r
), Ksk,r, Gk) ≤Ksk,r
v
v(X(k)
r
).
(4.30)
Markov’s inequality with E[
Ksk,r
v(Gk) | Gk] = r/sk implies that, given Gk,
Ksk,r
v(Gk)
p−→0 as
k →∞. Further, by Theorem 4.3 and the observation that X(k)
r
d= G(ξk(· ∩[0, r]2))
where ξk ∼embed(Gk, sk), it holds that v(X(k)
r
)
d
−→v(Γr) a.s. as k →∞. Since
the integrability conditions on graphexes guarantee that v(Γr) is almost surely ﬁnite,
we have
Ksk,r
v(Gk)v(X(k)
r
)
p−→0,
(4.31)
as k →∞and this implies,
P(X(k)
r
̸= H(k)
r
| v(X(k)
r
), Ksk,r, Gk)
p−→0,
(4.32)
as k →∞. Now,
P(X(k)
r
̸= H(k)
r
| Gk) = E[P(X(k)
r
̸= H(k)
r
| v(X(k)
r
), Ksk,r, Gk) | Gk],
(4.33)
and P(X(k)
r
̸= H(k)
r
| Gk) is bounded by 1 for all k, so the second claim follows by
the dominated convergence theorem for conditional expectations, [Dur10, Thm. 5.9].
The proof of the ﬁnal claim goes through mutatis mutandis as the proof of the
second assertion, subject to the observations that τk ↑∞a.s., that we must condition
on τk for each k, and that Theorem 4.4 should be used in place of Theorem 4.3.
□
Remark 4.6. In the case that W = (0, 0, W) and W is integrable, it holds that
v(Gk) = Ω(sk) a.s. and e(Gk) = Θ(s2
k) a.s. [BCCH16, Props. 2.18 and 5.2], in which
case the rate from the ﬁrst part of the above lemma is O(r3/sk). Note that in
this case, the convergence in probability may be replaced by convergence almost
surely. This lemma is in fact the only component of the proof where a weakening of
almost sure convergence is necessary, so (as remarked below), whenever almost sure
convergence holds for the equivalence of with and without replacement sampling,
almost sure convergence holds for the main estimation result.
◁


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
18
It remains to show that the Poi(r/skv(Gk)) and Bin(v(Gk), r/sk) samplings are
asymptotically equivalent. Note that the rate (v(Gk)/sk) at which the empirical
graphon is dilated guarantees that the expected number of vertices sampled according
to each scheme is equal; this is the reason that this rate was chosen.
Lemma 4.7. Let G be an almost surely ﬁnite random graph with v vertices. Let Hr
be a random subgraph of G given by sampling Bin(v, r
s) vertices with replacement
and returning the induced edge set, and let Mr be a random subgraph of G given
by sampling Poi(v r
s) vertices with replacement and returning the induced edge set.
Then there is a coupling such that
P(Hr ̸= Mr | G) ≤r
s a.s.
(4.34)
Proof. Conditional on G, Hr may be sampled by:
(1) sample Ks,r ∼Bin(v, r/s) vertices with replacement from G;
(2) return the edge set of the induced subgraph.
Conditional G, Mr may be sampled by:
(1) sample Js,r ∼Poi(r v
s) vertices with replacement from G.
(2) return the edge set of the induced subgraph.
Comparing the two sampling schemes, it is immediate that there is a coupling
such that
P(Hr ̸= Mr | G) ≤P(Ks,r ̸= Js,r | G).
(4.35)
Note that E[Ks,r | G] = E[Js,r | G]. The approximation of a sum of Bernoulli
random variables by a Poisson with the same expectation as the sum is well studied:
if X1, . . . , Xl are independent random variables with Bern(pi) distributions such
that λ = Pl
i=1 pi and T ∼Poi(λ) then there is a coupling [Hol12, Sec. 5.3] such
that P(T ̸= Pl
i=1 Xi) ≤1
λ
Ps
i=1 p2
i . This implies that there is a coupling of Ks,r
and Js,r such that
P(Ks,r ̸= Js,r | G) ≤r
s,
(4.36)
completing the proof.
□
4.3. Estimating W. We now combine our results to show that the law of the
graphex process generated by the empirical graphex converges to the law of a
graphex process generated by the underlying W.
There is an immediate subtlety to address: Section 4.1 deals with convergence
in distribution of point processes (i.e., labeled graphs), and Section 4.2 deals with
convergence in distribution of unlabeled graphs. We ﬁrst give the main convergence
result for the point process case.
In order to state this result compactly it is
convenient to metrize weak convergence. To this end, we recall that the space of
boundedly ﬁnite measures may be equipped with a metric such that it is a complete
separable metric space [DVJ03, Eqn. A.2.6]. Let dp(·, ·) be the Prokhorov metric on
the space of probability measures over boundedly ﬁnite measures induced by the
aforementioned metric. Then dp(·, ·) metrizes weak convergence: i.e., for a sequence
of boundedly ﬁnite random measures {Πn} it holds that Πn
d
−→Π as n →∞if and
only if dp(L(Πn), L(Π)) →0 as n →∞.


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
19
Theorem 4.8. Let Γ be a graphex process generated by non-trivial graphex W and
let s1, s2, . . . be a (possibly random) sequence in R+ such that sk ↑∞almost surely
as k →∞. Let Gk = G(Γsk) for k ∈N. Suppose that either
(1) (sk) is independent of Γk, or
(2) sk = τk for all k ∈N, where τ1, τ2, . . . are the jump times of Γ.
Then
dp(KEG( ˆW(Gk,sk)), KEG(W))
p−→0,
(4.37)
as k →∞.
Proof. For notational simplicity, we treat the deterministic index case ﬁrst.
For r ∈R+, let embed(Gk, sk)|r denote the probability measure over point
processes on [0, r]2 induced by generating a point process according to embed(Gk, sk)
and restricting to [0, r]2.
By the triangle inequality,
dp(KEG( ˆW(Gk,sk), r), KEG(W, r)) ≤dp(KEG( ˆW(Gk,sk), r), embed(Gk, sk)|r)
(4.38)
+ dp(embed(Gk, sk)|r, KEG(W, r)).
(4.39)
Conditional on Gk and sk, let Xk
r be an r/sk-sampling of Gk and let M k
r be a
random subgraph of Gk given by sampling Poi(v(Gk)r/sk) vertices with replacement
and returning the edge set of the vertex-induced subgraph. By Lemmas 4.5 and 4.7
it holds that there is a sequence of couplings such that
P(M k
r ̸= Xk
r | Gk, sk)
p−→0, k →∞.
(4.40)
Observe that Γk
r
d= Lblr(M k
r , {Ui}) and ξk
r
d= Lblr(Xk
r , {Ui}), where Ui
iid
∼Uni[0, r]
for i ∈N. Here ξk
r is a random labeling of Gk, as in Theorem 4.3. Thus, the
couplings of the unlabeled graphs lift to couplings of the point processes such that
P(Γk
r ̸= ξk
r | Gk, sk)
p−→0, k →∞.
(4.41)
The relationship between couplings and total variation distance then implies
∥KEG( ˆW(Gk,sk), r) −embed(Gk, sk)|r∥TV
p−→0, k →∞,
(4.42)
so also,
dp(KEG( ˆW(Gk,sk), r), embed(Gk, sk)|r)
p−→0, k →∞.
(4.43)
Second, by Theorem 4.3,
dp(embed(Gk, sk)|r, KEG(W))
p−→0, k →∞.
(4.44)
Thus,
dp(KEG( ˆW(Gk,sk), r), KEG(W, r))
p−→0, k →∞.
(4.45)
By [Kal01, Lem. 4.4], convergence in probability for each element of a sequence
lifts to convergence in probability of the entire sequence:
(dp(KEG( ˆW(Gk,sk), 1), KEG(W, 1)), dp(KEG( ˆW(Gk,sk), 2), KEG(W, 2)), · · · )
p−→0, k →∞.
(4.46)


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
20
As the space of boundedly ﬁnite measures on R2
+ is homeomorphic to the space of
sequences of restrictions of boundedly ﬁnite measures to [0, r]2, for r ∈N, it follows
that
dp(KEG( ˆW(Gk,sk)), KEG(W))
p−→0, k →∞.
(4.47)
The same proof mutatis mutandis applies for convergence along the jump times.
The main substitution is the use of Theorem 4.4 in place of Theorem 4.3.
□
Remark 4.9. For graphexes such that e(Gk)/s3
k →0 a.s. and v(Gk) = Ω(sk) the
convergence in probability above can be replaced by almost sure convergence by
replacing all the convergence in probability statements in the body of the proof
by almost sure statements. This class of such graphexes includes all integrable
(0, 0, W).
◁
We now turn to the analogous result for the case of unlabeled graphs generated
by the dilated empirical graphon. We begin with a technical lemma that allows
us to deduce convergence in distribution of unlabeled graphs from convergence in
distribution of the associated adjacency measures. Note that the map taking an
adjacency measure to its associated graph is measurable, but not continuous, and
so this result does not follow from a naive application of the continuous mapping
theorem.
Lemma 4.10. Let S be a discrete space, T a metric space, Q1, Q2, . . . a tight
sequence of probability measures on S, and K a probability kernel from S to T,
such that K is injective when considered as a map from probability measures on
S to probability measures on T. If Q1K, Q2K, . . . converge weakly to QK then
Q1, Q2, . . . converges weakly to Q.
Proof. Assume otherwise. Case 1: Qn →Q′ ̸= Q weakly. By [Kal01, Lem. 16.24]
and the discreteness of S, QnK →Q′K weakly. Since K is injective Q′K ̸= QK, a
contradiction.
Case 2:
Qn does not converge weakly.
Since the sequence Qn is tight it
does converge subsequentially. Choose two inﬁnite subsequences Qi1, Qi2, . . . and
Qj1, Qj2, . . . with respective limits Q′, Q′′ with Q′ ̸= Q′′. But then, by [Kal01,
Lem. 16.24] and the discreteness of S, Q′
ikK →Q′K and Q′′
jkK →Q′′K, hence
Q′K = QK = Q′′K, but K is injective, hence Q′ = Q = Q′′, a contradiction.
□
The motivating application of this last lemma is showing that a sequence of graphs
G1, G2, . . . converge in distribution if and only if their random labelings into [0, s] for
some s also converge in distribution. To parse the following theorem, note that when
G is a ﬁnite random graph, and s ∈R+, then P(G ∈·)embed(·, s) = P(Lbls(G) ∈·).
Lemma 4.11. Let Ks(·) = embed(·, s) for s ∈R+, let Q, Q1, Q2, . . . be probability
measures on the space of almost surely ﬁnite random graphs, let ζk = QkKs and let
ζ = QKs. Then, Qk →Q weakly as k →∞if and only if ζk →ζ weakly as k →∞.
Proof. The forward direction (convergence in distribution of the random graphs
implies convergence in distribution of the random adjacency measures) follows im-
mediately from the discreteness of the space of ﬁnite graphs and [Kal01, Lem. 16.24].
Conversely, suppose that ζk →ζ weakly as k →∞, and, for every n ∈N, let En
be the set of adjacency measures ξ such that ξ([0, s]2) ≤n, i.e., En is the event
that the graph has fewer than n edges. Note that En is a ζ-continuity set by the


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
21
deﬁnition of Ks, and therefore, by weak convergence, ζk(En) →ζ(En) as k →∞
for every n ∈N. Let E′
n be the set of graphs with fewer than n edges. By deﬁnition,
Qk(E′
n) = ζk(En) and Q(E′
n) = ζ(En), hence Qk(E′
n) →Q(E′
n). But E′
n is a ﬁnite
(hence, compact) set, hence {Qk}k∈N is tight. Noting in addition that Ks is injective,
the result follows from Lemma 4.10.
□
The following theorem is a formalization of ˆW(Gk,sk) →GP W as k →∞in
probability:
Theorem 4.12. Let Γ be a graphex process generated by non-trivial graphex W and
let s1, s2, . . . be a (possibly random) sequence in R+ such that sk ↑∞almost surely
as k →∞. Let Gk = G(Γsk) for k ∈N. Suppose that either
(1) (sk) is independent of Γk, or
(2) sk = τk for all k ∈N, where τ1, τ2, . . . are the jump times of Γ.
Then, for every inﬁnite sequence N ⊆N, there exists an inﬁnite subsequence N ′ ⊆N,
such that
ˆW(Gk,sk) →GP W a.s.
(4.48)
along N ′.
Proof. We ﬁrst treat the case (1) where the times (sk) are independent of Γ.
Let N ⊆N be an inﬁnite sequence. Theorem 4.8 implies that there is some inﬁnite
subsequence N ′ ⊆N such that, for all r ∈R+, KEG( ˆW(Gk,sk), r) →KEG(W, r)
weakly almost surely along N ′.
Let r ∈R+ and Kr(·) = embed(·, r). For all k ∈N ′,
uKEG( ˆW(Gk,sk), r)Kr = KEG( ˆW(Gk,sk), r) a.s.,
(4.49)
and uKEG(W, r)Kr = KEG(W, r). Moreover, the graph corresponding to a size-r
graphex process is almost surely ﬁnite. Thus Lemma 4.11 applies and we have that
uKEG( ˆW(Gk,sk), r) →uKEG(W, r) weakly a.s. along N ′. This holds for all r ∈R+,
so we have even that ˆW(Gk,sk) →GP W a.s. along N ′.
The same proof mutatis mutandis applies for convergence along the jump times.
□
Remark 4.13. For graphexes such that e(Gk)/s3
k →0 a.s. and v(Gk) = Ω(sk),
Theorem 4.8 implies that ˆW(Gk,sk) →GP W as k →∞almost surely and not
merely in probability. The class of graphexes with these two properties includes all
graphexes of the form (0, 0, W) for integrable W.
◁
5. Estimation for unknown sizes
We now turn to the case where only the graph structure of the graphex process
is observed, rather than the graph structure and the sizes of the observation.
We ﬁrst show how distinct adjacency measures can give rise to the same graph
sequence. For a measurable map φ : R+ →R+ and adjacency measure ξ, deﬁne ξφ
to be the measure given by ξφ(A × B) = ξ(φ−1(A) × φ−1(B)), for every measurable
A, B ⊆R+. The graph sequences underlying an adjacency measure ξ is invariant to
the action φ 7→ξφ of every strictly monotonic and increasing function φ.
Proposition 5.1. Let ξ be an adjacency measure and let φ : R+ →R+ be strictly
monotonic and increasing. Then G (ξ) = G (ξφ).


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
22
Proof. Let {τk} and {τ φ
k } be the stopping sizes of ξ and ξφ, respectively.
Since φ is strictly monotonic it is also invertible. From this observation it is easily
seen that (θi, θj) is an atom of ξ if and only if (φ(θi), φ(θj)) is an atom of ξφ. It
is then clear that, for all k ∈N, φ(τk) = τ φ
k and, moreover, the graph structure of
{(xi, τk) : (xi, τk) ∈ξ} is equal to the graph structure of (yi, τ φ
k ) : (yi, τ φ
k ) ∈ξφ. That
is, the subgraph of all edges added at the kth step is equal for both graph sequences,
for all k ∈N. Moreover, the ﬁrst entry of each graph sequence is (obviously) equal
to the subgraph of all edges added at the ﬁrst step. The proof is then completed by
induction.
□
If φ is an arbitrary strictly monotonic mapping and ξ is an exchangeable adjacency
measure, it will not generally be the case that ξφ is exchangeable. One family of
mappings that preserves exchangeability is φ(x) = cx, for c ∈R+. We deﬁne the
c-dilation of an adjacency measure ξ to be the adjacency measure ξφ for this map.
Because ξφ is exchangeable there is some graphex W′ that generates it: the next
result shows that the 1
c-dilation of a graphex process corresponds to a c-dilation of
its graphex.
Lemma 5.2. Let Γ be a graphex process with graphex W = (I, S, W). Then the
1
c-dilation of Γ is a graphex process Γ′ with generating graphex W′ = (I′, S′, W ′)
where I′ = c2I, S′(x) = cS(x/c), and W ′(x, y) = W(x/c, y/c).
Proof. Let Γ be a graphex process generated by W with latent Poisson processes
Π, Π1, Π2, . . . , and Πi on R2
+ and R3
+, respectively. Deﬁne f(Π) = {( 1
cθ, cϑ) :
(θ, ϑ) ∈Π}, deﬁne f(Πn) = {( 1
cσ, cχ) : (σ, χ) ∈Πn}, for n ∈N, and deﬁne f(Πi) =
{( 1
cρ, 1
cρ′, c2η) : (ρ, ρ′, η) ∈Πi}. Note that f(Π) and f(Πn), for n ∈N, are unit-rate
Poisson processes on R2
+, and f(Πi) is a unit-rate Poisson process on R3
+. Indeed, the
joint law of (Π, Π1, Π2, . . . , Πi) is the same as that of (f(Π), f(Π1), f(Π2), . . . , f(Πi)).
Then Γ′, the 1
c-dilation of Γ, is the graphex process generated by W′ with latent
Poisson processes f(Π), f(Π1), f(Π2), . . . , f(Πi) reusing the same i.i.d. collection
(ζ{i,j}) in [0, 1] as was used to generate Γ. To see this, note that Γ′ includes edge
( 1
cθi, 1
cθj) if and only if ζ{i,j} ≤W ′(cϑi, cϑj) if and only if ζ{i,j} ≤W(ϑi, ϑj) if and
only if Γ includes edge (θi, θj). Similarly, Γ′ includes edge ( 1
cθi, 1
cσij) if and only
if cχij ≤S′(cϑ) = cS(ϑ) if and only if χij ≤S(ϑ) if and only if Γ includes edge
(θi, σij). Finally, Γ′ includes edge ( 1
cρ, 1
cρ′) if and only if c2η ≤I′ = c2I if and only
if Γ includes edge (ρ, ρ′). Thus Γ′ is a 1
c-dilation of Γ, as was to be shown.
□
Deﬁne the c-dilation of a graphex W to be the graphex W′ deﬁned in the statement
of Lemma 5.2. We have the following consequence:
Theorem 5.3. Let W be a graphex, let W′ be the c-dilation of W for some c > 0,
and let Γ and Γ′ be graphex processes with graphexes W and W′, respectively. Then
G (Γ)
d= G (Γ′).
Proof. Follows immediately from Lemma 5.2 and Proposition 5.1.
□
As a consequence of this result, if the observed data is the graph sequence—that
is, if the size s is unknown—then the dilation of the generating graphex is not
identiﬁable. Therefore, the notion of estimation that we used in the known-size
setting is not appropriate, because it requires Gr(Wn)
d
−→Gr(W) as n →∞for all
sizes r ∈R+.


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
23
The appropriate notion of estimation in this setting is then:
Deﬁnition 5.4. Let W, W1, W2, . . . be a sequence of graphexes, and let Γ, Γ1, Γ2, . . .
be graphex processes generated by each graphex. Write Wk →GS W as k →∞
when G (Γk)
d
−→G (Γ) as k →∞.
Note that this is equivalent to requiring convergence in distribution of the length-l
preﬁxes of the graph sequences, for all l ∈N. Intuitively, a length-l graph sequence
generated by the estimator is close in distribution to a length-l graph sequence
generated by the true graphex, provided the observed graph is large enough. This
perspective explains how a sequence of compactly supported graphexes can estimate
a graphex that is not itself compactly supported. The following is immediate from
Theorem 5.3.
Corollary 5.5. Let W, W1, W2, . . . be a sequence of graphexes, let c, c1, c2, · · · > 0,
and let W c, W c1
1 , W c2
2 , . . . be the corresponding dilations. Then Wk →GS W as
k →∞if and only if Wck
k →GS Wc as k →∞.
Intuitively speaking, Wk →GS W as k →∞demands less than Wk →GP W as
k →∞, because in the former case we don’t need to ﬁnd a correct rate of dilation for
the graphex. The intuition that convergence in distribution of the graph sequence
is weaker than convergence in distribution of (G(Γs))s∈R+ is borne out by the next
lemma:
Lemma 5.6. Let W, W1, W2, . . . be graphexes where W is non-trivial and Wk →GP
W as k →∞. Then Wk →GS W as k →∞.
Proof. Let Γk be graphex processes generated by Wk, and let Γ be generated by W.
For n ∈N, let Gk
n = G(Γk
n), and let Gn = G(Γn).
Consider the sequence Hk
n = (G (Γk
1), G (Γk
2), . . . , G (Γk
n)), where each entry is itself
an a.s. ﬁnite graph sequence and entry j is a preﬁx of entry j+1. Let ηk
n = P(Hk
n ∈·),
and let ηn = P((G (Γ1), G (Γ2), . . . , G (Γn)) ∈·). Intuitively speaking, we are breaking
up the graph sequence of the entire graphex process into the graph sequences up
to size 1, 2, . . . and ηn is the joint distribution of the ﬁrst n of these partial graph
sequences. Our short term goal is to show that ηk
n →ηn weakly as k →∞.
To that end, let G be a ﬁnite graph and consider the random variable
Ln(G) = (G (Lbln(G)([0, j)2 ∩·)))j=1,...,n.
(5.1)
This is a nested sequence of graph sequences given by mapping G to an adjacency
measure on [0, n)2 and then returning the sequence of graph sequences corresponding
to this adjacency matrix at sizes 1, . . . , n. The signiﬁcance of this construction is
that we may use it to deﬁne a probability kernel,
Kn(G, ·) = P(Ln(G) ∈·),
(5.2)
such that that P(Gk
n ∈·)Kn = EKn(Gk
n, ·) = ηk
n and P(Gn ∈·)Kn = EKn(Gn, ·) =
ηn. By assumption, we have Wk →GP W as k →∞, whence Gk
n
d
−→Gn as k →∞.
By the discreteness of the space of ﬁnite graphs and [Kal01, Lem. 16.24] it then
holds that,
P(Gk
n ∈·)Kn →P(Gn ∈·)Kn,
(5.3)
weakly as k →∞. It thus holds by the construction of Kn that
ηk
n →ηn,
(5.4)
weakly as k →∞.


---

SAMPLING AND ESTIMATION FOR (SPARSE) EXCHANGEABLE GRAPHS
24
We now have that an arbitrary length preﬁx of the graph sequence converges
in distribution, when the notion of length is given by the latent sizes. It remains
to argue that this convergence holds for arbitrary preﬁxes in the usual sequence
sense. To that end, we observe that because Eq. (5.4) holds for all n ∈N, by [Kal01,
Thm. 4.29] it further holds that
(G (Γk
1), G (Γk
2), . . . )
d
−→(G (Γ1), G (Γ2), . . . ), k →∞.
(5.5)
There is function f such that, for every locally ﬁnite but inﬁnite adjacency measure
ξ on R2
+, with restrictions ξj to [0, j)2,
f(G (ξ1), G (ξ2), . . . ) = G (ξ)
(5.6)
and f is even continuous because every ﬁnite preﬁx of G (ξ) is determined by some
ﬁnite preﬁx of the left hand side. Extend f to the space of all nested graph sequences
arbitrarily. Note that f is continuous at Γ a.s. because Γ is a.s. locally ﬁnite and
∥W∥1 > 0 implies Γ is a.s. inﬁnite. Hence, the result follows by the continuous
mapping theorem [Kal01, Thm. 4.27].
□
We now turn to establishing the main estimation result for the setting where the
sizes are not included as part of the observation. In this setting, the observations
are increasing sequences of graphs G1, G2, . . . . There are two natural models for the
observations: In one model, Gk = G(Γsk) for some graphex process Γ and (possibly
random, independent, and a.s.) increasing and diverging sequence of sizes s1, s2, . . . .
Alternatively, in the other model, the sequence G1, G2, . . . is the graph sequence
G (Γ) of some graphex process Γ.
A natural estimator is the empirical graphon, ˜WGk, reﬂecting the intuition that
the dilation necessary in the previous section for convergence of the generated
graphex process is irrelevant for convergence in distribution of the associated graph
sequence. Somewhat more precisely, we view the empirical graphon as the canonical
representative of the equivalence class of graphons given by equating graphons that
induce the same distribution on graph sequences. The main result of this section is
that ˜WGk →GS W as k →∞in probability, for either of the natural models for the
observed sequence G1, G2, . . . .
Theorem 5.7. Let Γ be a graphex process generated by some non-trivial graphex
W and let G1, G2, . . . be some sequence of graphs such that either
(1) There is some random sequence (sk), independent from Γ, such that sk ↑∞
a.s. and Gk = G(Γsk) for all k ∈N, or
(2) (G1, G2, . . . ) = G (Γ).
Then, for every inﬁnite sequence N ⊆N, there is an inﬁnite subsequence N ′ ⊆N,
such that
˜WGk →GS W a.s.,
(5.7)
along N ′.
Proof. We prove case (1). Case (2) follows mutatis mutandis, substituting τk for sk.
Let ˆW(Gk,sk) denote the dilated empirical graphon of Gk with observation size
sk. By Theorem 4.12, for every sequence N ⊆N, there is an inﬁnite subsequence
N ′ ⊆N, such that ˆW(Gk,sk) →GP W along N ′. By Lemma 5.6 and W being
non-trivial, this implies that ˆW(Gk,sk) →GS W along N ′. For every k, ˜WGk is some
dilation of ˆW(Gk,sk), hence, the result follows by Corollary 5.5.
□


---

REFERENCES
25
Acknowledgements
The authors would like to thank Christian Borgs, Jennifer Chayes, and Henry
Cohn for helpful discussions. This work was supported by U.S. Air Force Oﬃce of
Scientiﬁc Research grant #FA9550-15-1-0074.
References
[BCCH16]
C. Borgs, J. T. Chayes, H. Cohn, and N. Holden. Sparse exchangeable
graphs and their limits via graphon processes. ArXiv e-prints (Jan.
2016). arXiv:1601.07134 [math.PR].
[BCLS+08]
C. Borgs, J. T. Chayes, L. Lovasz, V. T. Sós, and K. Vesztergombi.
Convergent sequences of dense graphs I: Subgraph frequencies, metric
properties and testing. Advances in Mathematics 219.6 (2008), pp. 1801
–1851.
[CF14]
F. Caron and E. B. Fox. Sparse graphs using exchangeable random
measures. ArXiv e-prints (Jan. 2014). arXiv:1401.1137 [stat.ME].
[DJ08]
P. Diaconis and S. Janson. Graph limits and exchangeable random
graphs. Rendiconti di Matematica, Serie VII 28 (2008), pp. 33–61.
eprint: 0712.2749.
[Dur10]
R. Durrett. Probability: Theory and Examples. 4th Edition. Cambridge
U Press, 2010.
[DVJ03]
D. J. Daley and D. Vere-Jones. An introduction to the theory of point
processes: volume I: elementary theory and methods. Second. Springer
Science & Business Media, 2003.
[Hol12]
F. den Hollander. Probability Theory: The Coupling Method. 2012.
[Kal01]
O. Kallenberg. Foundations of Modern Probability. 2nd. Springer, 2001.
[Kal05]
O. Kallenberg. Probabilistic Symmetries and Invariance Principles.
Springer, 2005.
[Kal90]
O. Kallenberg. Exchangeable random measures in the plane. English.
Journal of Theoretical Probability 3.1 (1990), pp. 81–136.
[Kal99]
O. Kallenberg. Multivariate sampling and the estimation problem for
exchangeable arrays. J. Theoret. Probab. 12.3 (1999), pp. 859–883.
[OR15]
P. Orbanz and D. Roy. Bayesian Models of Graphs, Arrays and Other
Exchangeable Random Structures. Pattern Analysis and Machine In-
telligence, IEEE Transactions on 37.2 (Feb. 2015), pp. 437–461.
[Orb16]
P. Orbanz. Subsampling and invariance in networks. Preprint. 2016.
[VR15]
V. Veitch and D. M. Roy. The Class of Random Graphs Arising
from Exchangeable Random Measures. ArXiv e-prints (Dec. 2015).
arXiv:1512.03099 [math.ST].
University of Toronto, Department of Statistical Sciences, Sidney Smith Hall,
100 St George Street, Toronto, Ontario, M5S 3G3, Canada
University of Toronto, Department of Statistical Sciences, Sidney Smith Hall,
100 St George Street, Toronto, Ontario, M5S 3G3, Canada


---
*Full text extracted from PDF for MemoVoice V3 algorithm training.*
