# Dinkel2020 et al. (2020) — Full Text Extraction

**Source file:** 2020_dinkel2020.pdf
**Pages:** 12
**Extracted by:** MemoVoice research pipeline (PyMuPDF)

---

## Full Paper Content

1
Text-based depression detection on sparse data
Heinrich Dinkel, Student Member, IEEE, Mengyue Wu, Member, IEEE, and Kai Yu, Senior Member, IEEE
Abstract—Previous text-based depression detection is com-
monly based on large user-generated data. Sparse scenarios like
clinical conversations are less investigated. This work proposes
a text-based multi-task BGRU network with pretrained word
embeddings to model patients’ responses during clinical inter-
views. Our main approach uses a novel multi-task loss function,
aiming at modeling both depression severity and binary health
state. We independently investigate word- and sentence-level
word-embeddings as well as the use of large-data pretraining for
depression detection. To strengthen our ﬁndings, we report mean-
averaged results for a multitude of independent runs on sparse
data. First, we show that pretraining is helpful for word-level
text-based depression detection. Second, our results demonstrate
that sentence-level word-embeddings should be mostly preferred
over word-level ones. While the choice of pooling function is less
crucial, mean and attention pooling should be preferred over
last-timestep pooling. Our method outputs depression presence
results as well as predicted severity score, culminating a macro F1
score of 0.84 and MAE of 3.48 on the DAIC-WOZ development
set.
Index Terms—Deep learning, depression detection, multitask
learning, GRU, text-embeddings.
I. INTRODUCTION
D
EPRESSION is an illness that affects, knowingly or
unknowingly, millions of people worldwide. Efﬁcient
and effective automatic depression diagnosis can be of sub-
stantial beneﬁt. However, this is an arduous task since a
variety of complicated symptoms are reported, and subjective
clinical interview is the golden standard. Classic depression
detection at its core is a binary classiﬁcation problem, with
classiﬁers explored from traditional methods like SVM [1],
naive Bayes [2], decision tree [3] and neural networks like
long short term memory (LSTM) [4] and convolutional neural
network (CNN) [5]. Severity prediction can either be seen as
a multi-class classiﬁcation or a regression problem, usually
associated with a psychological questionnaire score like Patient
Health Questionnaire (PHQ)-8/9 [6] or Beck’s Depression
Inventory (BDI) [7]. Though various deep learning models
have been utilized ([8], [9], [10], [11], [12], [13], [14]), the
assessment precision is far from satisfaction. For instance,
compared to regression modeling on other topics, the mean
absolute errors and root mean squared errors reported in severity
predictive tasks are quite high [9]. This again emphasizes the
complexity of depression symptoms and the difﬁculty of precise
predictions.
In particular, text-based depression detection has been
broadly investigated on user-generated data, e.g., a task
in the CLEF eRisk challenge aims at depression severity
prediction from data collected online, including questionnaire
answering [15] and written social media texts [16]. Similarly,
CLPsych organized tasks for PTSD and anxiety detection
Mengyue Wu and Kai Yu are the corresponding authors.
from user-generated texts [17]. Initial studies using text-based
approaches for early depression detection on the eRisk dataset
have shown promising performance [18], [19]. Different text
feature sets have been explored, ranging from hand-crafted
feature types such as n-grams, Bag of Words, Linguistic Inquiry
and Word Count (LIWC) [19], Paragraph Vector etc., to neural
word embeddings like Word2Vec [20] which consists of the
Continuous Bag of Words (CBoW) and the Skip-gram models,
fastText [21], [22], as well as GloVe [23]. The advantage of
user-driven datasets is that they can generally lead to larger
training corpora, where deep-learning methods have seen big
success in recent years [24], [25], [26], [27].
This study situates itself from a different angle: how to
detect depression via text in sparse data scenarios. Mainly, we
are interested in conversational data between an interviewee
and a clinical therapist. It should be noted that self-generated
data concerns a large number of users with potentially big
data, while clinical conversations conducted for depression
detection can be very limited. Clinical depression detection has
been explored by a different challenge, namely Audio/Visual
Emotion Challenge (AVEC) [28], on a ﬂagship dataset Distress
Analysis Interview Corpus - Wizard of Oz (DAIC-WOZ) [29],
[30], which includes video, audio along with its text transcripts.
This published dataset only includes 107 participants; thus,
the training scheme, feature selection, and pooling methods
greatly differ from those in a large data setting. However,
conversational data can potentially reveal more information
on the participant’s linguistic ability and cognitive function,
therefore can provide a different angle towards depression
detection. Hence, we would like to investigate robust depression
prediction based on conversational text, with much less data
compared to previous text sources.
A. Contribution
This paper mainly aims at robust depression detection on
sparse data: we ﬁrst examine text embeddings at the word- and
sentence- level then compare embeddings with and without
pretraining; Second, we analyze different pooling methods that
match the embeddings. Lastly, we adopt 5-Fold cross-validation
in all our experiments. Our results are reported on average as
well as best performances.
Accordingly, our main contributions include:
• An innovative multi-task model design, combining binary
depression detection with severity prediction.
• Investigating the usage of pretrained word/sentence em-
beddings to alleviate sparse-data depression detection
problems.
• Investigating the performance difference between word-
level and sentence-level embedding
• Providing analyses on different pooling functions that best
match each respective front-end text-embedding.
arXiv:1904.05154v3  [cs.LG]  8 Jul 2020


---

2
Healthy
Depressed
0
5
10
15
20
PHQ8 Score
Training PHQ8 score distribution
Healthy
Depressed
0
5
10
15
20
25
PHQ8 Score
Development PHQ8 score distribution
Fig. 1: DAIC-WOZ training and development data PHQ-8 score distribution. Each dot represents a respective patients’ score.
The rest of the paper is organized as follows: an overview
of the task and its related work is ﬁrstly provided in Section II,
followed by a detailed layout of our multi-task sequence mod-
eling approach for word/sentence-level depression detection
in Section III. Then in Section IV, we introduce the utilized
dataset and specify our training and evaluation framework.
The proposed approach is then evaluated, and results are
displayed in Section V. Conclusions and future work are drawn
in Section VI.
II. BACKGROUND AND RELATED WORK
A. Conversational Depression Datasets
To date, a number of research groups and hospitals are
dedicated to publishing better quality and larger quantities of
depression datasets for a review see [31]. However, publicly
available conversational datasets appropriate for incorporating
machine learning methods are surprisingly limited. The one
most broadly used is DAIC-WOZ [29], [30], which encom-
passes 50 hours of data collected from 189 clinical interviews
from a total of 142 patients. Two labels are provided for
each participant: a binary diagnosis of depressed/healthy and
the patient’s eight-item Patient Health Questionnaire score
(PHQ-8) metric [32]. Low PHQ-8 scores represent a healthy
patient, while high PHQ-8 scores represent possible depression
symptoms. Thirty speakers within the training (28 %) and
12 within the development (34 %) set are classiﬁed to have
depression (binary value is set to 1). This database was
previously used for the AVEC 2017 [28] challenge, stating that
scores larger than 10 are considered to be depressed. However,
in reality, any questionnaire could merely work as a reference,
and a clinical interview is the golden standard [33].
We analyzed the dataset to understand the difﬁculties
involved when modeling this task. Overall, three observations
can be made about depression detection from a data perspective:
1) the dataset itself is relatively insufﬁcient; 2) the PHQ-8
distribution of training and development sets are quite different,
ie., some scores are only seen in the training data; 3) the
depression state and PHQ-8 score are correlated, but one
characteristic does not necessarily predict the other.
B. Depression Detection Methods
Automatic depression detection research can either predict
the classiﬁcation results or a severity score, to associate with the
mental state label and PHQ-8 score. The depression presence is
a binary classiﬁcation model, predicting the healthy/depressed
state of a given speaker. F1 score, precision, and recall are
presented as the performance results. Different classiﬁcation
methods have been tested out over the last few decades
with different feature extraction modiﬁcations. The severity
prediction task is usually regressed with questionnaire scores,
which is assessed by the mean absolute error (MAE) and
root mean square error (RMSE). Classiﬁcation and regression
are traditionally seen as substitutions of each other. However,
though they are closely correlated, there is still a distinction
between the two. Thus, our work ﬁrstly adopted a multi-task
training method to model classiﬁcation and regression tasks at
the same time, details of which will be shown in Section III.
C. Text-based detection
Many text-based depression detection studies stem from
social media content, combined with its accompanying pic-
ture [34] or by itself [19], [18], [25]. Conversational text
generated from clinical interviews is rarely examined on its
own, commonly investigated as semantic features or linguistic
features along with other behavioral features [8]. [10] ex-
ploited word representations with Global Vectors for Word
Representation (GloVe), following a high-level feature learning
method. Since a clinical conversation involves questions
and answers from different parties, they separated semantic
information into content and context analysis. It concluded
that the semantic analysis of dialogue scripts via text-based
features is the most promising depression detection method
compared with other modalities. [8] compared different ways
of modeling a conversation with the combination of audio and
text features. In speciﬁc, they compared context-dependent,
context-free, and sequence modeling methods using Word2Vec
as their word embeddings. A recent study [12] employed a
hierarchical attention mechanism to model textual information
at both word and sentence levels, aiming to connect word-level


---

3
representations with sentence-level ones. The work used a GRU
network and GloVe as its textual embeddings, outputting the
binary prediction probability as its results.
As previously noted, clinical conversational depression
datasets are sparse. Thus word/sentence embeddings trained
from scratch might not be able to understand the context
and represent the word/sentence effectively. At the same
time, clinical interviews for depression detection are mostly
consisted of regular conversations, with seldom use of medical-
speciﬁc terms. Recently, general-purpose text-embeddings
such as ELMo [35] and BERT [36], become popular due
to their performance on many NLP benchmarks. Compared to
GloVe [23], which is the only pretrained word embeddings used
in previous depression detection tasks [10], [12], BERT and
ELMo are believed to be more context-aware. Therefore, the use
of pretrained contextual sentence embeddings is investigated
in the current work for their usage in depression detection.
III. APPROACH
In this section, we detailed our novel approach of using a
multi-task setting to model depression presence and severity
prediction tasks together, with the use of only text features for
neat real-world applications.
A. Multi-task modeling
Prior work on depression detection usually splits the tasks of
depression presence detection (binary classiﬁcation) [37] and
severity score prediction (regression with PHQ-8 score) [38].
However, as discussed previously, the two characteristics are
correlated, but one cannot necessarily predict the other. Hence,
both information sources are essential in order to ascertain the
patients’ state.
ℓbce (x, y) = −[y · log x + (1 −y) · log(1 −x)]
(1)
ℓhub (x, y) =
(
0.5(x −y)2,
if |x −y| < 1
|x −y| −0.5,
otherwise
(2)
We thus propose a multi-task setting to combine the
classiﬁcation and regression tasks. Two outputs are constructed,
one directly predicts the binary outcome of a participant being
depressed, the other outputs the estimated PHQ-8-score. We
opt to use a combination of binary cross entropy (BCE, for
classiﬁcation, Equation (1)) and huber loss (for regression,
Equation (2)) in our work. The Huber loss can be seen as
a compromise between mean average error (MAE, L1) and
mean square error (MSE, L2), resulting in a robust behavior
to outliers. While in theory, ℓbce + ℓhub is a reasonable choice
for a loss function, in practice ℓhub dominates ℓbce, meaning
that the model is likely to focus on regression rather than the
binary classiﬁcation.
ℓ(xr, xc, yr, yc) =
(3)
(1 −w)ℓbce(σ(xc), yc) + wℓhub(xr, yr)
To alleviate this problem, we introduce a ﬁxed weight factor
w, to create the convex combination loss ℓ(Equation (3)).
During training, w is set to 0.1. Here, xr represents the
regressive model output, xc represents the binary model output,
σ is the sigmoid function, yr is the PHQ-8 score, and yc is
the binary ground truth.
B. Pooling method
Since labels for this task are only given per interview,
meaning after a sequence of questions and answers, a pooling
layer is required to remove all time-variance to a single vector
representation z and evaluate the entire dialogue. Pooling
methods can be sub-categorized into hidden-level and output-
level approaches. Hidden-level pooling reduces an intermediate
representation, e.g., the BGRU output (O), while output-level
reduces per-timestep probability predictions to one, e.g., after
a soft-max layer. In literature, it has been observed that hidden-
level pooling, speciﬁcally in closely related tasks such as sound
event detection and speaker veriﬁcation is superior to output-
level ones [39], [40], [41], [42], [43]. We, therefore, focus on
hidden-level pooling methods. This paper models depression
detection as a sequence of text-embeddings X = [x1, . . . , xT ],
either on word- or sentence-level and their corresponding
BGRU representations O = [o1, . . . , oT ], where xt represents
a text-embedding at time t.
Name
Function
Time
zT = oT
Mean
zmean = 1
T
PT ot
Max
zmax = maxt(o1:T )
Attention
zatt = PT
t αtot
TABLE I: Pooling functions utilized in this work.
Previous text-based work in [8], [9] solely relied on the
last-timestep (zT ), further referred to as time-pooling, or mean-
pooling (zmean) methods as the response/query representation.
However, [44] has shown that time-pooling is only sub-optimal
since the network belief changes over time. In this work, we
investigate the usage of four different pooling functions, seen
in Table I. All approaches with the exception of attention
are parameter-free, which is potentially helpful in sparse data
scenarios. The individual attention weights (αt) are estimated
given the concatenated forward and backward hidden states
from the BGRU model at time t:
αt =
evot
P
j evoj
Here v is the trainable time-independent attention weight vector.
In addition to the novel multi-task approach and attention
pooling method stated above, our proposed architecture in this
work is a commonly used bidirectional gated recurrent unit
(BGRU) neural network structure (see Figure 2). After each
BGRU layer, we apply a recurrent dropout with a probability
of 20 %. In our initial experiments, we also investigated long
short term memory (LSTM) networks. Even though the best
performance achieved is on par with BGRU models, average
performance is signiﬁcantly worse, likely due to the additional
number of parameters in a BLSTM model. The source code is
publicly available.1
1https://gitlab.com/Richy/text-based-depression-detection


---

4
Fig. 2: Proposed model architecture. The output of the last
layer are two values, one for the regression (xr, PHQ-8) and
one for classiﬁcation (xc). The architecture consists of 3 layer
BGRU model with 128 hidden units.
C. Text embeddings
In previous depression detection research, context-free word
embeddings are usually used, either trained from scratch
with Word2Vec [45] or a simple pretrained word embedding
GloVe [23]. The most commonly used Word2Vec/Doc2Vec
models are usually trained on a shallow, two-layer deep neural
network architecture. While Word2Vec aims to capture the
context of a speciﬁc sentence, it only considers the surrounding
words as its training input; therefore, it does not capture the
intrinsic meaning of a sentence.
Pretraining: Since depression data is hard to come by,
using a model pretrained on large text corpora could help
alleviate data sparsity problems. In our work, we mainly focus
on analyzing pretrained word-embeddings against their from-
scratch counterparts.
Word and Sentence-embeddings: Text-embeddings can
be extracted on multiple, abstract levels such as character,
sub-word, word, sentence, and paragraph (Doc2Vec). Tradi-
tional methods such as Word2Vec are generally extracted on
word-level, meaning that each word is represented by a D
dimensional feature vector. Here, our focus is to compare
word-level embeddings to sentence-level ones. Our assumption
is that word-level embeddings are unﬁt to model depression
detection in a sequential fashion since each word contains little
information about the entire context of a session. Moreover, the
sequence length of the feature X on word-level is much larger
than on sentence-level. We, therefore, propose to compare
two variations of our utilized word-level features. Sentence-
level Word2Vec/fastText embeddings (Word2Vec (S)/fastText
(S)) from a sentence j are extracted from their word-level
representations wt as:
xj = 1
Tj
Tj
X
t=1
wt
where Tj is the number of words within that sentence. In
contrast, modern text-embeddings such as ELMo and BERT
are context-sensitive, meaning they produce at least sentence-
level representations. Recently, models like BERT [36] and
ELMo [35] made use of the self-attention mechanism and
LSTMs to build context-sensitive sentence representations.
ELMo and BERT models pretrained on large corpus such as
Wikipedia are publicly available, therefore, can be effectively
used as high-level feature extractors. Both models generate
embeddings for a word based on the context it appears in,
thus produces slight variations for each word occurrence.
Subsequently, they require to be fed an entire sentence before
generating an embedding. Therefore, they fundamentally differ
from traditional Word2Vec embeddings, which can create a
sentence embedding as an average of all word embeddings.
Four different text embeddings are thus experimented with:
Word2Vec, fastText, ELMo, and BERT:
Word2Vec A Word2vec [20] model was trained with the
hyper-parameters dimension D = 300, minimum word count
5, number of iterations 5, using hierarchical softmax as well
as skip-ngram. For training from scratch (), the entire DAIC-
WOZ [28] text corpus was utilized. For experiment utilizing
pretrained () embeddings, an up-to-date Wikipedia dump
(16 GB) was utilized as the dataset. The entire process is
implemented using the gensim library [46].
fastText Another popular text-embedding is fastText, which
different from Word2Vec is sub-word based (e.g., consecutive
sequences of characters). A fastText [21], [22] model was
trained with the hyper-parameters dimension D = 300, mini-
mum word count 5, number of iterations 8, using hierarchical
softmax as well as skip-ngram. Like Word2Vec, training from
scratch used the entire DAIC-WOZ dataset, while fastText
pretraining was done on Wikipedia.
ELMo Pretrained ELMo embeddings are utilized in this
work. The high-level ELMo representation is based on low-
level character-level inputs; thus, in theory, ELMo should be
able to handle out-of-vocabulary (OOV) words better than
other approaches, speciﬁcally non-verbal information such as
“laughter”. ELMo uses a three-layer bidirectional structure with
1024 nodes in each layer. The model was pretrained on the 1
billion word benchmark dataset. We used the average of all
three layer embeddings as our sentence representation.
BERT Currently, multiple BERT versions, each of different
size are commonly available [36]. Here, we utilize the basic
12 layers, 768 hidden size representation, Bert-Base model as
our standard BERT representation model. An embedding can


---

5
0
2500
5000
7500
Count
i
and
to
a
um
it
the
that
uh
you
Healthy
1-gram
0
500
1000
Count
('you', 'know')
('i', "'m")
('it', "'s")
('do', "n't")
('i', 'do')
('<', 'laughter')
('laughter', '>')
('and', 'i')
('i', 'was')
('it', 'was')
2-gram
0
200
400
600
Count
('<', 'laughter', '>')
('i', 'do', "n't")
('<', 'sigh', '>')
('do', "n't", 'know')
('you', 'know', 'i')
('a', 'lot', 'of')
('i', "'m", 'not')
('i', "'ve", 'been')
('i', 'did', "n't")
('when', 'i', 'was')
3-gram
0
100
200
Count
('i', 'do', "n't", 'know')
('<', 'clears', 'throat', '>')
('um', '<', 'sigh', '>')
('<', 'laughter', '>', 'i')
('i', 'do', "n't", 'like')
('<', 'deep', 'breath', '>')
('you', 'know', 'it', "'s")
('i', 'do', "n't", 'really')
('it', "'s", 'it', "'s")
('i', 'do', "n't", 'have')
4-gram
0
1000
2000
3000
Count
i
and
to
um
a
it
the
that
's
uh
Depressed
0
200
400
Count
('i', "'m")
('it', "'s")
('do', "n't")
('i', 'do')
('i', 'was')
('<', 'laughter')
('laughter', '>')
('you', 'know')
('and', 'i')
('a', 'lot')
0
100
200
Count
('<', 'laughter', '>')
('i', 'do', "n't")
('a', 'lot', 'of')
('<', 'sigh', '>')
('do', "n't", 'know')
('i', "'m", 'not')
('i', "'ve", 'been')
('you', 'know', 'i')
('a', 'little', 'bit')
('i', 'think', 'i')
0
25
50
75
Count
('i', 'do', "n't", 'know')
('<', 'clears', 'throat', '>')
('i', 'do', "n't", 'have')
('i', 'do', "n't", 'really')
('it', "'s", 'it', "'s")
('uh', '<', 'sigh', '>')
('and', 'i', 'do', "n't")
('do', "n't", 'know', 'i')
('<', 'laughter', '>', 'i')
('that', "'s", 'about', 'it')
Fig. 3: Top-10 Training data n-grams. Top row: Healthy patients; Bottom row: Depressed patients. Best viewed in color.
be extracted from each layer. The ofﬁcial Bert-Base model
has been pretrained on the Wikipedia dataset. The penultimate
BERT model layer was used to extract a single 768 dimensional
sentence embedding. A maximum sequence length of 125 was
set in order to reduce memory consumption.
Level
Embedding
Dimension
Word
Word2Vec (W)
300
fastText (W)
300
Sentence
Word2Vec (S)
300
fastText (S)
300
BERT
768
ELMo
1024
TABLE II: Text-embedding dimensions utilized in this work.
All utilized features with regards to their input dimension
D can be seen in Table II.
IV. EXPERIMENTS
A. Experimental Setting
Dataset Data was acquired from the publicly available WOZ-
DAIC [29], [30] corpus, which encompasses three major media:
video, audio, and manually transcribed text data. All modalities
are manually labeled, meaning that the available data is of
higher quality than, e.g., automatically transcribed data by a
machine. This also means that our work can be seen as a
possibly optimal performance towards automatic depression
detection since real-world approaches will require automatic
labeling, thus likely having erroneous transcriptions/labels.
Prior work on this dataset generally focuses on utilizing
modality fusion methods [8], [9], [10], and it is suggested
that a critical factor for depression detection is the addition
of semantic text information. An evaluation subset was also
published, yet labels for the evaluation are not available.
Therefore all experiments were validated on the development
subset. We ﬁrst analyze the data regarding any connection
between depressed and healthy patients in terms of the provided


---

6
50
100
150
200
250
300
350
400
# Sent
Train
100
150
200
250
300
350
Dev
Healthy
Depressed
0
1000
2000
3000
4000
# Words
Healthy
Depressed
500
1000
1500
2000
2500
3000
Fig. 4: Sentence and word length distribution between the
training (left) and development (right) dataset for depressed
and healthy patients.
transcripts. The n-gram counts for the training subset can be
seen in Figure 3. Multiple observations from the n-grams can
be made: 1) The difference in content words between healthy
and depressed patients is little. 2) Three- and four-grams both
contain a high count of behavioral states, e.g., laughter, sign,
clears throat. 3) Most n-grams do not contain meaningful
information, e.g., “I don’t know, I don’t have, I do”.
Data preprocessing Our approach used a sequential modeling
method, which processes the patients’ response in succession
without any regard to interactions within the questioner. The
raw text was ﬁrstly preprocessed, where tailing blanks were
removed, and every letter was set to be lowercase. The prepro-
cessed training set contains an overall 16895, the development
set 6674 written sentences. The training set contains 107, the
development set 35, patients. The average number of sentences
in each subset are 157 and 190 for training and development
subset respectively (see Figure 4).
Our sequential modeling approach treats an entire paragraph
spoken by a patient as a single sample. Thus the training
data only contains 107 samples. Meta information such as
<laughter>or <sigh>are possibly helpful to the model, thus
were kept. The training and development dataset distribution
regarding sentences and words can be seen in Figure 4. It
can be observed that the training set distributions for the
number of words as well as the number of sentences are
similar for depressed as well as healthy patients. However, in
the development set, the depressed patient distribution largely
shifts towards much longer sentences and higher words counts.
In order to ascertain reproducible and meaningful results,
all experiments utilize a stratiﬁed 5-Fold cross-validation
scheme [47]. For each experiment, we ﬁrst divide the 107
available training-data samples (patients) into 86 (80%) samples
used for model training and cross-validate (cv) on the rest 21
samples (20%). Due to stratiﬁcation, we assure that each folds’
binary state distribution is roughly similar. The seed generating
each fold is ﬁxed for all experiments, meaning that the main
difference between consecutive runs is the model parameter
initialization.
Neural network training Training was done by running
Adam [48] optimization for at most 200 epochs. The initial
learning rate was set to be 0.004, which was reduced by a factor
of 10 if the cross-validation loss did not improve for at most
3 epochs. Early stopping was utilized, whereas training stops
if no loss improvement has been seen for at most 10 epochs.
The model producing the lowest loss on the held-out cross-
validation set was chosen for evaluation on the development
(dev) set.
Regarding data handling, padding was applied on batch-level
by padding zeros towards the longest utterance within a batch.
In order to avoid any inﬂuence stemming from the padded
sequences, all pooling functions are implemented using a mask,
which neglects padded elements during computation of z. A
batch-size of 32 was utilized in this work. Due to the inherent
imbalance between depressed and healthy samples (≈2 : 1
ratio), random oversampling over the minority class (depressed
samples) was utilized. Further prevention of over-ﬁtting was
done by assuring a balanced sample distribution (1:1) within
each batch. Recurrent weights were initialized by the uniform
Xavier [49] method, where samples were drawn from [−β, β],
where β =
q
6
Win+Wout and biases were set to zero. Initial
weights regarding the attention parameter v were drawn from
a normal distribution N(0, 0.05). Pseudo-random generator
seeds for each respective experiment were ﬁxed regarding
weight initialization and data sampling. Deep learning models
generally require a large data corpus in order to work. Since
the available amount of data can be considered insufﬁcient,
the parameter initialization, as well as the choice of hyper-
parameters, is vital. In order to circumvent this problem [8]
proposed to grid-search for every possible hyper-parameter
aiming to ascertain a proper conﬁguration. In this work, a
hyper-parameter search was also conducted; thus the outlined
parameters can be considered optimal in our setting.
Evaluation metric For classiﬁcation, accuracy (Acc), macro
precision (pre), and recall (rec) scores are used to calculate the
F1-score. In terms of regression, the mean average error (|x−y|)
and root mean square error (
q
(x −y)2) are used between the
model prediction x and the ground truth PHQ-8 score y. Our
primary metrics for most experiments are F1, Acc and MAE.
Our secondary metrics, additionally use precision, recall and
RMSE. Note that previous work obtained the classiﬁcation
performance by thresholding the PHQ-8 regression result with
a value of 10, while our approach decouples classiﬁcation


---

7
F1
MAE
Acc
Fold
Data






1
cv
0.36
0.35
4.19
4.28
0.55
0.52
dev
0.42
0.40
5.65
5.68
0.56
0.55
2
cv
0.34
0.46
4.65
4.48
0.52
0.58
dev
0.35
0.49
5.63
5.28
0.52
0.60
3
cv
0.33
0.38
3.82
3.80
0.50
0.56
dev
0.33
0.40
5.66
5.59
0.50
0.56
4
cv
0.33
0.50
5.31
4.23
0.53
0.66
dev
0.33
0.52
5.60
5.15
0.52
0.62
5
cv
0.35
0.44
6.32
5.76
0.55
0.64
dev
0.35
0.48
5.69
5.28
0.53
0.61
Avg
cv
0.34
0.42
4.86
4.50
0.53
0.59
dev
0.36
0.45
5.67
5.39
0.53
0.58
TABLE III: Comparison between Word2Vec embeddings
trained from scratch () on the provided DAIC-WOZ data
and pretrained () on the Wikipedia dataset. Values represent
the average performance (µf(ρ)) for each fold. Best in bold.
and regression performance. For all experiments, if not further
speciﬁed, we report average scores after 100 runs on each fold.
B. Comparison Methods
Text settings Apart from sequence modeling, two other
different settings [8] are widely used:
Context-free modeling uses each response of the participant
as an independent sample, without information about the
question, nor the time it was asked.
Context-dependent modeling requires the use of question-
answer pairs, where each sample consists of a question asked
and its corresponding answer [50].
V. RESULTS
In this section, we provide our results and aim to interpret
the data. Our baseline pooling method for the following
experiments is mean-pooling.
A. Pretrained vs. from scratch on word-embeddings
Our ﬁrst experiment aims to investigate the use of pretrained
word-level embeddings (here ) for depression detection.
Word2Vec and fastText are chosen as our representative
embeddings (see Table II). Embeddings from scratch (here )
refer to training a Word2Vec/fastText model on the provided
DAIC dataset. In order to simplify the results, we only utilize
macro-F1, MAE, and accuracy as our main metrics. This
experiment uses mean-pooling and each reported metric ρ
having value r of fold f and run k is calculated as:
µf(ρ) = 1
K
K=100
X
k=1
rk
f(ρ)
The Word2Vec results can be seen in Table III and fastText
results in Table IV.
The results show that speciﬁcally, MAE scores between cv
and dev datasets largely differ. This is likely due to the great
difference in sentence length and word counts between cv (train)
and development subsets (see Figure 4). Similar discrepancies
between cv and development sets have also been reported [50].
F1
MAE
Acc
Fold
Data






1
cv
0.36
0.35
4.17
4.22
0.56
0.51
dev
0.38
0.40
5.65
5.66
0.55
0.54
2
cv
0.36
0.42
4.64
4.58
0.53
0.58
dev
0.37
0.44
5.63
5.53
0.53
0.58
3
cv
0.37
0.45
3.82
3.80
0.56
0.61
dev
0.36
0.41
5.63
5.66
0.54
0.57
4
cv
0.34
0.40
5.24
4.86
0.53
0.57
dev
0.34
0.41
5.68
5.53
0.52
0.54
5
cv
0.34
0.38
6.29
6.02
0.52
0.60
dev
0.34
0.40
5.68
5.56
0.51
0.57
Avg
cv
0.35
0.39
4.83
4.69
0.53
0.57
dev
0.35
0.41
5.58
5.65
0.53
0.65
TABLE IV: Comparison between fastText embeddings trained
from scratch () on the provided DAIC-WOZ data and
pretrained () on the Wikipedia dataset. Values represent the
average performance (µf(ρ)) for each fold. Best in bold.
Further, both utilized front-ends (Word2Vec/fastText), improve
across all utilized metrics when using pretrained embeddings.
Speciﬁcally, Word2Vec signiﬁcantly enhances its performance
on the development set (F1 0.36 →0.45, MAE 5.67 →
5.39) when pretraining is utilized. fastText also improves
across the F1 and Acc metrics, yet its performance gains are
smaller than Word2Vec ones (F1 0.35 →0.41). Interestingly,
fastTexts’ MAE results on the development set slightly drops
when pretraining is utilized. However, since its classiﬁcation
performance, especially regarding accuracy, is greatly enhanced,
we believe that fastText could potentially be a better feature
for classiﬁcation than regression.
Since conclusions drawn from these experiments consistently
demonstrate performance gain beneﬁting from pretrained
text embeddings, the following experiments by default use
pretrained embeddings.
B. Word- vs. sentence-level embeddings
Regarding different text embeddings, another critical ques-
tion is, do sentence-level features enhance performance com-
pared to word-level ones? Our BGRU model is capable of
sequence modeling, while lacking at word-level processing,
since each word contributes little information to the entire
session’s entire context. A popular approach to enhance partial
information modeling is chunking [51]. In our case, we see
chunking as the word-level averaging within a sentence, as
previously introduced in Section III-C.
Our experimental results comparing word-level (W) and
sentence-level (S) embeddings can be seen in Table V and
Table VI. For both front-end features, considerable improve-
ments are observed when using sentence-level features instead
of word-level ones. Word2Vec performance regarding F1 (0.45
→0.53), accuracy (0.58 →0.63) and MAE (4.50 →4.11)
all signiﬁcantly beneﬁt from sentence-level features. Like our
initial pretraining experiments using fastText, its performance
also improves, yet it is less notable than Word2Vec. We,
therefore, continue to compare sentence-level features towards
their usage in depression detection.


---

8
F1
MAE
Acc
Fold
Data
W
S
W
S
W
S
1
cv
0.35
0.33
4.28
4.35
0.52
0.49
dev
0.40
0.36
5.68
5.71
0.51
0.55
2
cv
0.46
0.62
4.48
3.95
0.58
0.51
dev
0.49
0.59
5.28
4.85
0.60
0.67
3
cv
0.38
0.43
3.80
3.72
0.56
0.53
dev
0.40
0.49
5.59
5.29
0.56
0.58
4
cv
0.50
0.64
4.23
3.36
0.66
0.79
dev
0.52
0.64
5.15
4.71
0.62
0.70
5
cv
0.44
0.56
5.76
5.19
0.64
0.73
dev
0.48
0.58
5.28
4.90
0.61
0.67
Avg
cv
0.42
0.51
4.50
4.11
0.59
0.64
dev
0.45
0.53
5.39
5.09
0.58
0.63
TABLE V: Comparison between Word2Vec pretrained embed-
dings on word (W) and sentence (S) level using mean-pooling.
Values represent the average performance (µf(ρ)) for each fold.
Best in bold.
F1
MAE
Acc
Fold
Data
W
S
W
S
W
S
1
cv
0.35
0.33
4.22
4.27
0.51
0.51
dev
0.40
0.36
5.68
5.71
0.51
0.55
2
cv
0.42
0.40
4.58
4.57
0.58
0.56
dev
0.44
0.39
5.53
5.52
0.58
0.56
3
cv
0.45
0.45
3.80
3.74
0.61
0.61
dev
0.41
0.45
5.66
5.48
0.57
0.59
4
cv
0.40
0.41
4.86
4.60
0.57
0.64
dev
0.41
0.47
5.53
5.31
0.54
0.62
5
cv
0.38
0.41
6.02
5.58
0.60
0.64
dev
0.40
0.50
5.56
5.14
0.57
0.63
Avg
cv
0.39
0.39
4.69
4.57
0.57
0.58
dev
0.41
0.42
5.58
5.43
0.55
0.58
TABLE VI: Comparison between fastText pretrained embed-
dings on word (W) and sentence (S) level using mean-pooling.
Each fold is run 100 times and the mean values for all
experiments are reported.
C. Comparing pooling methods
In this experiment we aim to ascertain the usage of the four
proposed pooling functions, namely Attention, Max, Mean, and
Time pooling. We report our ﬁndings (metric ρ) as a global
average (µ) across all runs, independent of folds:
µ(ρ) =
1
FK
F =5
X
f=1
K=100
X
k=1
rk
f(ρ)
The results are displayed in Table VII. Time-pooling can be
seen to be the least favorable pooling method, regardless of
which text embedding is utilized. The other three pooling
methods’ performance are slightly different depending on the
front-end embeddings. Across all utilized pooling functions,
ELMo is seen to provide consistently well performing results
regarding F1 and MAE. Utilizing ELMo with attention, mean or
max pooling results in an average MAE of ≈5.06. Since ELMo
is pretrained on a larger dataset (1 Billion word) compared
to the other embeddings, we believe that this is the main
reason for the performance gains on the development set. BERT
embeddings, while outperforming ELMo ones on the cv dataset,
degrade on the development set. Interestingly, Word2Vec
embeddings with mean-pooling are observed to produce the
lowest MAE error in the held-out cv set. We conclude from
Data
Pooling
Embedding
MAE
F1
Acc
cv
Att
BERT
4.16
0.53
0.69
ELMo
4.31
0.50
0.66
fastText (S)
4.20
0.50
0.67
Word2Vec (S)
4.14
0.53
0.66
Max
BERT
4.21
0.50
0.68
ELMo
4.24
0.52
0.64
fastText (S)
4.49
0.45
0.63
Word2Vec (S)
4.15
0.51
0.62
Mean
BERT
4.21
0.51
0.68
ELMo
4.28
0.51
0.67
fastText (S)
4.57
0.40
0.59
Word2Vec (S)
4.11
0.52
0.65
Time
BERT
4.66
0.43
0.65
ELMo
4.75
0.41
0.64
fastText (S)
4.62
0.42
0.66
Word2Vec (S)
4.62
0.41
0.66
dev
Att
BERT
5.21
0.51
0.64
ELMo
5.08
0.55
0.66
fastText (S)
5.26
0.51
0.63
Word2Vec (S)
5.13
0.53
0.64
Max
BERT
5.14
0.51
0.65
ELMo
5.03
0.59
0.68
fastText (S)
5.39
0.48
0.62
Word2Vec (S)
5.05
0.57
0.66
Mean
BERT
5.10
0.52
0.65
ELMo
5.07
0.54
0.66
fastText (S)
5.43
0.43
0.58
Word2Vec (S)
5.09
0.53
0.63
Time
BERT
6.76
0.34
0.47
ELMo
6.80
0.33
0.46
fastText (S)
6.81
0.33
0.45
Word2Vec (S)
6.80
0.32
0.43
TABLE VII: Comparison between Time, Mean, Attention and
Max pooling methods regarding four utilizied features. The
global average values µ(ρ) for each metric ρ are displayed.
the set of experiments, that the pooling functions are essential,
since by choosing the naive time-pooling strategy, performance
signiﬁcantly drops. Mean, attention and max pooling all seem
to be a viable approach for depression detection.
Result likelihood: A more detailed description of the
F1 results and their signiﬁcance can be seen in Figure 5,
where each run is plotted. Though the F1 scores distribute
differently on the cv and dev sets, our model did not experience
performance downfall on the dev set. Mainly, in terms of most
stable performance, attention can be seen to produce the least
amount of outliers (dots) between the cv and development
sets, while having acceptable performance. FastText is seen
to produce the most outliers using mean, time, and max
pooling. The most consistent results, with the smallest variance,
stem from time-pooling, even though those results are also
the worst. Different from the observed F1 scores, the MAE
distribution in Figure 6 contains overall less extreme outliers.
However according to MAE results, our models are more
prone to overﬁtting as the performance on the dev set sharply
degenerated. Attention and max pooling methods are more
stable than mean and time pooling functions. Consistently, time-
pooling is the least favorable method investigated, regardless
of the text embeddings.
1) Best average experiment: Lastly, we also report our best
average performance (µbest) regarding the F1 score in Table IX


---

9
0.2
0.4
0.6
0.8
1.0
Attention
cv
dev
0.2
0.4
0.6
0.8
1.0
Mean
0.2
0.4
0.6
0.8
1.0
Time
BERT
FastText (S)
Word2Vec (S)
ELMo
0.2
0.4
0.6
0.8
1.0
Max
BERT
FastText (S)
Word2Vec (S)
ELMo
F1 distribution
Fig. 5: F1 result distribution for each single run as a boxplot
for cv and development sets regarding to pooling function and
text-embedding. Higher is better. Best viewed in color.
calculated as:
µbest =
max
k=1,...,100

1
F
F =5
X
f=1
rk
f(F1)


Here, the best pooling method generating the result is also
displayed. As can be seen, for all features except fastText,
mean-pooling indeed provides competitive performance on all
data. The most stable feature is ELMo, which offers excellent
performance on the held-out cv (F1 0.73, MAE 4.26) as well
as development sets (F1 0.73, MAE 4.71).
Lastly, to compare our sequence modeling approach to
previous context-free and context-dependent techniques, we
also provide a single, best-performing score for each method
in Table VIII. Please note that our best performing models are
trained on a single fold of the training dataset, in line with other
3
4
5
6
7
8
Attention
cv
dev
3
4
5
6
7
8
Mean
3
4
5
6
7
8
Time
BERT
FastText (S)
Word2Vec (S)
ELMo
3
4
5
6
7
8
Max
BERT
FastText (S)
Word2Vec (S)
ELMo
MAE distribution
Fig. 6: MAE result distribution for each single run as a boxplot
for cv and development sets regarding to pooling function and
text-embedding. Lower is better. Best viewed in color.
methods. The the best experiment regarding F1 performance
(rbest) is therefore:
rbest =
max
k=1,...,100

max
f=1,...,5 rk
f(F1)

For the classiﬁcation task on whether a patient is depressed
or healthy, it is indicated that our sequence model with
pretrained sentence-level text embeddings using BERT or
Word2Vec, has achieved a macro F1 score of 0.84. Our
sequence approach signiﬁcantly outperforms previous context-
free methods. Speciﬁcally, our sequential approach is seen to
perform equally well as the context-dependent [10] methods
in terms of F1. Therefore, our proposed multi-task framework
approach outperforms other sequential text-based methods. In
terms of regression, our best model using ELMo features
achieves an MAE of 3.78, being the best in its class for se-


---

10
Classiﬁcation
PHQ-8-Regression
Model
Embedding
Setting
Pooling
Pre
Rec
F1
Acc
MAE
RMSE
C-CNN [9]
Word2Vec
Sequence
Time
-
-
-
-
6.16
-
C-CNN [9]
Doc2Vec
Sequence
Time
-
-
-
-
5.81
-
Gauss-Staircase [10]
GloVe (Fusion)
Context-Dep
-
-
-
0.84
-
3.34
4.46
Gauss-Staircase [10]
GloVe
Context-Dep
-
-
-
0.76
-
-
-
BLSTM [8]
Doc2Vec
Context-Free
Time
0.71
0.5
0.59
-
7.02
9.43
BLSTM [8]
Doc2Vec
Sequence
Time
0.57
0.8
0.67
-
5.18
6.38
GRU [12]
GloVe
Sequence
HAtt
-
0.60
0.60
-
-
-
BGRU
BERT
Sequence
Mean
0.85
0.83
0.84
0.86
3.91
5.09
BGRU
ELMo
Sequence
Att
0.87
0.81
0.83
0.86
4.73
5.62
BGRU
fastText (S)
Sequence
Mean
0.82
0.85
0.82
0.83
3.48
4.65
BGRU
Word2Vec (S)
Sequence
Att
0.85
0.83
0.84
0.86
3.79
5.05
TABLE VIII: Evaluation results on the development subset. We compare our approach (bottom) to previous text based approaches
(top). The reported results represent the best achieved results during our experiments for a single fold (80%) of the training data.
Embedding
Pooling
Data
Pre
Rec
F1
Acc
MAE
BERT
Mean
cv
0.82
0.69
0.69
0.80
4.08
dev
0.66
0.56
0.55
0.66
5.19
ELMo
Mean
cv
0.73
0.66
0.66
0.76
4.26
dev
0.73
0.66
0.64
0.72
4.71
fastText (S)
Att
cv
0.69
0.67
0.66
0.78
4.16
dev
0.59
0.62
0.60
0.68
5.06
Word2Vec (S) Mean
cv
0.71
0.71
0.67
0.77
4.00
dev
0.61
0.64
0.61
0.70
4.95
TABLE IX: Best performing pooling function across averaged
5-fold runs (µbest) regarding each embedding. Results on the
dev set utilize the best average performing models on the CV
set. Bold displays best performance on the dev/cv sets.
quential depression modeling. As the results of our experiments
indicated, Doc2Vec largely underperforms in terms of MAE
and F1, compared to ELMo and BERT approaches, which we
think is due to the limited training data available as well as its
incapability to extract a context-dependent vector representation.
One possible reason why general-purpose pretrained word
embeddings are useful for depression detection is that clinical
interview involves similar content to a normal conversation.
Thus knowledge gained from large data pretraining can be
effectively passed down to depression detection.
As can be seen, the classiﬁcation performance difference
between all four utilized features is marginal. While our
proposed method does not outperform [10] in terms of MAE,
we emphasize that the proposed approach is only requiring
patients’ response data to be utilized effectively. Compared
to [9], all our proposed methods have superior (lower) MAE.
Speciﬁcally, note that our average performance in Table VII
also outperforms the best-performing methods in [9], [12]. This
is likely due to our multi-task training scheme and improved
data balancing pipeline, such that the model is prevented from
overﬁtting towards depressed/healty patients.
VI. CONCLUSION
This work proposed the use of multi-task modeling in
conjunction with pretrained sentence embeddings, namely
Word2Vec, fastText, ELMo and BERT, for text-based modeling
depression. A BGRU model with an attention mechanism is
utilized as the classiﬁer. In our initial experiment we aim to
investigate if pretraining on a large, unrelated dataset, is helpful
to depression detection. We observe large performance gains on
both the held-out cross-validation dataset and the DAIC-WOZ
development dataset. Further, we investigate simple sentence
averaging and its use for text-based depression detection. Our
experiments show that indeed sentence-level features should
be preferred over word-level ones. Lastly, we investigate four
pooling functions (mean, max, time, attention) for depression
detection. Our results show that with the exception of time-
pooling all other three approaches perform on average equally
well. Our highest fold-average macro F1 score goes as high
as 0.69 and MAE as low as 4.00. In our ﬁnal submission,
we compare our best achieved models to other methods. Our
proposed model outperforms previous text-based detection
approaches in terms of classiﬁcation and PHQ-8 regression,
culminating in a F1 score of 0.84 and a MAE of 3.48.
Future studies can explore the model interpretations of
modality fusion with speech, which can be compared with
such linguistic results. Further conclusions can be drawn on
whether a model will pay attention to different content since
it has ‘heard’ how the words are spoken.
ACKNOWLEDGMENT
This work has been supported by the Major Program of
National Social Science Foundation of China (No.18ZDA293).
Experiments have been carried out on the PI supercomputer at
Shanghai Jiao Tong University.
REFERENCES
[1] J. F. Cohn, T. S. Kruez, I. Matthews, Y. Yang, M. H. Nguyen, M. T.
Padilla, F. Zhou, and F. De la Torre, “Detecting depression from facial
actions and vocal prosody,” in 2009 3rd International Conference on
Affective Computing and Intelligent Interaction and Workshops.
IEEE,
2009, pp. 1–7.
[2] M. Deshpande and V. Rao, “Depression detection using emotion artiﬁcial
intelligence,” in 2017 International Conference on Intelligent Sustainable
Systems (ICISS).
IEEE, 2017, pp. 858–862.
[3] L. Yang, D. Jiang, L. He, E. Pei, M. C. Oveneke, and H. Sahli, “Decision
tree based depression classiﬁcation from audio video and language
information,” in Proceedings of the 6th International Workshop on
Audio/Visual Emotion Challenge, 2016, pp. 89–96.
[4] M.-H. Su, C.-H. Wu, K.-Y. Huang, and Q.-B. Hong, “Lstm-based text
emotion recognition using semantic and emotional word vectors,” in
2018 First Asian Conference on Affective Computing and Intelligent
Interaction (ACII Asia).
IEEE, 2018, pp. 1–6.
[5] A. H. Orabi, P. Buddhitha, M. H. Orabi, and D. Inkpen, “Deep learning
for depression detection of twitter users,” in Proceedings of the Fifth
Workshop on Computational Linguistics and Clinical Psychology: From
Keyboard to Clinic, 2018, pp. 88–97.


---

11
[6] J. M. Zich, C. C. Attkisson, and T. K. Greenﬁeld, “Screening for depres-
sion in primary care clinics: the ces-d and the bdi,” The International
Journal of Psychiatry in Medicine, vol. 20, no. 3, pp. 259–277, 1990.
[7] S. Gilbody, D. Richards, S. Brealey, and C. Hewitt, “Screening for
depression in medical settings with the patient health questionnaire (phq):
a diagnostic meta-analysis,” Journal of general internal medicine, vol. 22,
no. 11, pp. 1596–1602, 2007.
[8] T. Al Hanai, M. Ghassemi, and J. Glass, “Detecting depression with
audio/text sequence modeling of interviews,” in Proc. Interspeech 2018,
2018, pp. 1716–1720. [Online]. Available: http://dx.doi.org/10.21437/
Interspeech.2018-2522
[9] A. Haque, M. Guo, A. S. Miner, and L. Fei-Fei, “Measuring Depression
Symptom Severity from Spoken Language and 3D Facial Expressions,”
nov 2018. [Online]. Available: http://arxiv.org/abs/1811.08592
[10] J. R. Williamson, E. Godoy, M. Cha, A. Schwarzentruber, P. Khorrami,
Y. Gwon, H.-T. Kung, C. Dagli, and T. F. Quatieri, “Detecting depression
using vocal, facial and semantic communication cues,” in Proceedings of
the 6th International Workshop on Audio/Visual Emotion Challenge, ser.
AVEC ’16.
New York, NY, USA: ACM, 2016, pp. 11–18. [Online].
Available: http://doi.acm.org/10.1145/2988257.2988263
[11] Z. Zhao, Z. Bao, Z. Zhang, J. Deng, N. Cummins, H. Wang, J. Tao, and
B. Schuller, “Automatic Assessment of Depression from Speech via a
Hierarchical Attention Transfer Network and Attention Autoencoders,”
IEEE Journal on Selected Topics in Signal Processing, vol. 14, no. 2,
pp. 423–434, feb 2020.
[12] A. Mallol-Ragolta, Z. Zhao, L. Stappen, N. Cummins, and B. W.
Schuller, “A Hierarchical Attention Network-Based Approach for
Depression
Detection
from
Transcribed
Clinical
Interviews,”
in
Proc. Interspeech 2019, 2019, pp. 221–225. [Online]. Available:
http://dx.doi.org/10.21437/Interspeech.2019-2036
[13] E. Rejaibi, D. Kadoch, K. Bentounes, R. Alfred, M. Daoudi, A. Hadid,
and A. Othmani, “Towards robust deep neural networks for affect and
depression recognition.” arXiv: Human-Computer Interaction, 2020.
[14] X. Ma, H. Yang, Q. Chen, D. Huang, and Y. Wang, “DepAudioNet:
An efﬁcient deep model for audio based depression classiﬁcation,”
in AVEC 2016 - Proceedings of the 6th International Workshop on
Audio/Visual Emotion Challenge, co-located with ACM Multimedia
2016.
New York, New York, USA: Association for Computing
Machinery, Inc, oct 2016, pp. 35–42. [Online]. Available: http:
//dl.acm.org/citation.cfm?doid=2988257.2988267
[15] Losada, David E. and Crestani, Fabio and Parapar, Javier, “erisk 2020:
Self-harm and depression challenges,” in European Conference on
Information Retrieval.
Springer, 2020, pp. 557–563.
[16] ——, “erisk 2017: Clef lab on early risk prediction on the internet:
experimental foundations,” in International Conference of the Cross-
Language Evaluation Forum for European Languages.
Springer, 2017,
pp. 346–360.
[17] G. Coppersmith, M. Dredze, C. Harman, K. Hollingshead, and
M. Mitchell, “Clpsych 2015 shared task: Depression and ptsd on twitter,”
in Proceedings of the 2nd Workshop on Computational Linguistics and
Clinical Psychology: From Linguistic Signal to Clinical Reality, 2015,
pp. 31–39.
[18] M. Trotzek, S. Koitka, and C. M. Friedrich, “Utilizing neural networks
and linguistic metadata for early detection of depression indications in
text sequences,” IEEE Transactions on Knowledge and Data Engineering,
vol. 32, no. 3, pp. 588–601, 2020.
[19] M. Trotzek, S. Koitka, and C. M. Friedrich, “Word embeddings and
linguistic metadata at the CLEF 2018 tasks for early detection of
depression and anorexia,” in CEUR Workshop Proceedings, vol. 2125,
2018. [Online]. Available: http://www.reddit.com/r/depression,
[20] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of
word representations in vector space,” in 1st International Conference
on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA,
May 2-4, 2013, Workshop Track Proceedings, Y. Bengio and Y. LeCun,
Eds., 2013. [Online]. Available: http://arxiv.org/abs/1301.3781
[21] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, “Bag of tricks for
efﬁcient text classiﬁcation,” arXiv preprint arXiv:1607.01759, 2016.
[22] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word
vectors with subword information,” Transactions of the Association for
Computational Linguistics, vol. 5, pp. 135–146, 2017.
[23] J. Pennington, R. Socher, and C. Manning, “Glove: Global vectors for
word representation,” in Proceedings of the 2014 conference on empirical
methods in natural language processing (EMNLP), 2014, pp. 1532–1543.
[24] Q. Kong, Y. Wang, X. Song, Y. Cao, W. Wang, and M. D. Plumbley,
“Source Separation with Weakly Labelled Data: an Approach to Com-
putational Auditory Scene Analysis,” in ICASSP 2020 - 2020 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP).
Institute of Electrical and Electronics Engineers (IEEE), apr
2020, pp. 101–105.
[25] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D.
Plumbley, “PANNs: Large-Scale Pretrained Audio Neural Networks
for Audio Pattern Recognition,” dec 2019. [Online]. Available:
http://arxiv.org/abs/1912.10211
[26] H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, “Vggsound: A Large-
Scale Audio-Visual Dataset,” in ICASSP 2020 - 2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP).
Institute of Electrical and Electronics Engineers (IEEE), apr 2020, pp.
721–725.
[27] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.
Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold, M. Slaney,
R. J. Weiss, and K. Wilson, “CNN architectures for large-scale audio
classiﬁcation,” in ICASSP, IEEE International Conference on Acoustics,
Speech and Signal Processing - Proceedings.
Institute of Electrical and
Electronics Engineers Inc., jun 2017, pp. 131–135.
[28] F. Ringeval, B. Schuller, M. Valstar, J. Gratch, R. Cowie, S. Scherer,
S. Mozgai, N. Cummins, M. Schmitt, and M. Pantic, “Avec 2017:
Real-life depression, and affect recognition workshop and challenge,”
in Proceedings of the 7th Annual Workshop on Audio/Visual Emotion
Challenge, ser. AVEC ’17.
New York, NY, USA: ACM, 2017, pp. 3–9.
[Online]. Available: http://doi.acm.org/10.1145/3133944.3133953
[29] J. Gratch, R. Artstein, G. Lucas, G. Stratou, S. Scherer, A. Nazarian,
R. Wood, J. Boberg, D. DeVault, S. Marsella, D. Traum, A. Rizzo,
and L.-P. Morency, “The Distress Analysis Interview Corpus of human
and computer interviews,” in Proceedings of the Ninth International
Conference on Language Resources and Evaluation (LREC 2014).
Reykjavik, Iceland: LREC, May 2014, pp. 3123–3128. [Online]. Avail-
able: http://ict.usc.edu/pubs/The%20Distress%20Analysis%20Interview%
20Corpus%20of%20human%20and%20computer%20interviews.pdf
[30] D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast, A. Gainer, K. Georgila,
J. Gratch, A. Hartholt, M. Lhommet, G. Lucas, S. Marsella, F. Morbini,
A. Nazarian, S. Scherer, G. Stratou, A. Suri, D. Traum, R. Wood,
Y. Xu, A. Rizzo, and L.-P. Morency, “Simsensei kiosk: A virtual human
interviewer for healthcare decision support,” in Proceedings of the
2014 International Conference on Autonomous Agents and Multi-agent
Systems, ser. AAMAS ’14.
Richland, SC: International Foundation
for Autonomous Agents and Multiagent Systems, 2014, pp. 1061–1068.
[Online]. Available: http://dl.acm.org/citation.cfm?id=2615731.2617415
[31] N. Cummins, S. Scherer, J. Krajewski, S. Schnieder, J. Epps, and T. F.
Quatieri, “A review of depression and suicide risk assessment using
speech analysis,” Speech Communication, vol. 71, pp. 10–49, 2015.
[32] K. Kroenke, T. Strine, R. Spitzer, J. Williams, J. Berry, and A. Mokdad,
“The phq-8 as a measure of current depression in the general population,”
Journal of Affective Disorders, vol. 114, no. 1-3, pp. 163–173, 4 2009.
[33] M. Martin-Subero, K. Kroenke, C. Diez-Quevedo, T. Rangil, M. de An-
tonio, R. M. Morillas, M. E. Lor´an, C. Mateu, J. Lupon, R. Planas
et al., “Depression as measured by phq-9 versus clinical diagnosis as an
independent predictor of long-term mortality in a prospective cohort of
medical inpatients,” Psychosomatic medicine, vol. 79, no. 3, pp. 273–282,
2017.
[34] T. Gui, L. Zhu, Q. Zhang, M. Peng, X. Zhou, K. Ding, and Z. Chen,
“Cooperative multimodal approach to depression detection in twitter,”
in The Thirty-Third AAAI Conference on Artiﬁcial Intelligence, AAAI
2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence
Conference, IAAI 2019, The Ninth AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence, EAAI 2019, Honolulu, Hawaii, USA,
January 27 - February 1, 2019.
AAAI Press, 2019, pp. 110–117.
[Online]. Available: https://doi.org/10.1609/aaai.v33i01.3301110
[35] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and
L. Zettlemoyer, “Deep contextualized word representations,” in Proc. of
NAACL, 2018.
[36] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.
[37] L. Yang, D. Jiang, L. He, E. Pei, M. C. Oveneke, and H. Sahli,
“Decision tree based depression classiﬁcation from audio video
and language information,” in Proceedings of the 6th International
Workshop on Audio/Visual Emotion Challenge, ser. AVEC ’16.
New York, NY, USA: ACM, 2016, pp. 89–96. [Online]. Available:
http://doi.acm.org/10.1145/2988257.2988269
[38] S. Scherer, Z. Hammal, Y. Yang, L.-P. Morency, and J. F. Cohn,
“Dyadic behavior analysis in depression severity assessment interviews,”
in Proceedings of the 16th International Conference on Multimodal
Interaction, ser. ICMI ’14.
New York, NY, USA: ACM, 2014, pp. 112–
119. [Online]. Available: http://doi.acm.org/10.1145/2663204.2663238


---

12
[39] L. Lin, X. Wang, H. Liu, and Y. Qian, “Specialized Decision
Surface and Disentangled Feature for Weakly-Supervised Polyphonic
Sound Event Detection,” IEEE/ACM Transactions on Audio, Speech,
and Language Processing, pp. 1–1, may 2020. [Online]. Available:
http://arxiv.org/abs/1905.10091
[40] C. Kao, M. Sun, W. Wang, and C. Wang, “A comparison of pooling
methods on lstm models for rare acoustic event classiﬁcation,” in ICASSP
2020 - 2020 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), 2020, pp. 316–320.
[41] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, “X-
vectors: Robust dnn embeddings for speaker recognition,” in 2018 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2018, pp. 5329–5333.
[42] E. Variani, X. Lei, E. McDermott, I. L. Moreno, and J. Gonzalez-
Dominguez, “Deep neural networks for small footprint text-dependent
speaker veriﬁcation,” in ICASSP, IEEE International Conference on
Acoustics, Speech and Signal Processing - Proceedings, 2014, pp. 4052–
4056. [Online]. Available: http://ieeexplore.ieee.org/document/6854363/
[43] Y. Wang, J. Li, and F. Metze, “A Comparison of Five Multiple Instance
Learning Pooling Functions for Sound Event Detection with Weak
Labeling,” ICASSP, IEEE International Conference on Acoustics, Speech
and Signal Processing - Proceedings, vol. 2019-May, pp. 31–35, oct
2019. [Online]. Available: http://arxiv.org/abs/1810.09050
[44] S. Mirsamadi, E. Barsoum, and C. Zhang, “Automatic Speech Emotion
Recognition Using Recurrent Neural Networks with Local Attention,”
2017.
[45] Q. Le and T. Mikolov, “Distributed representations of sentences and
documents,” in International conference on machine learning, 2014, pp.
1188–1196.
[46] R. ˇReh˚uˇrek and P. Sojka, “Software Framework for Topic Modelling
with Large Corpora,” in Proceedings of the LREC 2010 Workshop on
New Challenges for NLP Frameworks.
Valletta, Malta: ELRA, May
2010, pp. 45–50, http://is.muni.cz/publication/884893/en.
[47] K. Sechidis, G. Tsoumakas, and I. Vlahavas, “On the stratiﬁcation
of multi-label data,” Machine Learning and Knowledge Discovery in
Databases, pp. 145–158, 2011.
[48] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. [Online]. Available:
http://arxiv.org/abs/1412.6980
[49] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep
feedforward neural networks,” in In Proceedings of the International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS10). Society
for Artiﬁcial Intelligence and Statistics, 2010.
[50] Y. Gong and C. Poellabauer, “Topic modeling based multi-modal
depression detection,” in Proceedings of the 7th Annual Workshop on
Audio/Visual Emotion Challenge.
ACM, 2017, pp. 69–76.
[51] F. Zhai, S. Potdar, B. Xiang, and B. Zhou, “Neural models for sequence
chunking,” CoRR, vol. abs/1701.04027, 2017. [Online]. Available:
http://arxiv.org/abs/1701.04027


---
*Full text extracted from PDF for MemoVoice V3 algorithm training.*
