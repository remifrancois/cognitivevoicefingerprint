# Luz2021 et al. (2021) — Full Text Extraction

**Source file:** 2021_luz2021.pdf
**Pages:** 5
**Extracted by:** MemoVoice research pipeline (PyMuPDF)

---

## Full Paper Content

Detecting cognitive decline using speech only:
The ADReSSo Challenge
Saturnino Luz1, Fasih Haider1, Soﬁa de la Fuente1, Davida Fromm2, Brian MacWhinney2
1Usher Institute, Edinburgh Medical School, The University of Edinburgh, UK
2Department of Psychology, Carnegie Mellon University, USA
{S.Luz, fasih.haider, sofia.delafuente}@ed.ac.uk, {fromm, macw}@andrew.cmu.edu
Abstract
Building on the success of the ADReSS Challenge at Inter-
speech 2020, which attracted the participation of 34 teams from
across the world, the ADReSSo Challenge targets three dif-
ﬁcult automatic prediction problems of societal and medical
relevance, namely: detection of Alzheimer’s Dementia, infer-
ence of cognitive testing scores, and prediction of cognitive de-
cline. This paper presents these prediction tasks in detail, de-
scribes the datasets used, and reports the results of the base-
line classiﬁcation and regression models we developed for each
task.
A combination of acoustic and linguistic features ex-
tracted directly from audio recordings, without human interven-
tion, yielded a baseline accuracy of 78.87% for the AD classi-
ﬁcation task, a root mean squared (RMSE) error of 5.28 for
prediction of cognitive scores , and 68.75% accuracy for the
cognitive decline prediction task.
Index Terms: Cognitive Decline Detection, Affective Comput-
ing, Alzheimer’s dementia, computational paralinguistics
1. Introduction
Dementia is a category of neurodegenerative diseases which en-
tail long-term and usually gradual decrease of cognitive func-
tioning. The main risk factor for dementia is age and, hence,
it is increasingly prevalent in our ageing society. Due to the
severity of the disease, institutions and researchers worldwide
are investing considerably on dementia prevention, early detec-
tion and disease progression monitoring [1]. There is a need for
cost-effective and scalable methods for detection of early signs
of Alzheimer’s Dementia (AD) as well as prediction of disease
progression.
Methods for screening and tracking the progression of de-
mentia traditionally involve cognitive tests such as the Mini-
Mental Status Examination (MMSE) [2] and the Montreal Cog-
nitive Assessment (MoCA) [3]. MMSE and MoCA are widely
used because, unlike imaging methods, they are cheap, quick
to administer and easy to score. Despite its shortcomings in
speciﬁcity in early stages of dementia, the MMSE is still widely
used [4]. The promise of speech technology in comparison to
cognitive tests is twofold. First, speech can be collected pas-
sively, naturally and continuously throughout the day, gathering
increasing data points while burdening neither the participant
nor the researcher.
Furthermore, the combination of speech
technology and machine learning creates opportunities for au-
tomatic screening and diagnosis support systems for dementia.
The ADReSSo Challenge aims to generate systematic evidence
for these promises towards their clinical implementation.
As with its predecessor, the overall objective of the
ADReSSo Challenge is to host a shared task for the systematic
comparison of approaches to the detection of cognitive impair-
ment and decline based on spontaneous speech. As has been
pointed out elsewhere [5, 6], the lack of common, standardised
datasets and tasks has hindered the benchmarking of the various
approaches proposed to date, resulting in a lack of translation of
these speech based methods into clinical practice.
The ADReSSo Challenge provides a forum for researchers
working on approaches to cognitive decline detection based on
speech data to test their existing methods or develop novel ap-
proaches on a new shared standardised dataset. The approaches
that performed best on last year’s dataset [5] employed features
extracted from manual transcripts which were provided along
with the audio data [7, 8]. The best performing method [8] made
interesting use of pause and disﬂuency annotation provided with
the transcripts. While this provided interesting insights into the
predictive power of these paralinguistic features for detection of
cognitive decline, extracting such features, and indeed accurate
transcripts from spontaneous speech remains an open research
issue. This year’s ADReSSo (Alzheimer’s Dementia Recog-
nition through Spontaneous Speech only) tasks provide more
challenging and improved spontaneous speech datasets, requir-
ing the creation of models straight from speech, without manual
transcription, though automatic transcription is encouraged.
The ADReSSo datasets are carefully matched so as to avoid
common biases often overlooked in evaluations of AD detec-
tion methods, including repeated occurrences of speech from
the same participant (common in longitudinal datasets), varia-
tions in audio quality, and imbalances of gender and age distri-
bution. The challenge deﬁnes three tasks:
1. an AD classiﬁcation task, where participants were re-
quired to produce a model to predict the label (AD
or non-AD) for a short speech session.
Participants
could use the speech signal directly (acoustic features),
or attempt to convert the speech into text automatically
(ASR) and extract linguistic features from this automat-
ically generated transcript;
2. an MMSE score regression task, where participants were
asked to create models to infer the patients’ MMSE score
based solely on speech data; and
3. a cognitive decline (disease progression) inference task,
where they created models for prediction of changes in
cognitive status over time, for a given speaker, based on
speech data collected at baseline (i.e. the beginning of a
cohort study).
These tasks depart from neuropsychological and clinical
evaluation approaches that have employed speech and language
[9] by focusing on prediction and recognition using sponta-
neous speech. Spontaneous speech analysis has the potential to
enable novel applications for speech technology in longitudinal,
unobtrusive monitoring of cognitive health [10], in line with the
theme of this year’s INTERSPEECH, “Speech Everywhere!”.
 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted March 27, 2021. 
; 
https://doi.org/10.1101/2021.03.24.21254263
doi: 
medRxiv preprint 
NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.


---

This paper describes the ADReSSo dataset and presents
baselines for all ADReSSo tasks, including feature extraction
procedures and models for AD detection, MMSE score regres-
sion and prediction of cognitive decline.
2. Related work
There has been increasing research on speech technology for
dementia detection over the last decade. The majority of this re-
search has focused on AD classiﬁcation, but some of it targets
MCI detection as well [6]. These objectives are most closely
related with our ﬁrst task, namely, the AD classiﬁcation task.
Such related research includes the best performing models pre-
sented in the ADReSS challenge in 2020. These achieved an
85.45% [7] and 89.6% [8] accuracy in AD classiﬁcation using
acoustic features and text-based features extracted from manual
transcripts. Classiﬁcation based on acoustic features only was
also attempted in [7], and obtained 76.85% accuracy with the
IS10-Paralinguistics feature set (a low dimensional version of
ComParE [11]) and Bag-of-Acoustic-Words (BoAW).
Few works rely exclusively on acoustic features or text fea-
tures extracted through ASR. One of these achieved a 78.7%
accuracy on a subset of the Cookie Theft task of the Pitt dataset,
using different comprehensive paralinguistic feature sets and
standard machine learning algorithms [12]. Another, using the
complete Pitt dataset, obtained 68% accuracy using only vocal-
isation features (i.e. speech-silence patterns) [10]. A classiﬁca-
tion accuracy of 62.3% was reported in a study that used fully
automated ASR features with a different dataset [13].
As regards the second task, regression over MMSE scores,
there is less literature available and most of it has been pre-
sented in recent workshops [6]. Several of these works used the
above mentioned Pitt dataset to extract different linguistic and
acoustic features and predict MMSE scores. A recent study cap-
tured different levels of cognitive impairment with a multiview
embedding and obtained a mean absolute error (MAE) of 3.42
[14]. Another study reported a MAE of 3.1, having relied solely
on acoustic features to build their regression model (a set of 811
features) [15]. Error scores as low as 2.2 (MAE) have been ob-
tained, but relying on non-spontaneous speech data elicited in
semantic verbal ﬂuency (SVF) tasks [16].
Studies addressing the progression task are far less com-
mon. Notable in this category is [17], which incorporated a
comprehensive set of features (i.e. lexicosyntactic, semantic
and acoustic) into Bayesian network with, reporting a MAE
of 3.83 on prediction of MMSE scores throughout different
study visits. Two other studies account for disease progression
in classiﬁcation experiments. One of them extracted speech-
based from the ISLE dataset achieving 80.4% accuracy to detect
intra-subject cognitive changes, that is, to distinguish healthy
participants who remained healthy from those who developed
some kind of cognitive impairment [18]. The second study uses
SVF scores to build a machine learning classiﬁer able to predict
changes from MCI to AD over a 4-year follow-up, with 84.1%
accuracy [19].
3. The ADReSSo Datasets
We provided two distinct datasets for the ADReSSo Challenge:
1. a dataset consisting of speech recordings of Alzheimer’s
patients performing a category (semantic) ﬂuency task
[20] at their baseline visit, for prediction of cognitive de-
cline over a two year period, and
2. a set of recordings of picture descriptions produced by
cognitively normal subjects and patients with an AD di-
agnosis, who were asked to describe the Cookie Theft
picture from the Boston Diagnostic Aphasia Examina-
tion [21, 22].
The recorded data also included speech from different ex-
perimenters who gave instructions to the patients and occasion-
ally interacted with them in short dialogues. No transcripts were
provided with either dataset, but segmentation of the recordings
into vocalisation sequences with speaker identiﬁers [23] were
made available for optional use. The ADReSSo challenge’s par-
ticipants were asked to specify whether they made use of these
segmentation proﬁles in their predictive modelling. Recordings
were acoustically enhanced with stationary noise removal and
audio volume normalisation was applied across all speech seg-
ments to control for variation caused by recording conditions
such as microphone placement.
The dataset used for AD and MMSE prediction was
matched for age and gender so as to minimise risk of bias in the
prediction tasks. We matched the data using a propensity score
approach [24, 25] implemented in the R package MatchIt [26].
The ﬁnal dataset matched according to propensity scores de-
ﬁned in terms of the probability of an instance of being treated
as AD given covariates age and gender. All standardised mean
differences for the age and gender covariates were < 0.001 and
all standardised mean differences for age2 and two-way inter-
actions between covariates were well below .1, indicating ade-
quate balance for the covariates. The propensity score was esti-
mated using a probit regression of the treatment on the covari-
ates age and gender as probit generated a better balanced than
logistic regression. The age/gender matching is summarised
in Figure 1, which shows the respective (empirical) quantile-
quantile plots for the original and balanced datasets. A quantile-
quantile plot showing instances near the diagonal indicates good
balance.
The resulting dataset encompasses 237 audio ﬁles. These
were split into training and test sets, with 70% of instances allo-
cated to the former and 30% allocated to the latter. These parti-
tions were generated so as to preserve gender and age matching.
The dataset for the disease prognostics task (prediction of
cognitive decline) was created from a longitudinal cohort study
involving AD patients. The time period for assessment of dis-
ease progression spanned the baseline and the year-2 data col-
lection visits of the patients to the clinic. The task involves
classifying patients into ’decline’ or ’no-decline’ categories,
given speech collected at baseline as part of a verbal ﬂuency
test. Decline was deﬁned as a difference in MMSE score be-
tween baseline and year-2 greater than or equal 5 points (i.e.
mmse(baseline) −mmse(y2) ≥5). This dataset has a to-
tal of 105 audio recordings split into training and test sets as
with the diagnosis dataset (70% and 30% of recordings, respec-
tively).
4. Data representation
4.1. Acoustic features
We applied a sliding window with a length of 100 ms on the au-
dio ﬁles of the dataset with no overlap and extracted eGeMAPS
features over such frames. The eGeMAPS feature set [27] re-
sulted from an attempt to reduce the somewhat unwieldy fea-
ture sets above to a basic set of acoustic features based on their
potential to detect physiological changes in voice production, as
well as theoretical signiﬁcance and proven usefulness in previ-
 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted March 27, 2021. 
; 
https://doi.org/10.1101/2021.03.24.21254263
doi: 
medRxiv preprint 


---

Figure 1: Quantile-quantile plots for data before (left) and after
matching (right) by age and gender.
ous studies. It contains the F0 semitone, loudness, spectral ﬂux,
MFCC, jitter, shimmer, F1, F2, F3, alpha ratio, Hammarberg
index and slope V0 features, as well as their most common sta-
tistical functionals, totalling 88 features per 100ms frame. We
then applied the active data representation method (ADR) [12]
to generate a data representation using frame level acoustic in-
formation for each audio recording. The ADR method has been
tested previously for generating representations for large scale
time-series data. It employs self-organising mapping to cluster
the original acoustic features and then computes second-order
features over these cluster to extract new features (see [12] for
details). Note that this method is entirely automatic in that no
speech segmentation or diarisation information is provided to
the algorithm.
4.2. Linguistic Features
We used the Google Cloud-based Speech Recogniser for auto-
matically transcribing the audio ﬁles. The transcripts were con-
verted into CHAT format which is compatible with CLAN [28],
a set of programs that allows for automatic analysis of a wide
range of linguistic and discourse structures. Next, we used the
automated MOR function to assign lexical and morphological
descriptions to all the words in the transcripts. Then, we used
two commands: EVAL which creates a composite proﬁle of 34
measures, and FREQ to compute the Moving Average Type To-
ken Ratio [29].
5. Diagnosis baseline
5.1. Task 1: AD Classiﬁcation
The AD classiﬁcation experiments were performed using ﬁve
different methods, namely decision trees (DT, where the leaf
size is optimised through a grid search within a range of 1 to
20), nearest neighbour (KNN, where K parameter is optimised
through a grid search within a range of 1 to 20), linear discrim-
inant analysis (LDA), Tree Bagger (TB, with 50 trees, where
leaf size is optimised through a grid search within a range of 1
to 20), and support vector machines (SVM, with a linear kernel,
where box constraint is optimised by trying a grid search be-
tween 0.1 to 1.0, and a sequential minimal optimisation solver).
The results for accuracy in the AD vs Control (CN) classi-
ﬁcation task are summarised in Table 1. As indicated in bold-
face, the best performing classiﬁer in cross-validation (CV) was
DT, achieving 78.92% and 72.89% accuracy using acoustic and
linguistic features, respectively. On the test set, however, the re-
sults were reversed, with linguistic features producing an over-
all best accuracy of 77.46%, with the SVM classiﬁer. Late fu-
sion of the acoustic and linguistic models improves the accuracy
on the test set further to 78.87% (Figure 2).
Table 1:
Task1:
AD classiﬁcation accuracy on leave-one-
subject out CV and test data.
LDA
DT
SVM
TB
KNN mean (sd)
CV
Acoustic
62.65 78.92 69.28 65.06 65.06 68.19 (6.4)
Linguistic 72.29 72.89 72.89 75.90 65.06 71.81 (4.0)
Test Acoustic
50.70 60.56 64.79 63.38 53.52 58.59 (6.2)
Linguistic 76.06 74.65 77.46 73.24 59.15 72.11 (7.4)
Predicted Class
True Class
Task 1
7
8
28
28
80.0%
77.8%
Cn
Ad
Ad
Cn
Rec.
Pre.
Accu =78.87%
F1
8. 7%
8. 7%
mean_F1  =78.87%
Figure 2: Task 1: Late (decision) fusion of the best results of
acoustic and linguistic models. Precision (Pre) , recall (Rec),
accuracy (Accu) and mean F1 scores are shown on the margins.
5.2. Task 2: MMSE prediction
For this regression task we also used ﬁve types of regression
models: linear regression (LR), DT, with leaf size of 20 and
CART algorithm, support vector regression (SVR, with a ra-
dial basis function kernel with box constraint of 0.1, and se-
quential minimal optimisation solver), Random Forest regres-
sion ensembles (RF), and Gaussian process regression (GP, with
a squared exponential kernel). The regression methods are im-
plemented in MATLAB [30] using the statistics and machine
learning toolbox.
The results are summarised in Table 2. As with classiﬁca-
tion, DT regression outperformed the other models in CV, with
ASR linguistic features outperforming acoustic ADR features.
This trend persisted in the test set, with linguistic features pro-
ducing a minimum RMSE of 5.28 in a SVR model. We then
fused the best results of linguistic and acoustic features and took
 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted March 27, 2021. 
; 
https://doi.org/10.1101/2021.03.24.21254263
doi: 
medRxiv preprint 


---

Table 2: Task2: MMSE score prediction error scores (RMSE)
for leave-one-subject out CV and test data.
LR
DT
SVR
RF
GP
mean (sd)
CV
Acoustic
6.88 6.88
6.96
7.89 6.71 7.06 (0.47)
Linguistic
6.65 5.92
6.42
7.02 6.50 6.50 (0.40)
Test
Acoustic
6.23 6.47
6.09
8.18 6.81 6.75 (0.84)
Linguistic
5.87 6.24
5.28
6.94 5.43 5.95 (0.67)
a weighted mean, ﬁnding the weights through grid search on the
validation results, which resulted in an improvement (6.37) on
the validation dataset. We then used the same weights to fuse
the test results and obtained an RMSE of 5.29 (r = 0.69).
6. Prognosis baseline
6.1. Task 3: prediction of progression
We tested the same classiﬁcation methods used in Task 1 for
the task of identifying those patients who went on to exhibit
cognitive decline within two years of the baseline visit in which
the speech samples used in our models were taken. The acous-
tic and linguistic features were generated as described in Sec-
tion 4.The results of this prediction task are summarised in Ta-
ble 3. As the classes for this task are imbalanced we report aver-
age F1 results rather than accuracy, Once again DT performed
best on CV, but the F1 results for the test set was considerably
lower, reaching only a maximum of 66.67%, for linguistic fea-
tures and 61.02% for acoustic features.
Table 3: Task3: cognitive decline progression results (mean of
F1Score) for leave-one-subject-out CV and test data.
LDA
DT
SVM
TB
KNN
mean (sd)
Val
Acoustic
59.89 84.94 55.64 63.85 65.92 66.05 (11.27)
Linguistic 55.19 76.52 45.24 63.10 55.25 59.06 (11.64)
test
Acoustic
61.02 53.62 40.74 40.74 38.46 46.91 (9.89)
Linguistic 54.29 66.67 40.74 56.56 39.62 51.58 (11.41)
As before, we fused the predictions of the best models for
each feature type, hoping that the diversity of models might im-
prove classiﬁcation. The confusion matrix for the fusion model
is shown in Figure 3. This time, however, decision fusion did
not yield any improvement in accuracy. However, it is noted
that the recall (sensitivity) improved from 40% to 70% for pa-
tients exhibiting cognitive decline.
nd
d
Predicted Class
True Class
Task 3
3
7
7
15
68.2%
70.0%
50.0%
83.3%
nd
d
Pre.
Rec.
F1
Accuracy= 68.75%
mean_F1=66.67%
75.0%
58.3%
Figure 3: Task 3: Decision fusion of the best results of acoustic
and linguistic features on the test set.
7. Discussion
The AD classiﬁcation baseline yielded a maximum accuracy of
78.87% on the test set, through the fusion of models based on
linguistic and acoustic features. Despite the fact that the ASR
transcripts had relatively high word error rates, linguistic fea-
tures contributed considerably to the predictions. The overall
baseline results for this task are in fact comparable to results
obtained for similar picture description data using manual tran-
scripts (see Section 2). DT classiﬁers performed well on the CV
experiments, but accuracy decreased on the test set, indicating
probable overﬁtting. Overall, however, all models proved fairly
robust.
A similar picture was observed in the MMSE regression
task. Linguistic features contributed appreciably to the predic-
tion, even though the transcripts contained many errors. In this
case, however, late fusion only improved the RMSE score in
CV; the test set RMSE remained practically unchanged.
The prognosis task proved to be the most difﬁcult predic-
tion task. The CV results varied considerably among models,
with a standard deviation of 11.64 for the linguistic models.
The test set results were also varied, reaching a maximum F1
score of 66.67%, even when the best model predictions were
fused. Although the acoustic features produced the best classi-
ﬁcation results in CV (F1 = 66.05% vs 59.06% for linguistic
features), these results were not born out by test set evaluation,
suggesting that the acoustic features made the classiﬁers more
prone to overﬁtting. It is possible that this could be mitigated
by training the acoustic feature extractor (ADR) on a larger set
of off-task recordings (data augmentation) and ﬁne tuning the
resulting model on the ADReSSo data.
8. Conclusions
The ADReSSo Challenge is the ﬁrst shared task to target
cognitive status prediction using raw, non-annotated a non-
transcribed speech, and to address prediction of changes in
cognition over time. We believe this moves the speech pro-
cessing and machine learning methods one step closer to the
real-world of clinical applications. A limitation the AD clas-
siﬁcation and the MMSE regression tasks share with most ap-
proaches to the use of these methods in dementia research is
that they provide little insight into disease progression. This
has been identiﬁed as the main issue hindering translation of
these technologies into clinical practice [6] and, hence, preclin-
ical modelling emerges as clear avenue for future research [31].
However, these tasks remain relevant in application scenarios
involving automatic cognitive status monitoring, in combina-
tion with wearable and ambient technology. The addition of the
progression task should open avenues for relevance also in more
traditional clinical contexts.
9. Acknowledgements
This research is funded by the European Union’s Horizon 2020
research programme, under grant agreement 769661, SAAM
project. SdlFG is supported by the Medical Research Council.
10. References
[1] K. Ritchie, I. Carri`ere, L. Su, J. T. O’Brien, S. Lovestone,
K. Wells, and C. W. Ritchie, “The midlife cognitive proﬁles of
adults at high risk of late-onset Alzheimer’s disease: The PRE-
VENT study,” Alzheimer’s & Dementia, vol. 13, no. 10, pp. 1089–
1097, 2017.
[2] M. F. Folstein, S. E. Folstein, and P. R. McHugh, ““mini-mental
state”: a practical method for grading the cognitive state of pa-
tients for the clinician,” Journal of psychiatric research, vol. 12,
no. 3, pp. 189–198, 1975.
[3] Z. S. Nasreddine, N. A. Phillips, V. B´edirian, S. Charbonneau,
 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted March 27, 2021. 
; 
https://doi.org/10.1101/2021.03.24.21254263
doi: 
medRxiv preprint 


---

V. Whitehead, I. Collin, J. L. Cummings, and H. Chertkow, “The
montreal cognitive assessment, moca: a brief screening tool for
mild cognitive impairment,” Journal of the American Geriatrics
Society, vol. 53, no. 4, pp. 695–699, 2005.
[4] I. Arevalo-Rodriguez, N. Smailagic, M. R. i Figuls, A. Ciapponi,
E. Sanchez-Perez, A. Giannakou, O. L. Pedraza, X. B. Cosp, and
S. Cullum, “Mini-mental state examination (mmse) for the detec-
tion of alzheimer’s disease and other dementias in people with
mild cognitive impairment (mci),” Cochrane Database of System-
atic Reviews, no. 3, 2015.
[5] S.
Luz,
F.
Haider,
S.
de
la
Fuente,
D.
Fromm,
and
B. MacWhinney, “Alzheimer’s dementia recognition through
spontaneous speech: The ADReSS Challenge,” in Proceedings
of INTERSPEECH 2020, Shanghai, China, 2020. [Online].
Available: https://arxiv.org/abs/2004.06833
[6] S. de la Fuente Garcia, C. Ritchie, and S. Luz, “Artiﬁcial intelli-
gence, speech and language processing approaches to monitoring
Alzheimer’s disease: a systematic review,” Journal of Alzheimer’s
Disease, vol. 78, no. 4, pp. 1547–1574, 2020.
[7] M. S. S. Syed, Z. S. Syed, M. Lech, and E. Pirogova, “Auto-
mated Screening for Alzheimer’s Dementia Through Spontaneous
Speech,” in Proc. Interspeech 2020, 2020, pp. 2222–2226.
[8] J. Yuan, Y. Bian, X. Cai, J. Huang, Z. Ye, and K. Church, “Dis-
ﬂuencies and Fine-Tuning Pre-Trained Language Models for De-
tection of Alzheimer’s Disease,” in Proc. Interspeech 2020, 2020,
pp. 2162–2166.
[9] V. Taler and N. A. Phillips, “Language performance in alzheimer’s
disease and mild cognitive impairment: A comparative review,”
Journal of Clinical and Experimental Neuropsychology, vol. 30,
no. 5, pp. 501–556, 2008.
[10] S. Luz, “Longitudinal monitoring and detection of Alzheimer’s
type dementia from spontaneous speech data,” in Computer Based
Medical Systems.
IEEE Press, 2017, pp. 45–46.
[11] B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Devillers,
C. M¨uller, and S. S. Narayanan, “The INTERSPEECH 2010 par-
alinguistic challenge,” in Procs. of the 11th Annual Conference
of the International Speech Communication Association, INTER-
SPEECH, 2010, pp. 2794–2797.
[12] F. Haider, S. de la Fuente, and S. Luz, “An assessment of paralin-
guistic acoustic features for detection of alzheimer’s dementia in
spontaneous speech,” IEEE Journal of Selected Topics in Signal
Processing, vol. 14, no. 2, pp. 272–281, 2020.
[13] B. Mirheidari, D. Blackburn, T. Walker, A. Venneri, M. Reuber,
and H. Christensen, “Detecting Signs of Dementia Using Word
Vector Representations.” in Interspeech, 2018, pp. 1893–1897.
[14] C. Pou-Prom and F. Rudzicz, “Learning multiview embeddings
for assessing dementia,” in Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing, 2018, pp.
2812–2817.
[15] S. Al-Hameed, M. Benaissa, and H. Christensen, “Detecting and
predicting alzheimer’s disease severity in longitudinal acoustic
data,” in Proceedings of the International Conference on Bioin-
formatics Research and Applications 2017, 2017, pp. 57–61.
[16] N. Linz, J. Tr¨oger, J. Alexandersson, M. Wolters, A. K¨onig, and
P. Robert, “Predicting dementia screening and staging scores from
semantic verbal ﬂuency performance,” in 2017 IEEE Interna-
tional Conference on Data Mining Workshops (ICDMW).
IEEE,
2017, pp. 719–728.
[17] M. Yancheva, K. C. Fraser, and F. Rudzicz, “Using linguistic fea-
tures longitudinally to predict clinical scores for alzheimer’s dis-
ease and related dementias,” in Proceedings of SLPAT 2015: 6th
Workshop on Speech and Language Processing for Assistive Tech-
nologies, 2015, pp. 134–139.
[18] J. Weiner and T. Schultz, “Detection of Intra-Personal Develop-
ment of Cognitive Impairment From Conversational Speech,” in
Speech Communication; 12. ITG Symposium, 2016, pp. 1–5.
[19] D. G. Clark, P. M. McLaughlin, E. Woo, K. Hwang, S. Hurtz,
L. Ramirez, J. Eastman, R. M. Dukes, P. Kapur, T. P. DeRamus,
and L. G. Apostolova, “Novel verbal ﬂuency scores and structural
brain imaging for prediction of cognitive outcome in mild cogni-
tive impairment,” Alzheimer’s and Dementia: Diagnosis, Assess-
ment and Disease Monitoring, vol. 2, pp. 113–122, 2016.
[20] A. L. Benton, “Differential behavioral effects in frontal lobe dis-
ease,” Neuropsychologia, vol. 6, no. 1, pp. 53–60, 1968.
[21] J. T. Becker, F. Boller, O. L. Lopez, J. Saxton, and K. L. McGo-
nigle, “The Natural History of Alzheimer’s Disease,” Archives of
Neurology, vol. 51, no. 6, p. 585, 1994.
[22] H. Goodglass, E. Kaplan, and B. Barresi, BDAE-3: Boston Di-
agnostic Aphasia Examination – Third Edition.
Lippincott
Williams & Wilkins Philadelphia, PA, 2001.
[23] S. Luz, S. de la Fuente, and P. Albert, “A method for analysis of
patient speech in dialogue for dementia detection,” in Resources
for processing of linguistic, paralinguistic and extra-linguistic
data from people with various forms of cognitive impairment,
D. Kokkinakis, Ed.
ELRA, May 2018, pp. 35–42.
[24] P. R. Rosenbaum and D. B. Rubin, “The central role of the propen-
sity score in observational studies for causal effects,” Biometrika,
vol. 70, no. 1, pp. 41–55, 04 1983.
[25] D. B. Rubin, “Matching to remove bias in observational studies,”
Biometrics, vol. 29, no. 1, pp. 159–183, 1973.
[26] D. Ho,
K. Imai,
G. King,
and E. A. Stuart,
“Matchit:
Nonparametric preprocessing for parametric causal inference,”
Journal of Statistical Software, Articles, vol. 42, no. 8, pp. 1–28,
2011. [Online]. Available: https://www.jstatsoft.org/v042/i08
[27] F. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andr´e,
C. Busso, L. Y. Devillers, J. Epps, P. Laukka, S. S. Narayanan
et al., “The Geneva minimalistic acoustic parameter set GeMAPS
for voice research and affective computing,” IEEE Trans. Affect.
Comput., vol. 7, no. 2, pp. 190–202, 2016.
[28] B. MacWhinney, “Tools for analyzing talk part 2: The CLAN
program,” 2017, pittsburgh, PA: Carnegie Mellon University.
[Online]. Available: http://talkbank.org/manuals/CLAN.pdf
[29] M. A. Covington and J. D. McFall, “Cutting the gordian knot: The
moving-average type–token ratio (mattr),” Journal of quantitative
linguistics, vol. 17, no. 2, pp. 94–100, 2010.
[30] MATLAB, version 9.6 (R2019a).
Natick, Massachusetts: The
MathWorks Inc., 2019.
[31] S. de la Fuente, C. Ritchie, and S. Luz, “Protocol for a
conversation-based analysis study: Prevent-ED investigates dia-
logue features that may help predict dementia onset in later life,”
BMJ Open, vol. 9, no. 3, 2019.
 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted March 27, 2021. 
; 
https://doi.org/10.1101/2021.03.24.21254263
doi: 
medRxiv preprint 


---
*Full text extracted from PDF for MemoVoice V3 algorithm training.*
