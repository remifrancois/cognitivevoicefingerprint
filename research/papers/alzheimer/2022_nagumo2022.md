# Nagumo2022 et al. (2022) — Full Text Extraction

**Source file:** 2022_nagumo2022.pdf
**Pages:** 14
**Extracted by:** MemoVoice research pipeline (PyMuPDF)

---

## Full Paper Content

npj | digital medicine
Article
Published in partnership with Seoul National University Bundang Hospital
https://doi.org/10.1038/s41746-025-02105-z
A systematic review of explainable
artiﬁcial intelligence methods for speech-
based cognitive decline detection
Check for updates
Ravi Shankar1
, Ziyu Goh2, Fiona Devi3 & Qian Xu4
Artiﬁcial intelligence models analyzing speech show remarkable promise for identifying cognitive
decline, achieving performance comparable to clinical assessments. However, their “black box”
nature poses signiﬁcant barriers to clinical adoption, as healthcare professionals require transparent
decision-making processes. This challenge is compounded by regulatory requirements, including
GDPR mandates for explainability and medical device regulations emphasizing AI transparency.
Following PRISMA guidelines, we systematically reviewed explainable AI (XAI) techniques for speech-
based detection of Alzheimer’s disease and mild cognitive impairment across six databases through
May 2025. From 2077 records, 13 studies met the inclusion criteria, employing XAI methods including
SHAP, LIME, attention mechanisms, and novel approaches across machine learning architectures.
Models achieved AUC values of 0.76-0.94, consistently identifying acoustic markers (pause patterns,
speech rate) and linguistic features (vocabulary diversity, pronoun usage). While XAI techniques
demonstrate promise for clinical interpretability, signiﬁcant gaps remain in stakeholder engagement,
real-world validation, and standardized evaluation frameworks.
As of 2019, an estimated 57 million individuals worldwide were living with
dementia, the majority of whom had Alzheimer’s disease and other
dementias (ADODs), and this number is projected to rise to 153 million by
20501. This reﬂects a dramatic global increase in dementia prevalence, lar-
gely driven by aging populations, particularly in low- and middle-income
countries2. Early detection of cognitive decline is crucial for timely inter-
vention, treatment planning, and support for patients and caregivers.
However, current diagnostic approaches face signiﬁcant limitations3. Neu-
ropsychological assessments require specialized training and can be time-
consuming, while neuroimaging techniques such as PETscans and MRI are
expensive and not universally accessible4. These constraints have driven the
search for more accessible, cost-effective screening methods that can be
deployed at scale.
Speech and language changes often manifest as early indicators of
cognitive decline, sometimes preceding other clinical symptoms by
several years5. These changes encompass multiple dimensions: reduced
lexical diversity, increased use of pronouns and ﬁller words, simpliﬁed
syntactic structures, altered speech ﬂuency, and changes in acoustic
properties such as pause patterns and articulation rate6,7. The multi-
faceted nature of these speech biomarkers makes them particularly
suitable for AI-based analysis, which can capture subtle patterns across
multiple features simultaneously.
Advances in natural language processing (NLP) and machine learning
(ML) have enabled the detection of these subtle speech markers with
accuracies exceeding 90% in distinguishing cognitively normal individuals
from those with dementia8,9. These AI models analyze a comprehensive
range of features, including acoustic characteristics (such as pitch variability
and speech rate), linguistic markers (vocabulary richness, grammatical
complexity), and semantic content (coherence, information density)10.
Despite their impressive performance, the clinical adoption of AI-
based cognitive assessment tools remains limited. A primary barrier is the
“black box” nature of many AI models, particularly deep learning archi-
tectures, which provide little insight into their decision-making processes11.
This lack of transparency creates several critical challenges. Healthcare
professionals need to understand the reasoning behind AI predictions to
effectively integrate them into diagnostic and treatment decisions. Without
clear explanations, clinicians may be reluctant to rely on AI recommenda-
tions, particularly in high-stakes diagnostic scenarios. Medical device reg-
ulations increasingly require transparency and interpretability in AI
systems12. The European Union’s Medical Device Regulation (MDR) and
1Clinical Research & Innovation Ofﬁce, Tan Tock Seng Hospital, National Healthcare Group, Singapore, 308433, Singapore. 2Yong Loo Lin School of Medicine,
National University of Singapore, Singapore, 117597, Singapore. 3Medical Affairs – Research Innovation & Enterprise, Alexandra Hospital, National University
Health System, Singapore, 159964, Singapore. 4School of Civil, Aerospace and Design Engineering, University of Bristol, Bristol, BS8 1TH, UK.
e-mail: ravisr.srivastava@gmail.com
npj Digital Medicine |  (2025) 8:724 
1
1234567890():,;
1234567890():,;


---

the FDA’s guidelines on AI/ML-based medical devices emphasize the need
for explainable decision-making processes13. Additionally, the General Data
Protection Regulation (GDPR) explicitly mandates explainability for
automated decision-making systems, creating both legal and clinical
imperatives for transparent AI implementations in healthcare settings.
When AI models make incorrect predictions, the lack of interpret-
abilitymakes it difﬁcult to identifythe sourceof errors ortodeterminewhen
the model might be unreliable for speciﬁc patient populations or clinical
contexts. Clinicians need to explain diagnostic decisions to patients and
their families. Opaque AI predictions complicate this communication and
may undermine patient trust in the diagnostic process.
Explainable AI (XAI) methods aim to address these challenges by
making AI models more interpretable and transparent14. XAI encompasses
a diverse set of techniques designed to provide insights into modelbehavior,
feature importance, and decision rationale. In the context of cognitive
decline detection, XAI can serve multiple purposes. Feature attribution
identiﬁeswhichspeechfeatures(e.g.,pausefrequency,word choice,acoustic
properties) most strongly inﬂuence the model’s predictions, allowing clin-
icians to understand the linguistic and acoustic markers driving the
assessment15. Clinical alignment maps AI model behavior to established
clinical knowledge about speech changes in dementia, validating that the
model is learning clinically relevant patterns rather than spurious correla-
tions. Individual explanations provide patient-speciﬁc explanations that
highlight the particular speech characteristics contributing to their risk
assessment, enabling personalized clinical insights. Quality assurance
enables clinicians to verify that model predictions are based on appropriate
features and to identify potential biases or limitations in the model’s rea-
soning. For this review, we adopted a comprehensive deﬁnition of XAI
encompassing not only post-hoc explanation methods like SHAP and
LIME, but also attention mechanisms in neural networks, rule-based
interpretable models, and intrinsically transparent algorithms that provide
inherent interpretability.
While individual studies have begun incorporating XAI techniques
into speech-based cognitive assessment systems, the ﬁeld lacks a compre-
hensive synthesis of these approaches. Key questions remain unanswered
regarding which XAI techniques are most effective for different types of
speech analysis models and clinical applications, how well current XAI
implementations align with clinical needs and workﬂows, what evidence
exists for the clinical utility and impact of XAI-enhanced cognitive assess-
ment tools, and what technical and practical challenges exist in imple-
menting XAI for speech-based cognitive screening.
This systematic review aims to address these knowledge gaps by
providing a comprehensive overview of the speech datasets, AI archi-
tectures, and XAI techniques currently used for cognitive decline
detection. While previous systematic reviews have examined AI appli-
cations in speech-based dementia detection generally8,16, this review is
the ﬁrst to speciﬁcally focus on explainable AI implementations. Unlike
prior work that primarily assessed predictive performance, our review
uniquely evaluates interpretability methods, clinical translation efforts,
and stakeholder engagement, thereby addressing the critical gap
between AI development and clinical adoption that has emerged as
regulatory and clinical demands for transparency have intensiﬁed. We
analyze the technical implementation and evaluation of XAI methods
across studies, including their strengths and limitations. We examine the
clinical interpretability and translation of XAI outputs for cognitive
assessment, including stakeholder engagement and validation approa-
ches. We identify open challenges, methodological limitations, and
future directions for advancing XAI in this domain. Finally, we develop
recommendations for researchers and clinicians on best practices for
implementing
and
evaluating
XAI
in
speech-based
cognitive
assessment.
By critically appraising the latest research through an XAI lens, this
review provides actionable insights for developing more interpretable,
trustworthy, and clinically useful AI screening tools for early dementia
detection. The synthesis of technical approaches, clinical applications, and
implementation challenges offers a roadmap for advancing the ﬁeld toward
practical, deployable solutions that can enhance clinical decision-making
while maintaining transparency and trust.
Results
Study selection
The systematic search across six databases yielded 2077 records. After
removing 1118 duplicates, 959 unique records underwent title and abstract
screening. Of these, 831 were excluded as clearly not meeting the inclusion
criteria, leaving 128 records for full-text assessment. Following detailed full-
text review, 115 studies were excluded for the following reasons: no
explainable AI component implemented (n = 30, including 18 studies using
only traditional machine learning methods and 12 using statistical analyses
without XAI), not focused on speech-based analysis (n = 22), not targeting
cognitive decline/dementia (n = 23), wrong study type including reviews,
protocols, and abstracts (n = 18), language limitations (n = 5), study non-
retrieval (n = 9), and other reasons (n = 8). Ultimately, 13 studies met all
inclusion criteria and were included in the systematic review. The PRISMA
ﬂow diagram illustrates the complete study selection process (Fig. 1).
Workﬂow overview
Figure 2. presents a typical workﬂow for XAI-enhanced speech-based
cognitive assessment, illustrating the standard pipeline from audio acqui-
sition through feature extraction, model training, explanation generation,
and clinical interpretation.
Study Characteristics
The included studies were published between 2021 and 2025, with a notable
increaseinpublicationsfrom2023onward,includingonestudyin2021,one
in 2022, three in 2023, four in 2024, and four in 2025. This temporal
distribution reﬂects the recent emergence of XAI as a priority in healthcare
AI applications. Studies originated from diverse geographical regions, with
ﬁve from Europe (Spain, Greece, UK), four from Asia (Hong Kong, China),
three from North America (USA, Canada), and one international colla-
boration. All included studies were experimental in design, with primary
objectives including development of interpretable models for AD/MCI
detection (n = 8), comparison of XAI techniques for speech analysis (n = 3),
and clinicalvalidationofXAI-enhancedsystems(n = 2).Samplesizesvaried
considerably, ranging from 42 to 758 participants with a median of 162 and
interquartile range of 156-291. The total number of unique participants
across all studies was approximately 2,800, though some studies used
overlapping public datasets.
Population characteristics
The combined participant demographics across studies revealed mean ages
ranging from 63.1 to 85 years, with most reporting participants in their 70 s.
Age matching between cognitive groups was attempted in nine studies,
though four reported signiﬁcant age differences between controls and
patients. Overall, there was a slight female predominance at 58% female and
42% male, consistent with dementia epidemiology, though individual stu-
dies showed considerable variation ranging from 33% to 67% female. Only
ﬁve studies reported education levels, with means ranging from 12.1 to 15.3
years, representing a signiﬁcant gap in understanding potential con-
founders. Language and cultural diversity were limited, with ten studies
using English, three multilingual studies including Cantonese, Italian, and
Spanish, and most studies lacking ethnic/racial diversity reporting.
Across all studies, participants were distributed as follows: cognitively
normal controls comprised 1245 participants (44.5%), mild cognitive
impairment included 178 participants (6.4%), Alzheimer’s disease encom-
passed 821 participants (29.3%), other or mixed dementia included 352
participants (12.6%), subjective cognitive complaints comprised 9 partici-
pants (0.3%), and 195 participants (6.9%) were unspeciﬁed. Diagnostic
criteria varied across studies, with eight using clinical diagnosis by experts,
four using standardized criteria such as DSM-5 or NIA-AA, and one study
using cognitive test scores only.
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
2


---

Current dataset sizes and diversity constrain practical usability
claims. The median sample of 162 participants (IQR: 156-291) falls below
thresholds for robust machine learning, particularly for deep learning
requiring thousands of samples. Six studies using the same 156-
participant ADReSS subset limit independent validation. Population
homogeneity further restricts generalizability: ten studies used exclusively
English speakers, most lacked ethnic/racial diversity reporting, and only
ﬁve reported education levels. Vocabulary-based features may not gen-
eralize across educational levels or languages, and pause patterns may
differ across cultural conversational norms. Clinical deployment requires
validation in larger diverse cohorts (n > 1000), prospective real-world
settings, multiple languages, and populations with comorbidities reﬂect-
ing clinical heterogeneity. Current evidence supports proof-of-concept,
not practical deployment.
Fig. 1 | PRISMA ﬂow diagram illustrating the study selection process. The sys-
tematic review identiﬁed 2077 initial studies from database searches and 30 addi-
tional references from other sources. After removing 1148 duplicates, 959 studies
were screened. Full-text review of 128 articles resulted in 13 studies meeting all
inclusion criteria for ﬁnal analysis.
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
3


---

Speech data characteristics
Studies employed diverse speech elicitation tasks. Picture description was
used in nine studies, with seven using the Cookie Theft picture from the
Boston Diagnostic Aphasia Examination and two using other pictures,
typically requiring 1–3 min. Sequential narrative tasks were employed in
two studies, including the Rabbit Story with a 15-image sequence in one
study and custom multi-image tasks in another. Conversational speech was
utilized in three studies through semi-structured interviews, free con-
versation, and topic-prompted discussion. Other tasks used in four studies
included story recall, verbal ﬂuency tests, reading tasks, and memory
description.
Public datasets were commonly used, with ADReSS/ADReSSo (Alz-
heimer’s Dementia Recognition through Spontaneous Speech) employed in
six studies, DementiaBank/PittCorpusinfourstudies, and the Lu Corpus in
one study. Custom datasets included CU-MARVEL-RABBIT with 758
participants in one study and local clinical collections in four studies.
Recording conditions were predominantly controlled laboratory or clinic
settings in ten studies, home or telephone-based in two studies, and mixed
settings in one study.
Speech processing and feature extraction
Common preprocessing steps included noise reduction in eight studies,
volumenormalizationinsevenstudies,voiceactivitydetectioninsixstudies,
and sampling rate standardization, typically to 16 kHz, in nine studies.
Studies extracted diverse feature sets across multiple categories. Acoustic
features were used in twelve studies and included prosodic features such as
pitch (F0) statistics, energy, and speaking rate; spectral features including
MFCCs, formants, and spectral centroid; voice quality measures like jitter,
shimmer, and harmonic-to-noise ratio; and temporal features encompass-
ing pause patterns, silence ratios, and phonation time. Linguistic features
were employed in nine studies, covering lexical aspects like vocabulary
diversity, word frequency, and type-token ratio; syntactic measures
includingparsetreedepth,POStags,andgrammaticalcomplexity;semantic
features such as word embeddings, coherence measures, and information
content; and psycholinguistic categories from LIWC and concreteness
ratings. Combined approaches were used in seven studies, integrating
acoustic and linguistic feature fusion, multimodal embeddings, and end-to-
end deep features.
For transcript-based analysis, various automatic speech recognition
systems were employed. Google Cloud Speech-to-Text was used in two
studies, OpenAI Whisper in four studies, and other ASR systems in three
studies, while three studies relied on manual transcription only. Notably,
onlythreestudiesreportedworderrorrates,whichrangedfrom31%to34%.
AI model architectures
Traditional machine learning approaches were used in eight studies. Sup-
port Vector Machines (SVM) were employed in four studies using linear
and RBF kernels, achieving AUC values of 0.76–0.89. Random Forest
classiﬁers were the most common traditional approach, used in ﬁve studies
and often showing the best performance among traditional models with
Fig. 2 | Typical workﬂow for XAI-enhanced speech-based cognitive assessment.
Standard pipeline showing the progression from audio acquisition through fea-
ture extraction, model training, explanation generation, and clinical interpreta-
tion. The workﬂow encompasses speech task collection (picture description,
conversation), preprocessing steps (noise reduction, normalization), feature
extraction (acoustic and linguistic), AI model prediction, XAI method applica-
tion (SHAP, LIME, attention mechanisms), and clinical decision support
integration.
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
4


---

accuracies of 75–89%. Other traditional models included Logistic Regres-
sion in four studies, XGBoost in three studies, and Naïve Bayes in two
studies.
Deep learning approaches were utilized in ﬁve studies. Transformer-
based models included BERT variants in three studies and custom trans-
formers in two studies, generally achieving superior performancewith AUC
values of 0.84–0.94. Other deep architectures included CNNs in one study,
LSTMs in one study, and attention networks in two studies. Performance
metrics varied by task and dataset, with binary classiﬁcation of AD versus
control achieving AUC values of 0.76–0.94, multi-class classiﬁcation
including MCI showing accuracies of 71–96%, MMSE prediction demon-
strating MAE of 3.7–4.7 points, and the best overall performance coming
from multimodal transformer approaches (Table 1).
Dataset overlap existed across the included studies, with six studies
utilizing the ADReSS/ADReSSo dataset17–22, most analyzing identical 156-
participant subsets23 from the challenge dataset. Two studies used the
DementiaBank/Pitt Corpus24,25 with different participant cohorts. Li et al.26
employed both ADReSS and their novel CU-MARVEL-RABBIT corpus
(758 participants). Lima et al.21 uniquely incorporated multiple datasets
including ADReSSo, the Lu corpus (54 participants), and an independent
pilot study (22 participants). The remaining studies (de Arriba-Pérezet al.27,
Ambrosini et al.28, Jang et al.29, and Chandler et al.30) used custom datasets
without overlap and without relying on standard benchmarks. The total
number of unique participants across all studies was approximately 2500
which islower than the apparentcombined sample when overlap is ignored.
We addressed this by analyzing ﬁndings at the study level rather than
pooling participant data, prioritizing convergent ﬁndings across truly
independent datasets as stronger evidence, and giving greater weight to
novel datasets like CU-MARVEL-RABBIT in our synthesis. This overlap
pattern underscores the ﬁeld’s heavy reliance on established benchmark
datasets and highlights the need for validation across more diverse and
independent cohorts to establish true generalizability of XAI approaches in
speech-based cognitive assessment.
Explainable AI implementations
Feature attribution methods were employed in eleven studies. SHAP
(SHapley Additive exPlanations) was used in seven studies with variants
including TreeSHAP in three studies, DeepSHAP in one, KernelSHAP in
one, and standard SHAP in two. SHAP was applied to Random Forest,
XGBoost, SVM, and neural networks, with visualizations including sum-
mary plots, waterfall plots, and force plots. LIME (Local Interpretable
Model-agnostic Explanations) was implemented in two studies using
standard LIME with 5000 samples, applied to BERT and traditional ML
models. Other attribution methods used in three studies included permu-
tation importance, correlation analysis, and mutual information.
Attention-based methods were utilized in three studies, encompassing
self-attention visualization in transformers, cross-modal attention between
speech and text, and temporal attention patterns. Rule extraction was
employedinonestudythroughdecisiontreeapproximationandif-thenrule
generation. Novel approaches in two studies included counterfactual gen-
eration using LLMs and retrieval-augmented generation for explanations.
Implementation characteristics showed that ten studies used post-hoc
explanation, two studies employed intrinsically interpretable models, and
one study used hybrid approaches.
Our analysis reveals distinct clinical utility proﬁles for different XAI
methods. SHAP, employed in seven studies, demonstrated the strongest
clinicalalignmentbyprovidingbothglobalfeatureimportancerankingsand
patient-speciﬁc explanations that map to familiar clinical concepts like
speech timing and vocabulary diversity. However, SHAP’s computational
complexity and occasional instability across similar cases limit real-time
clinical applications. LIME, used in two studies, offered model-agnostic
ﬂexibility but suffered from inconsistent explanations for similar patients,
raising concerns about clinical reliability. Attention mechanisms in three
studies provided intuitive visualizations of temporal speech patterns but
remained limited to speciﬁc neural architectures and lacked validation
against clinical knowledge. Rule extraction methods, implemented in one
study, generated directly interpretable clinical decision trees but over-
simpliﬁed the complex speech-cognition relationship. These ﬁndings sug-
gest that while SHAP shows the most promise for near-term clinical
integration, none of the current XAI approaches fully meet the dual
requirements of technical accuracy and clinical interpretability needed for
widespread adoption.
The granularity of explanations varied, with eleven studies providing
global feature importance, eight offering instance-level explanations, and
three presenting temporal or sequential explanations. Computational
considerations were sparsely reported, with only two studies mentioning
real-time capability, four specifying GPU requirements, and three discuss-
ing scalability (Table 2).
XAI evaluation methods
Only ﬁve studies conducted a formal technical evaluation of XAI methods.
Quantitative metrics included feature stability across folds in three studies,
consistency measures in two studies, and faithfulness evaluation in one
study. Ablation studies were more common, with four studies performing
feature removal validation and three comparing models with and without
top features.
Clinical evaluation was limited, with expert review conducted in only
three studies. These involved clinician assessment of feature relevance,
alignment with clinical knowledge,and facevalidityevaluation.Userstudies
were similarly rare, conducted in three studies with medical professionals in
two studies and mixed stakeholders in one. Evaluation metrics included
interpretability ratings, usefulness scores, and trust measures. Studies
reporting formal evaluation found mean interpretability ratings of 3.96/5,
clinical relevance scores of 3.85/5, diagnostic utility ratings of 3.70/5, and
risk of misinterpretation scores of 2.38/5 (where lower is better).
Clinical translation and implementation
Visual explanations were the predominant format, used in eleven studies.
Feature importance bar charts were most common, followed by SHAP
summary plots in seven studies, heat maps in four studies, and interactive
dashboards in two studies. Textual explanations were less common,
appearing in four studies as natural language summaries, clinical report
generation, and risk stratiﬁcation narratives. Hybrid approaches combining
visual and textual explanations were used in three studies, offering multi-
level explanations with an overview and details.
Clinical integration strategies varied across studies. Four studies pro-
posed integration into decision support systems, including electronic health
records, screening workﬂows, and telemedicine platforms. Three studies
implemented risk stratiﬁcation using three-tier systems with low, medium,
and high-risk categories, continuous risk scores, and actionable thresholds.
Deployment considerations were notably limited, with only two studies
discussing privacy and data security, one mentioning regulatory com-
pliance, and two addressing training requirements.
Clinical Adoption Readiness Assessment
To evaluate the practical readiness of XAI implementations for clinical
deployment,weassessedeach studyacrossﬁvecriticaldomains:stakeholder
engagement, explanation format quality, XAI evaluation rigor, availability
of training materials, and workﬂow integration considerations (Supple-
mentary Table 1).
The assessment revealed substantial gaps in clinical readiness across all
included studies. Only two studies (15%) achieved a clinical readiness score
of 3/5 or higher, with the majority (85%) scoring 2/5 or below. The most
critical deﬁciencies were in stakeholder engagement, where 12 of 13 studies
(92%) failed to involve clinicians, patients, or other end-users in the design
or evaluation process. Training materials were completely absent across all
studies (100%), representing a universal barrier to clinical adoption.
Explanation format quality showed the strongest performance,
with 11 studies (85%) providing adequate visual or textual explanations
through methods such as SHAP visualizations, feature importance
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
5


---

plots, or natural language summaries. However, XAI evaluation rigor
remained limited, with only 4 studies (31%) conducting formal
assessments of explanation quality, clinical utility, or user satisfaction.
Workﬂow integration considerations were addressed partially or ade-
quately in only 6 studies (46%), with most studies failing to demonstrate
how their XAI-enhanced tools would integrate into existing clinical
workﬂows.
The two highest-scoring studies distinguished themselves through
user studies involving clinical stakeholders and formal evaluation of
explanation quality. Jang et al.29 conducted user experience evaluation
Table 1 | Technical Implementation and Evaluation of XAI Methods
Study
XAI method(s) and
implementation
Feature types
and base models
Performance
metrics
Evaluation framework
and stability assessment
Quantitative
metrics
Datasets used
Heitz et al. (2024)17
- SHAP
- Applied to Random
Forest
- Mean absolute SHAP
value for importance
- Lexical
- Syntactic
- Semantic
Models:
- BERT
- Random Forest
AUROC:
- Manual BERT:
0.899
- Manual RF: 0.888
- ASR BERT: 0.837
- ASR RF: 0.865
- Cross-validation
- Feature stability
measurement
- Most stable:
avg_word_length, mattr
- Most unstable:
ﬂesch_kincaid
- Balanced dataset
validation
- Feature stability
(sj metric)
- SHAP value
distribution
ADReSS Challenge
Dataset
Ilias & Askounis
(2022)18
- LIME
- 5000 samples
- Model-agnostic
approach
- Lexical
- Syntactic
- Semantic
Models:
- BERT variants
- BioBERT
- Siamese
networks
- Accuracy: 87.50%
- F1-score: 86.73%
- Statistical signiﬁcance
testing
- Feature correlation
analysis
- POS tag stability analysis
- Point-biserial
correlation
- Jaccard’s index
- 5-fold cross-
validation
ADReSS Dataset
de Arriba-Pérez et al.
(2024)27
- Meta-transformer
wrapper
- Tree algorithm-based
- Component-based
analysis
- Content-
independent
- High-level
reasoning
- Context-
dependent
Models:
- Random Forest
- Decision Tree
- Naive Bayes
- LLM (ChatGPT)
- Accuracy: 98.47%
- Precision: 98.49%
- Component-based
analysis
- Feature selection metrics
- Context-independent
features
- Mean Decrease
in Impurity
- Pearson
correlation
- 10-fold cross-
validation
Celia web application
dataset
Ambrosini et al.
(2024)28
- SHAP
- Feature attribution
method
- Feature importance
ranking
- Voice periodicity
- Spectral
- Syllabic
Models:
- SVM
- CatBoost
- Logistic
Regression
- Italian: 80-86%
- Spanish: Lower
performance
- Multi-language validation
- Cross-lingual stability
- Language-speciﬁc
features
- Feature
attribution scores
- Cross-lingual
metrics
- Multi-center
validation
Custom dataset
Tang et al. (2023)19
- SHAP
- Open-source Python
package
- Global feature
importance
- Lexical
- Syntactic
- Semantic
Models:
- SVM
- MLP
- AdaBoost
- Ensemble
- Accuracy: 89.58%
- AUC: 0.9531
- Global-local
interpretation
- Feature ranking
- ASR stability analysis
- Feature elimination
- SHAP values
- Feature
importance
scores
ADReSS-IS2020
dataset
Chandler et al.
(2023)30
- Feature attribution
methods
- Decision Tree
extraction
- Statistical testing
- Lexeme-level
- Syntactic
- Semantic
Models:
- Random Forest
- Overall: 75%
- F1-scores:
0.72-0.77
- Statistical testing
- Clinical correlation
- Temporal stability
- F-statistic
- Univariate
selection
- Clinical
validation
Custom dataset
Iqbal et al. (2024)24
- LIME + SHAP
- Feature importance
extraction
- Statistical testing
- Lexical
- Syntactic
- POS tags
Models:
- Random Forest
- Accuracy: 80%
- F1-score: 79-81%
- LIME-SHAP comparison
- Statistical validation
- POS tag stability
- Conﬁdence
metrics
- Feature
importance
- Random search
DementiaBank
(ADReSSo challenge
dataset)
Han et al. (2025)25
- SHAP (Tree-based)
- Counterfactual
generation via LLM
- Chain-of-thought
prompting
- TF-IDF features
- Pause features
Model:
- XGBoost
- Sensitivity: 4% →
42%
- F1-score: 4%
→35%
- Feature analysis before/
after generation
- Euclidean distance
metrics
- SHAP values
- Feature
elimination
- Distance metrics
Pitt corpus from
DementiaBank
Oiza-Zapata &
Gallardo-Antolín
(2025)20
- SHAP
- Mutual Information
- Dual use for selection &
interpretation
- eGeMAPS (88
features)
Models:
- SVM
- Random Forest
- XGBoost
- Accuracy: 75.00%
- AUC: 0.76
- CUI: 0.5643
- Clinical Utility Index
- ~70% computation
reduction
- Systematic evaluation
- SHAP
importance
- MI scores
- Clinical utility
ADReSS Dataset
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
6


---

with 127 participants achieving >90% satisfaction, while Ntampakis
et al.22 obtained clinical relevance ratings of 3.85/5 from medical pro-
fessionals. These ﬁndings highlight the critical importance of stake-
holder engagement and formal validation for advancing XAI tools
toward clinical deployment.
Key ﬁndings from XAI insights
Across studies using XAI, convergent ﬁndings emerged regardingimportant
speech markers, demonstrating consistency that supports the robustness of
these methods. Acoustic markers consistently identiﬁed across seven inde-
pendent studies using different XAI techniques (SHAP, LIME, attention
mechanisms) included increased pause frequency and duration, reduced
speech rate and articulation clarity, altered pitch variability (usually
decreased), and changes in voice quality measures. Linguistic markers
repeated across nine studies spanning diverse datasets (ADReSS, Demen-
tiaBank, custom cohorts), strengthening conﬁdence that these represent
genuinebiomarkersratherthandatasetartifacts.However,onlythreestudies
formally assessed explanation stability across cross-validation folds, showing
moderate consistency (correlations 0.65-0.82). This suggests individual XAI
explanationsmayexhibitvariabilityaffectingclinicalreliability.Standardized
evaluation protocols for stability, faithfulness, and consistency are required
before these methods can be considered truly robust for clinical deployment.
Different model architectures emphasized different aspects of speech
analysis. Traditional ML models focused primarily on statistical feature
aggregates, while deep learning models better captured temporal dynamics
and context. Multimodal models successfully leveraged complementary
information sources, suggesting that different architectures may be suited
for different clinical applications.
Risk of bias assessment
We assessed methodological quality using QUADAS-2, evaluating four
risk of bias domains and three applicability domains. Decision rules:
Low risk (+) for appropriate methodology, representative samples,
and clear protocols; unclear risk (?) for limited reporting or insufﬁcient
detail; high risk (−) for signiﬁcant bias, inappropriate methods, or
major methodological ﬂaws. Overall quality was good across 13 studies
(91 domain assessments): 65 assessments (71.4%) low risk, 18 (19.8%)
unclear risk, and 8 (8.8%) high risk. All studies demonstrated appro-
priate index test methodologies (100% low risk), while patient selection
showed more variability with 5 studies (38.5%) low risk, 6 studies
(46.2%) unclear risk, and 2 studies (15.4%) high risk. The following
table (Table 3) provides immediate per-study quality assessment. The
comments column provides a brief summary of the key methodological
strengths and limitations for each study.
Challenges and limitations reported
Studies reported three primary technical limitations: dataset constraints
(small samples in 8 studies, class imbalance in 6), generalization challenges
Table 1 (continued) | Technical Implementation and Evaluation of XAI Methods
Study
XAI method(s) and
implementation
Feature types
and base models
Performance
metrics
Evaluation framework
and stability assessment
Quantitative
metrics
Datasets used
Jang et al. (2021)29
- Feature importance
analysis
- T-statistics from LR
coefﬁcients
- Odds ratio
interpretation
- Acoustic
(MFCCs)
- Linguistic
- Eye-tracking
Models:
- Logistic
Regression
- Random Forest
- Gaussian
Naïve Bayes
- AUC: 0.83 ± 0.01
- Task fusion
performance
- 10-fold cross-validation
- Feature ranking
- Multi-modal analysis
- T-statistics
- Odds ratios
- Conﬁdence
intervals
Custom dataset
Li et al. (2025)26
- SHAP
- Attention mechanisms
- Correlation analysis
- Acoustic
- Linguistic
- Topic modeling
(DTM)
Models:
- SVM
- TITAN (custom)
- Accuracy: 71.0%
- AUC: 0.8120
- F1: 0.7238
- Spearman correlation
- Cross-modal consistency
- Temporal analysis
- SHAP beeswarm
plots
- Attention
weights
- R² = 0.3876
CU-MARVEL-RABBIT
Corpus, ADReSS
Lima et al. (2025)21
- SHAP (TreeExplainer)
- Feature importance
- Risk stratiﬁcation
- NLP features
(100)
- eGeMAPS
- GPT
embeddings
Models:
- Random Forest
- XGBoost
- DNN
- Accuracy: 76.5%
- AUC: 0.857
- MAE: 3.7 (MMSE)
- 10-fold cross-validation
- External validation
- Demographic parity
- SHAP values
- Feature rankings
- Risk categories
ADReSSo, Lu Corpus,
Pilot study
Ntampakis et al.
(2025)22
- Attention visualization
- RAG-based
explanations
- Literature grounding
- Acoustic (47)
- Wav2Vec2
embeddings
- DeBERTa
embeddings
Model:
- Multimodal
Transformer
- Accuracy: 95.77%
- F1: 0.9576
- Medical professional
evaluation
- Interpretability: 3.96/5
- Clinical relevance: 3.85/5
- Attention maps
- Explanation
quality scores
- Clinical utility:
3.70/5
IS2021 ADReSSo
Challenge Dataset
AD Alzheimer’s disease, ADReSS Alzheimer’s dementia recognition through spontaneous speech, ASR automatic speech recognition, AUC area under the curve, AUROC area under the receiver operating
characteristic curve, BERT bidirectional encoder representations from transformers, BioBERT Biomedical BERT, CNN convolutional neural network, CUI Clinical Utility Index, CV cross-validation,
DeBERTa decoding-enhanced BERT with disentangled Attention, DTM Dynamic Topic Model, eGeMAPS extended Geneva Minimalistic Acoustic Parameter Set, F1 = F1-score, GPT generative pre-
trained transformer, LIME local interpretable model-agnostic explanations, LIWC linguistic inquiry and word count, LLM large language model, LR logistic regression, LSTM long short-term memory, MAE
mean absolute error, MCI mild cognitive impairment, MFCC Mel-frequency Cepstral coefﬁcients, MI mutual information, MLP multi-layer perceptron, MMSE mini-mental state examination, NLP natural
language processing, POS part-of-speech, QUADAS-2 Quality Assessment of Diagnostic Accuracy Studies-2, RAG retrieval-augmented generation, RF Random Forest, RNN recurrent neural network,
SHAP SHapley Additive exPlanations, SVM support vector machine, TF-IDF term frequency-inverse document frequency, TITAN text-image temporal alignment network, Wav2Vec2 wave-to-vector
version 2, XAI explainable artiﬁcial intelligence, XGBoost eXtreme gradient boosting
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
7


---

(cross-dataset validation in 4 studies, ASR errors in 3), and XAI complexity
(computational demands in 2 studies). These technical limitations sig-
niﬁcantly impact the robustness and generalizability of ﬁndings.
Clinical translation barriers were more signiﬁcant: insufﬁcient stake-
holder engagement (7 studies), lack of diversity (9 studies), and missing
workﬂow integration studies (10 studies) highlight the ﬁeld’s early
development stage. XAI-speciﬁc limitations included the trade-off between
modelperformanceandinterpretabilitymentionedinfourstudies,difﬁculty
in validating explanation correctness noted in ﬁve studies, lack of standar-
dized XAI evaluation metrics mentioned in six studies, user comprehension
of complex explanations not assessed in eight studies, and potential for
misinterpretation of XAI outputs discussed in three studies.
Table 2 | Clinical translation and explainability approaches
Study
Clinical Interpretability &
Feature Mapping
Implementation Strategy
Local Explainability
Features
Global Explainability
Features
Clinical Applications &
Validation
Heitz et al. (2024)17
- Linguistically meaningful
features
- Clinical relevance
emphasis
- Linguistic features →
cognitive markers
- Direct integration with
model pipeline
- Real-time processing
capability
- Individual prediction
explanations
- Case-speciﬁc feature
analysis
- Overall feature
importance
- Model behavior
patterns
- Public dataset validation
- Clinical correlation
- Individual assessment
- Population-level
screening
Ilias & Askounis
(2022)18
- Natural language
explanations
- Clinical decision support
- Language patterns →
cognitive status
- Interactive visualization
- Clinical workﬂow
integration
- Instance-level
explanations
- Individual conﬁdence
scores
- Feature importance
hierarchy
- Model interpretation
- Validation against existing
research
- Clinical testing
- Patient-speciﬁc diagnosis
- General screening
de Arriba-Pérez et al.
(2024)27
- Domain-adapted
explanations
- Patient-friendly
interpretations
- High-level reasoning
features
- Web application interface
- Real-time analysis
- Individual session
analysis
- Personal feature
importance
- Population-level
patterns
- Feature relationships
- MMSE score correlation
- Clinical validation
- Individual monitoring
- Group analysis
Ambrosini et al.
(2024)28
- Multi-language support
- Clinical workﬂow
integration
- Acoustic-cognitive
mapping
- Mobile app integration
- Privacy preservation
- Subject-speciﬁc
analysis
- Individual language
patterns
- Cross-language
patterns
- Population trends
- Multi-center validation
- Cross-cultural testing
- Individual assessment
- Population screening
Tang et al. (2023)19
- Feature-based explanation
- Clinical correlation
- Linguistic markers →AD
indicators
- Clinical decision support
- Real-time analysis
- Case-based
explanations
- Individual feature
impact
- Model-wide patterns
- Feature importance
- Clinical dataset validation
- Feature veriﬁcation
- Patient diagnosis
- General screening
Chandler et al.
(2023)30
- Telephone-based
screening
- Clinical accessibility
- Language features →
cognitive status
- Remote assessment tool
- Clinical integration
- Individual call analysis
- Personal patterns
- Population trends
- Feature relationships
- MMSE correlation
- TICS-M validation
- Individual screening
- Population monitoring
Iqbal et al. (2024)24
- Binary classiﬁcation focus
- Clinical screening tool
- Clinical feature mapping
- POS patterns →cognitive
decline
- Clinical screening tool
- Feature-based analysis
- Case-speciﬁc analysis
- Individual thresholds
- Global feature
patterns
- Model behavior
- ADReSS dataset
validation
- Clinical testing
- Individual diagnosis
- General screening
Han et al. (2025)25
- Key speech markers
identiﬁed
- MCI-speciﬁc patterns
- Counterfactual insights
- LLM-based generation
- Real-time capable
- Patient-speciﬁc
counterfactuals
- Individual marker
analysis
- Population-level
patterns
- Feature directionality
- Framework validation
- Marker veriﬁcation
- Early MCI detection
Oiza-Zapata &
Gallardo-Antolín
(2025)20
- Acoustic biomarkers
- Clinical utility focus
- Smart city healthcare
- Efﬁcient pipeline
- Automated screening
- Patient-level analysis
- Personalized features
- Population patterns
- Feature rankings
- CUI assessment
- Healthcare integration
- Screening tool
Jang et al. (2021)29
- Multi-modal integration
- Clinical feature
interpretation
- Window features →AD
markers
- Testing platform
- Multi-sensor setup
- Individual task
performance
- Personal biomarkers
- Cross-task patterns
- Feature correlations
- Expert diagnosis
validation
- Novel task evaluation
- >90% user satisfaction
Li et al. (2025)26
- Topic evolution analysis
- Cross-modal consistency
- Macrostructural markers
- SHAP + attention
- Temporal modeling
- Individual narrative
patterns
- Session-speciﬁc
features
- Topic variability
metrics
- Population-level
insights
- Two-dataset validation
- Severity correlation
- Monitoring potential
Lima et al. (2025)21
- Risk stratiﬁcation (3-tier)
- Clinical markers
- Pronoun/disﬂuency
patterns
- Automated pipeline
- Conversational AI ready
- Patient risk proﬁles
- Individual explanations
- Feature importance
- Population patterns
- External validation
- Real-world pilot (n = 22)
- Demographic parity
Ntampakis et al.
(2025)22
- Literature-grounded
explanations
- Medical professional
design
- Evidence-based markers
- RAG architecture
- Dual-component system
- Patient-speciﬁc
explanations
- Clinical evidence links
- Model behavior
analysis
- Feature relationships
- Medical professional
evaluation
- Low misinterpretation risk
(2.38/5)
- Clinical utility: 3.70/5
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
8


---

Table 3 | QUADAS-2 Risk of Bias Assessment: Per-Study Domain Ratings
Low Risk (+) |
Unclear Risk (?) |
High Risk (−)
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
9


---

Subgroup analyses
Meta-analysis was not feasible due to substantial heterogeneity in XAI
methods (7 different primary techniques), outcome measures (accuracy
metrics ranging from 71-96% with different validation approaches), model
architectures (traditional ML vs. deep learning vs. hybrid), and evaluation
frameworks (technical vs. clinical vs. none). Instead, we conducted struc-
tured subgroup analyses:
Feature type analysis:
•
Acoustic-only models (n = 4): Mean AUC 0.79 (range 0.76–0.83)
•
Linguistic-only models (n = 3): Mean AUC 0.86 (range 0.84–0.89)
•
Multimodal models (n = 6): Mean AUC 0.88 (range 0.84–0.94)
Model Architecture Analysis:
•
Traditional ML (n = 8): Mean accuracy 81% (range 71–89%)
•
Deep learning (n = 5): Mean accuracy 89% (range 84–96%)
Dataset Source Analysis:
•
Public datasets only (n = 9): Limited generalizability concerns
•
Clinical datasets (n = 4): Higher clinical relevance but smaller
samples
Discussion
This systematic review of 13 studies reveals a rapidly evolving landscape in
explainable AI for speech-based cognitive decline detection. The ﬁeld has
made substantial progress in developing interpretable models that can
identify subtle speech markers of dementia while providing clinically
meaningful explanations. However, signiﬁcant gaps remain in clinical
validation, stakeholder engagement, and real-world implementation. The
convergence of ﬁndings across studies using different XAI techniques
strengthens conﬁdence in identiﬁed speech biomarkers. Acoustic features
related to speech timing and prosody, combined with linguistic markers of
reduced lexical diversity and increased disﬂuency, consistently emerged as
importantindicatorsacrossdifferentmodelarchitecturesandXAImethods.
This alignment with established clinical knowledge about language changes
in dementia provides face validity for the XAI approaches. Increased pause
frequency and reduced speech rate correspond to documented word-
ﬁnding difﬁculties in AD. Reduced vocabulary diversity mirrors semantic
memorydeterioration.Increasedpronounusagealignswith“emptyspeech”
where patients substitute vague terms for inaccessible nouns. Temporal
patterns showing progressive discourse deterioration match clinical litera-
ture on macrostructural coherence impairments while sentence-level
grammar remains preserved. However, some XAI-identiﬁed features like
speciﬁc MFCC patterns lack clear clinical precedent, highlighting potential
novel biomarkers requiring mechanistic validation.
The predominance of SHAP in seven studies reﬂects its versatility
across modeltypes and strong theoretical foundations. SHAP is particularly
useful in clinical contexts where both population-level insights and patient-
speciﬁc explanations are essential. For screening applications, SHAP’s glo-
bal feature importance helps clinicians understand which speech markers
(e.g., pause patterns, vocabulary diversity) most reliably indicate cognitive
decline across patient populations, enabling conﬁdent interpretation of
screening results. For individual patient assessment, SHAP’s instance-level
explanations allow clinicians to identify speciﬁc speech characteristics
drivinga particularpatient’sriskclassiﬁcation,supportingpersonalizedcare
planning and patient communication. SHAP’s ability to provide both global
feature importance and instance-level explanations makes it particularly
suitable for clinical applications where both population-level insights and
patient-speciﬁc information are valuable. However, the computational
complexity of SHAP, particularly for large neural networks, may limit real-
time applications. LIME’s model-agnostic approach is useful in contexts
where model ﬂexibility is paramount—for example, when comparing dif-
ferent AI architectures or when model types may change over time in a
clinical system. Its ability to work with any “black box” model makes it
valuable for research settings exploring diverse approaches. However, LIME
showed promise in two studies but faced challenges with stability and
consistency across similar instances, limiting its reliability for clinical
decision-making where consistent explanations are essential.
Attention mechanisms are particularly useful in deep learning contexts
where temporal dynamics matter, such as analyzing progressive speech
changes during extended narratives or conversations. By visualizing which
moments in a speech sample most inﬂuenced the model’s prediction,
attention mechanisms help clinicians understand whether the model
focuses on clinically meaningful patterns (e.g., mid-conversation topic drift)
or spurious correlations. This temporal granularity is especially valuable for
monitoring applications tracking cognitive change over time.
Identiﬁed speech markers align with established clinical observations,
supporting medical meaningfulness. Features highlighted by XAI methods
(pause patterns, reduced speech rate, decreased vocabulary diversity,
increased pronoun usage) correspond to documented language deteriora-
tion in Alzheimer’s disease. Studies correlating XAI-identiﬁed features with
clinical outcomes showed signiﬁcant associations: pause frequency and
lexicaldiversitycorrelated with MMSE scores and diagnoses. However,only
three studies conducted expert review of feature relevance, representing a
critical validation gap. Future work must demonstrate that XAI-highlighted
features provide actionable insights improving clinical decisions and patient
outcomes beyond correlation with cognitive status.
The emergence of novel approaches, including LLM-based counter-
factual generation and retrieval-augmented explanation systems, represents
an innovative direction that leverages recent advances in generative AI.
These methods offer the potential for more natural, context-aware expla-
nationsthatcouldbetteralignwithclinicalcommunicationneeds.However,
their validation and reliability require further investigation.
Ouranalysisrevealsanotabletensionbetweenmodelperformanceand
interpretability. Deep learning approaches, particularly transformer-based
models, achieved the highest performance metrics with AUC values up to
0.94 but required more complex XAI techniques that may be harder for
clinicians to understand. Conversely, traditional ML models like Random
Forests offered more straightforward feature importance rankings but with
moderately lower performance, showing AUC values of 0.76–0.89. This
trade-off has important implications for clinical deployment. The optimal
balance likely depends on the speciﬁc use case: screening applications may
prioritize interpretability for widespread adoption, while specialized diag-
nosticsupporttoolsmightacceptgreatercomplexityforenhancedaccuracy.
Future research should explicitly evaluate these trade-offs through user
studies with clinical stakeholders.
The division betweenstudies using hand-crafted features (acoustic and
linguistic) versus end-to-end deep learning approaches reﬂects a funda-
mental tension in the ﬁeld. Hand-crafted features offer inherent interpret-
ability and align with clinical understanding but may miss subtle patterns.
End-to-end approaches can discover novel patterns but require post-hoc
explanation methods that may not fully capture the model’s decision pro-
cess. The most promising direction appears to be hybrid approaches that
combine the strengths of both paradigms. For instance, using deep learning
for feature extraction while maintaining interpretable higher-level features
could provide both strong performance and meaningful explanations.
Despite technical advances, we found a striking gap between XAI
development and clinical implementation. Only three studies conducted
user evaluations with medical professionals, and none reported real-world
deployment outcomes. This implementation gap reﬂects several challenges.
Most studies developed XAI methods without early input from end-users.
Participatory design approaches involving clinicians, patients, and care-
givers from the projectinception could better ensure that explanations meet
actualclinical needs.The lackof integration studiesmeans we have a limited
understandingofhowXAI-enhancedtoolswouldﬁtintoexistingdiagnostic
workﬂows. Questionsremainaboutoptimal presentationformats, timingof
AI assistance, and handoff between automated screening and clinical
assessment. Even well-designed XAI systems require user training for
effective utilization. The absence of training protocols or support materials
in the reviewed studies represents a signiﬁcant barrier to adoption.
The studies primarily focused on explanations for clinicians, with
limited consideration of other stakeholders. A comprehensive approach
shouldconsiderdifferentneeds:forclinicians,technicalaccuracy,alignment
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
10


---

with clinical knowledge, and actionable insights for diagnosis and treatment
planning; for patients and families, understandable explanations that avoid
technical jargon, provide context for the assessment, and support shared
decision-making;forresearchers,detailedtechnicalexplanationsthatenable
model improvement and scientiﬁc advancement; and for regulators,
transparent documentation of model behavior, limitations, and validation
processes. Only one study22 explicitly designed explanations for multiple
stakeholder groups, highlighting this as an important area for future
development.
The heavy reliance on a few public datasets, particularly ADReSS/
ADReSSo used in six studies, raises concerns about generalizability. While
these datasets enable benchmarking, they may not represent the full spec-
trum of linguistic diversity, clinical heterogeneity, and recording conditions
found in real-world settings. Most studies used English speech data, limiting
applicability to other languages and cultures. Real-world populations
include various dementia types, comorbidities, and severity levels not fully
captured in research datasets. Laboratory recordings may not reﬂect nat-
uralistic speech patterns that would be encountered in clinical practice. The
largest custom dataset, CU-MARVEL-RABBIT with 758 participants,
demonstrated the value of larger, more diverse cohorts but was limited to a
single geographical region.
The lack of standardized XAI evaluation metrics hampers comparison
across studies. While some studies reported interpretability ratings and
clinicalrelevancescores,thesewereoftenad-hocandnotvalidated.Theﬁeld
needs standardized measures of explanation ﬁdelity, stability, and con-
sistency; validated scales for clinical utility, decision impact, and user
satisfaction; and assessment of potential harms from misinterpretation or
over-reliance on AI explanations. Most studies analyzed single speech
samples cross-sectionally, missing the opportunity to examine longitudinal
changes. Given that cognitive decline is a progressive process, XAI methods
that can explain temporal patterns and trajectory predictions would have
greater clinical value. Only one study26 explicitly modeled temporal
dynamics, representing a signiﬁcant gap.
SeveralpromisingdirectionsemergeforadvancingXAIinthisdomain.
Hierarchical explanations that provide information at multiple levels of
abstraction, from high-level clinical insights to detailed feature contribu-
tions, could better serve diverse user needs. Contrastive explanations that
clarify not just why a prediction was made, but why alternative diagnoses
were ruled out, align with clinical differential diagnosis reasoning.
Uncertainty-awareexplanationsincorporatingpredictionuncertaintycould
help clinicians better calibrate their trust and identify cases requiring
additional assessment. Interactive explanation systems that allow users to
query speciﬁc aspects of the prediction or explore “what-if” scenarios could
support more engaged clinical decision-making.
While this review focused on speech analysis, cognitive assessment
often involves multiple modalities. Future XAI systems should explain
how different data sources including speech, cognitive tests, imaging,
and biomarkers contribute to integrated predictions. This poses tech-
nical challenges in explaining cross-modal interactions but could pro-
vide more comprehensive clinical insights. The limited diversity in
current studies highlights the need for language-agnostic methods that
can transfer across languages while accounting for linguistic differences,
cultural adaptation of explanations that consider cultural factors in
communication patterns and clinical interpretation, and personalized
baselines that explain changes relative to individual baselines rather than
population norms.
Asthese toolsmove toward clinicaldeployment,severalconsiderations
require attention. XAI methods must meet evolving regulations for medical
AItransparency,includingnotjusttechnicaldocumentationbutevidenceof
clinical validation and safety. The use of AI for cognitive assessment raises
ethical questions about consent, especially for impaired individuals, data
privacy, and potential discrimination. XAI can help address some concerns
by making decision processes transparent, but careful ethical frameworks
are needed. Clear frameworks for how XAI explanations factor into clinical
liability and decision responsibility are essential for adoption.
For XAI-enhanced speech analysis to meaningfully impact clinical
practice, three key integration pathways emerge from our analysis. First, as
screening support tools, these systems could ﬂag individuals for compre-
hensive assessment, with explanations helping clinicians understand which
speech markers triggered the recommendation. This requires standardized
explanation formats that highlight clinically relevant features (pause pat-
terns, vocabulary changes) rather than technical metrics.
Second, as monitoring instruments, XAI explanations could track
longitudinal changes in speech markers, helping clinicians detect subtle
decline patterns. This demands explanation consistency across time points
and clear visualization of trend data that aligns with clinical assessment
schedules.
Third, successful implementation requires clinician training programs
that teach the interpretation of XAI outputs without overwhelming busy
practitioners. Our analysis suggests clinicians need: (1) brief explanations
focusing on familiar speech concepts, (2) clear conﬁdence indicators for AI
predictions, and (3) explicit guidance on when AI recommendations should
trigger further evaluation versus routine monitoring.
However, a fundamental challenge underlying these integration
pathways concerns whether the features identiﬁed by XAI methods are
genuinely interpretable by physicians. Feature interpretability varies sub-
stantially. Linguistic features (“increased pronoun usage,” “reduced voca-
bulary diversity,” “ﬁller words”) map directly to observable characteristics
clinicians can verify and align with familiar clinical concepts. Temporal
acoustic features like pause frequency and speech rate are readily under-
standable. However, many acoustic features present interpretability chal-
lenges. Technical parameters like mel-frequency cepstral coefﬁcients have
no clinical referent and cannot be perceived without specialized equipment.
XAI showing “MFCC coefﬁcient 7” as discriminative offers no actionable
clinical insight.
Only a few studiesdesigned explanations for physician interpretability.
Ntampakis et al.22 provided literature-grounded natural language transla-
tions of technical features, achieving clinical relevance ratings of 3.85/5.
Lima et al.21 implemented risk stratiﬁcation focusing on clinically familiar
markers. These approaches suggest effective interpretability requires
translation between XAI outputs and clinical frameworks, not merely
visualizing feature importance.
Forclinicalutility,futureimplementationsshould:prioritizeinherently
interpretable features when performance is comparable, provide clinical
contextualization explaining medical relevance beyond statistical impor-
tance, enable veriﬁcation through direct patient interaction, and undergo
formal usability testing with clinicians to ensure explanations support
decision-making.
Based on our analysis, XAI-enhanced speech analysis tools show
promise but are not yet ready for standalone clinical use. The technology
could currently best serve as screening support for identifying individuals
who may beneﬁt from comprehensive assessment, monitoring tools for
tracking changes over time with explainable trend analysis, and research
instruments for discovering new speech biomarkers with clinical validation.
For successful clinical translation, future systems need prospective valida-
tion in real clinical settings with diverse populations, user-centered design
involving all stakeholders in explanation design and evaluation, clear inte-
gration protocols for incorporating tools into existing workﬂows, compre-
hensive training programs for effective and safe use, and continuous
monitoring systems for tracking real-world performance and updating
models.
This review’s strengths include a comprehensive search across six
databases with additional hand-searching ensuring broad coverage, rigor-
ous methodology following PRISMA guidelines with pre-registration
enhancing transparency, a dual focus examining both technical and clin-
ical aspects providing holistic insights, and contemporary relevance by
including recent studies capturing the latest developments. However, lim-
itations include heterogeneity in methods and outcomes that precluded
meta-analysis, possible publication bias with under-representation of
negative results, and the rapid evolution of the ﬁeld meaning newest
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
11


---

developments may be missed. The included studies showed several meth-
odological limitations including small sample sizes (median 162 partici-
pants), limited population diversity with most studies using English-
speaking populations, reliance on cross-sectional designs missing long-
itudinal cognitive changes, and inconsistent reporting of technical imple-
mentation details.
This systematic review has limitations including heterogeneity in
methods and outcomes that precluded meta-analysis, possible publication
bias with potential under-representation of negative results, and the rapid
evolution of the ﬁeld meaning newest developments may be missed.
Additionally, the search was conducted in May 2025, and relevant studies
published after this date were not captured.
Based on our ﬁndings, we offer recommendations for different stake-
holder groups. Researchers should prioritize participatory design involving
clinical stakeholders from project inception, develop standardized XAI
evaluation frameworks speciﬁc to clinical applications, conduct prospective
validation studies in diverse real-world settings, explore novel XAI techni-
ques that align with clinical reasoning processes, and address the full
pipeline from data collection to clinical integration. Clinicians should
engage with AI researchers to ensure tools meet clinical needs, advocate for
interpretable AI in institutional adoption decisions, maintain critical eva-
luation of AI explanations in clinical context, and participate in validation
studies to shape appropriate use cases. Policy makers should develop reg-
ulatoryframeworksthatbalanceinnovationwithsafety,supportfundingfor
implementation and validation studies, establish standards for XAI in
medical applications, and promote diverse and inclusive dataset develop-
ment. Technology developers should design with the end-user in mind
rather than just technical performance, build in explanation capabilities
from the start rather than as an afterthought, provide comprehensive
documentation and training materials, and establish feedback mechanisms
for continuous improvement.
Methods
Protocol and registration
This systematic review was conducted following the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020
guidelines31. The review protocol was registered in the International Pro-
spective Register of Systematic Reviews (PROSPERO) with registration
number CRD42025637901. The complete protocol is available at: https://
www.crd.york.ac.uk/prospero/display_record.php?ID = CRD42025637901.
Three deviations from our registered protocol occurred during the
review process. First, we expanded our inclusion criteria to incorporate
high-quality preprints from arXiv and medRxiv, in addition to peer-
reviewed publications. This amendment was made in February 2025 due to
the rapidly evolving nature of explainable AI research, where signiﬁcant
methodological contributions often appear ﬁrst in preprint form before
formal publication. Given that several inﬂuential XAI techniques and
implementations were initially disseminated through preprint servers,
excluding these sources would have resulted in an incomplete synthesis of
the current state of knowledge. Second, we added evidence mapping as a
synthesis method in July 2025, beyond the originally planned narrative
synthesis. This methodological enhancement was implemented to better
visualize the distribution of XAI techniques across different model archi-
tectures and clinical applications, and to more clearly identify research gaps
and methodological patterns across studies. Third, we removed the planned
GRADE assessment from our original protocol as it was deemed inap-
propriate for this type of technical review focusing on AI implementation
methods rather than clinical interventions. These changes were made to
enhance the comprehensiveness and utility of the review while maintaining
methodological rigor through our established inclusion criteria and quality
assessment procedures.
Research question formulation
We formulated our research question using the PICOTS (Population,
Intervention, Comparison, Outcomes, Timing, Setting) framework. The
population included adults with or at risk of Alzheimer’s disease, mild
cognitive impairment, or related cognitive decline. The intervention con-
sisted of speech-based AI models for cognitive decline detection that
incorporate explainable AI techniques. Where available, we compared
speech-based AI models without explainable components or different XAI
approaches. Our outcomes of interest were technical implementation and
evaluation metrics for XAI methods, clinical interpretability and translation
of XAI outputs, and challenges, limitations, and future directions for XAI in
this domain. We placed no restriction on follow-up duration or study
period, and included studies from any setting including community, clinic,
research laboratory, or home-based assessment.
Eligibility criteria
Studies were included if they met all of the following criteria. Original
research articles published in peer-reviewed journals or conference pro-
ceedings were eligible, including both experimental and observational stu-
dies. The population had to involve adults (≥18 years) with conﬁrmed or
suspected cognitive decline, including Alzheimer’s disease at all stages, mild
cognitive impairment, subjective cognitive decline, or healthy controls for
comparison. Studies must have used AI/ML models to analyze speech or
language data, implemented explainable AI techniques to interpret model
predictions, and reported on the explainability component as a primary or
secondary outcome. Studies were included regardless of publication lan-
guage, with attempts made to translate non-English publications, and no
restriction on publication date was applied to capture the full evolution of
the ﬁeld. High-quality preprints were included, given the rapidly evolving
nature of XAI research, where signiﬁcant developments often appear in
preprint form before formal publication. Preprints were evaluated using the
same quality criteria as published studies, with particular attention to
methodologicalrigorandsufﬁcientdetailforassessment.Sensitivityanalysis
excluding preprints (n = 4) did not materially change our conclusions
regarding XAI method effectiveness or clinical translation gaps.
Studies were excluded if they did not use AI/ML models (e.g., tradi-
tional statistical analyses only), did not incorporate any explainable AI
component or only mentioned interpretability without implementation,
used non-speech data exclusively (e.g., neuroimaging, gait patterns, hand-
writing), focused exclusively on non-AD cognitive disorders without rele-
vance to dementia (e.g., developmental language disorders, stroke-related
aphasia), were reviews, commentaries, editorials, conference abstracts
without full papers, or study protocols, or analyzed written language only
without spoken speech components.
Information sources and search strategy
A comprehensive literature search was conducted in May 2025 across six
electronic databases: Embase (n = 570), Web of Science (n = 173),
PubMed (n = 83), CINAHL (n = 17), Scopus (n = 595), and Cochrane
Library (n = 639). Each database was searched from inception to the
search date without date restrictions. The search strategy was developed
in collaboration with a medical librarian and reﬁned through iterative
testing. It combined four conceptual blocks: cognitive decline terms
(Alzheimer’s, dementia, cognitive impairment, MCI, cognitive decline,
memory loss, neurocognitive disorder), speech analysis terms (speech,
voice, spoken language, vocal, acoustic, verbal, linguistic, discourse,
conversation), artiﬁcial intelligence terms (machine learning, deep
learning, neural network, artiﬁcial intelligence, NLP, classiﬁcation,
prediction), and explainability terms (explainable, interpretable, trans-
parency, XAI, SHAP, LIME, feature importance, attribution, visualiza-
tion). Terms within each block were combined with OR operators, and
blocks were combined with AND operators. Complete search strategies
for all databases are provided in Supplementary Note 1.
To ensure comprehensive coverage, we also screened reference lists of
includedarticles,performedforwardcitationsearchesusingGoogleScholar,
consulted with domain experts to identify potentially missed studies, and
searched relevant conference proceedings including INTERSPEECH,
ICASSP, and NeurIPS workshops.
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
12


---

Study selection process
Study selection was performed using Covidence systematic review software
(Veritas Health Innovation, 2025). The process involved two stages. In title
and abstract screening, two independent reviewers screened all retrieved
records for relevance. Disagreements were resolved through discussion,
with a third reviewer consulted when consensus could not be reached. For
full-text review, potentially eligible studies were retrieved in full text and
assessed against the detailed inclusion criteria by the same two reviewers
independently.Reasonsforexclusionweredocumented.Cohen’skappawas
calculated to assess inter-rater agreement at both screening stages. We
achievedκ = 0.82fortitle/abstractscreeningandκ = 0.89forfull-textreview,
indicating substantial to almost perfect agreement.
Data collection process
A standardized data extraction form was developed and pilot-tested on ﬁve
studies. Two reviewers independently extracted data from each included
study.The extraction form captured studycharacteristicsincludingauthors,
year, country, funding sources, study design and objectives, and sample size
and participant ﬂow. Population details included demographics (age, sex,
education, ethnicity), cognitive status distribution, diagnostic criteria used,
and recruitment setting. Speech data and processing information encom-
passed speech tasks employed, recording conditions and equipment, audio
preprocessing steps, feature extraction methods, and feature types(acoustic,
linguistic, combined). AI model details covered model architectures,
training procedures, hyperparameter settings, performance metrics, and
validation approaches. XAI implementation data included XAI techniques
used, implementation details, visualization methods, evaluation of expla-
nations, and user studies if conducted. Clinical translation aspects captured
target users, explanation formats, clinical validation, integration strategies,
and reported barriers and facilitators. Missing data that could not be
obtained were noted in our extraction tables.
Risk of bias assessment
Two reviewers independently assessed the risk of bias using the Quality
Assessment of Diagnostic Accuracy Studies-2 (QUADAS-2) tool32. This
tool evaluates four domains: patient selection (risk of spectrum bias and
applicabilityconcerns),indextest (riskofbiasinAImodeldevelopmentand
XAI implementation), reference standard (quality and appropriateness of
cognitive assessments), and ﬂow and timing (appropriate intervals and
missing data). Each domain was rated as high, low, or unclear risk of bias
and applicability concerns. Disagreements between reviewers were resolved
through discussion, with a third reviewer consulted in cases where con-
sensus could not be reached after initial discussion. For example, studies
with convenience sampling from single centers were typically rated as
‘unclear risk’ for patient selection due to potential spectrum bias, while
studies with well-deﬁned inclusion criteria and representative populations
received ‘low risk’ ratings. Given the focus on XAI, we additionally assessed
the transparency of XAI method reporting, the appropriateness of XAI
techniques for model types, the rigor of XAI evaluation, and the clinical
relevance of explanations.
Data synthesis
Due to substantial heterogeneity in XAI approaches, outcomes, and eva-
luation methods, we conducted a narrative synthesis structured around our
research objectives. The synthesis was organized by XAI technique cate-
gories (feature attribution including SHAP and LIME, attention mechan-
isms, rule extraction, example-based methods), model architecture types
(traditional ML, deep learning, ensemble methods), clinical applications
(screening, diagnosis support, monitoring), and evaluation approaches
(technical metrics, clinical validation, user studies). We created evidence
maps to visualize the distribution of XAI techniques across model types, the
relationship between performance metrics and interpretability, and gaps in
clinical validation and user evaluation.
The following deviations from our registered protocol occurred: we
expanded inclusion criteria to include high-quality preprints given the
rapidly evolving nature of the ﬁeld, and we added evidence mapping as a
synthesis method to better visualize patterns and gaps. These changes were
made to enhance the comprehensiveness and relevance of the review.
Data availability
This is a systematic review of published literature. All data analyzed in this
study are derived from publicly available published studies identiﬁed
through systematic database searches. No new data were generated or col-
lected forthis review. The completelist of included studies,search strategies,
data extraction forms, and quality assessment results are provided in the
manuscript and Supplementary Information. The PROSPERO protocol
registration (CRD42025637901) is publicly available at https://www.crd.
york.ac.uk/prospero/display_record.php?ID = CRD42025637901.
Code availability
No custom code or scripts were developed for this systematic review. Data
extraction and analysis were conducted using standard systematic review
methodologies. Software used included Covidence systematic review soft-
ware (Veritas Health Innovation, 2025) for study screening and data
management, Microsoft Excel 2021 for data compilation and tabulation,
and statistical software for descriptive analyses and evidence mapping
visualizations.
Received: 14 July 2025; Accepted: 21 October 2025;
References
1.
Chen, S. et al. The global macroeconomic burden of Alzheimer’s
disease and other dementias: estimates and projections for 152
countries or territories. Lancet Glob. Health 12, e1534–e1543 (2024).
2.
Nichols, E. et al. Estimation of the global prevalence of dementia in
2019 and forecasted prevalence in 2050: an analysis for the Global
Burden of Disease Study 2019. Lancet Public Health 7, e105–e125
(2022).
3.
Fowler, N. R. et al. Implementing early detection of cognitive
impairment in primary care to improve care for older adults. J. Intern.
Med. 298, 31–45 (2025).
4.
Lee, H.-B. et al. Machine learning based prediction of cognitive
metrics using major biomarkers in SuperAgers. Sci. Rep. 15, 18735
(2025).
5.
Mueller, K. D. et al. Connected language in late middle-aged adults at
risk for Alzheimer’s disease. J. Alzheimer’s Dis. 54, 1539–1550
(2016).
6.
Fraser, K. C., Meltzer, J. A. & Rudzicz, F. Linguistic features identify
Alzheimer’s disease in narrative speech. J. Alzheimer’s Dis. 49,
407–422 (2015).
7.
Lindsay, H., Tröger, J. & König, A. Language impairment in
Alzheimer’s disease—robust and explainable evidence for AD-related
deterioration of spontaneous speech through multilingual machine
learning. Front. Aging Neurosci. 13, 2021 (2021).
8.
de la Fuente Garcia, S., Ritchie, C. W. & Luz, S. Artiﬁcial intelligence,
speech, and language processing approaches to monitoring
Alzheimer’s disease: a systematic review. J. Alzheimers Dis. 78,
1547–1574 (2020).
9.
Favaro, A., Dehak, N., Thebaud, T., Oh, E. & Moro-Velázquez, L.
Evaluation of interpretable speech biomarkers for monitoring
alzheimer’s disease and mild cognitive impairment progression.
Alzheimer’s Dementia 19, https://doi.org/10.1002/alz.080449 (2023).
10. Lopes da Cunha, P. et al. Automated free speech analysis reveals
distinct markers of Alzheimer’s and frontotemporal dementia. PLoS
ONE 19, e0304272 (2024).
11. Sadeghi, Z. et al. A review of explainable artiﬁcial intelligence in
healthcare. Comput. Electr. Eng. 118, 109370 (2024).
12. Kiseleva, A., Kotzinos, D. & De Hert, P. Transparency of AI in
healthcare as a multilayered system of accountabilities: between legal
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
13


---

requirements and technical limitations. Front. Artif. Intell. 5, 879603
(2022).
13. Santra, S., Kukreja, P., Saxena, K., Gandhi, S. & Singh, O. V.
Navigating regulatory and policy challenges for AI enabled
combination devices. Front. Med. Technol. 6, 1473350 (2024).
14. Barredo Arrieta, A. et al. Explainable Artiﬁcial Intelligence (XAI):
Concepts, taxonomies, opportunities and challenges toward
responsible AI. Inf. Fusion 58, 82–115 (2020).
15. Perez-Toro, P. et al. Interpreting acoustic features for the assessment
of Alzheimer’s disease using ForestNet. Smart Health 26, 100347
(2022).
16. Shankar, R., Bundele, A. & Mukhopadhyay, A. A Systematic review of
natural language processing techniques for early detection of
cognitive impairment. Mayo Clin. Proc.: Digital Health 3, 100205
(2025).
17. Heitz, J., Schneider, G. & Langer, N. The inﬂuence of automatic
speech recognition on linguistic features and automatic Alzheimer’s
disease detection from spontaneous speech. In Proceedings of the
2024 Joint International Conference on Computational Linguistics,
Language Resources and Evaluation (eds Calzolari, N. et al.)
15955–15969 (LREC-COLING 2024).
18. Ilias, L. & Askounis, D. Explainable Identiﬁcation of Dementia From
Transcripts Using Transformer Networks. IEEE J. Biomed. Health
Inform. PP, 1–1 (2022).
19. Tang, L., Zhang, Z., Feng, F., Yang, L. Z. & Li, H. Explainable
Alzheimer’s disease detection using linguistic features from
automatic speech recognition. Dement Geriatr. Cogn. Disord. 52,
240–248 (2023).
20. Oiza-Zapata, I. & Gallardo-Antolín, A. Alzheimer’s disease detection
from speech using shapley additive explanations for feature selection
and enhanced interpretability. Electronics 14 (2025).
21. Lima, M. R. et al. Evaluating spoken language as a biomarker for
automated screening of cognitive impairment. Preprint at https://
arxiv.org/abs/2501.18731 (2025).
22. Ntampakis, N. et al. NeuroXVocal: detection and explanation of
Alzheimer’s disease through non-invasive analysis of picture-
prompted speech. Preprint at https://arxiv.org/abs/2502.10108
(2025).
23. Ferrante, F. J. et al. Multivariate word propertiesin ﬂuency tasks reveal
markers of Alzheimer’s dementia. Alzheimers Dement 20, 925–940
(2024).
24. Iqbal, F., Syed, Z., Syed, M. S. S. & Syed, A. An explainable ai
approach to speech-based alzheimer’s dementia screening. In
SMM24, Workshop on Speech, Music and Mind 2024 (2024).
25. Han, Y., Lam, J., Li, V. & Cheung, L. A large language model based
data generation framework to improve mild cognitive impairment
detection sensitivity. Data & Policy 7, https://doi.org/10.1017/dap.
2025.8 (2025).
26. Li, J. et al. Detecting neurocognitive disorders through analyses of
topic evolution and cross-modal consistency in visual-stimulated
narratives. Preprint at https://arxiv.org/abs/2501.03727
(2025).
27. de Arriba Pérez, F., García-Méndez, S., Otero-Mosquera, J. &
Castaño, F. Explainable cognitive decline detection in free dialogues
with a Machine Learning approach based on pre-trained Large
Language Models. Appl. Intell. 54, 12613–12628 (2024).
28. Ambrosini, E. et al. Automatic spontaneous speech analysis for the
detection of cognitive functional decline in older adults: multilanguage
cross-sectional study. JMIR Aging 7, e50537 (2024).
29. Jang, H. et al. Classiﬁcation of Alzheimer’s disease leveraging multi-
task machine learning analysis of speech and eye-movement data.
Front Hum. Neurosci. 15, 716670 (2021).
30. Chandler, C., Diaz-Asper, C., Turner, R. S., Reynolds, B. & Elvevåg, B.
An explainable machine learning model of cognitive decline derived
from speech. Alzheimers Dement (Amst.) 15, e12516 (2023).
31. Page, M. J. et al. The PRISMA 2020 statement: an updated guideline
for reporting systematic reviews. BMJ 372, n71 (2021).
32. Whiting, P. F. et al. QUADAS-2: a revised tool for the quality
assessment of diagnostic accuracy studies. Ann. Intern. Med. 155,
529–536 (2011).
Acknowledgements
The authors acknowledge the medical librarian who assisted in developing
and reﬁning the comprehensive search strategy across six electronic
databases. Wethank the researchers and clinicianswhose workcontributed
to the body of literature synthesized in this systematic review, and the
medical professionals who provided input during the protocol development
phase. This research received no speciﬁc grant from any funding agency in
the public, commercial, or not-for-proﬁt sectors.
Author contributions
R.S. conceived and designed the systematic review, developed the search
strategy, performed study selection and data extraction, conducted the
quality assessment, analyzed and synthesized the data, provided
methodological guidance, supervised the systematic review process, and
wrote the manuscript. Z.G. independently performed study selection, data
extraction, and quality assessment, resolved discrepancies through
discussion, and critically reviewed the manuscript. F.D. validated the data
synthesis, and critically revised the manuscript for important intellectual
content. X.Q. provided senior oversight, contributed to the interpretation of
ﬁndings, and critically revised the manuscript. All authors reviewed and
approved the ﬁnal version of the manuscript.
Competing interests
The authors declare no competing interests.
Additional information
Supplementary information The online version contains
supplementary material available at
https://doi.org/10.1038/s41746-025-02105-z.
Correspondence and requests for materials should be addressed to
Ravi Shankar.
Reprints and permissions information is available at
http://www.nature.com/reprints
Publisher’s note Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long
as you give appropriate credit to the original author(s) and the source,
provide a link to the Creative Commons licence, and indicate if changes
were made. The images or other third party material in this article are
included in the article’s Creative Commons licence, unless indicated
otherwise in a credit line to the material. If material is not included in the
article’s Creative Commons licence and your intended use is not permitted
by statutory regulation or exceeds the permitted use, you will need to
obtain permission directly from the copyright holder. To view a copy of this
licence, visit http://creativecommons.org/licenses/by/4.0/.
© The Author(s) 2025
https://doi.org/10.1038/s41746-025-02105-z
Article
npj Digital Medicine |  (2025) 8:724 
14


---
*Full text extracted from PDF for MemoVoice V3 algorithm training.*
